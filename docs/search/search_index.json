{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\/\\s\\-\\.]+"},"docs":[{"location":"","text":"OpenAI \u5e73\u53f0 \u00b6 https://platform.openai.com/docs OpenAI \u5e73\u53f0 \u662f\u4e00\u7ec4\u65e8\u5728\u4f7f AI \u7814\u7a76\u3001\u5f00\u53d1\u548c\u90e8\u7f72\u66f4\u52a0\u6613\u4e8e\u4f7f\u7528\u7684\u5de5\u5177\u548c\u670d\u52a1\u7684\u96c6\u5408\u3002\u8be5\u5e73\u53f0\u63d0\u4f9b\u4e86\u591a\u79cd\u5f3a\u5927\u7684 API\uff0c\u5141\u8bb8\u5f00\u53d1\u4eba\u5458\u5c06\u5c16\u7aef\u7684 AI \u6280\u672f\u96c6\u6210\u5230\u4ed6\u4eec\u7684\u5e94\u7528\u7a0b\u5e8f\u4e2d\u3002\u6b64\u5916\uff0cOpenAI \u8fd8\u63d0\u4f9b\u4e86\u57f9\u8bad AI \u6a21\u578b\u7684\u8d44\u6e90\u548c\u5de5\u5177\uff0c\u4f7f\u7814\u7a76\u4eba\u5458\u548c\u5f00\u53d1\u4eba\u5458\u80fd\u591f\u5feb\u901f\u6784\u5efa\u9ad8\u8d28\u91cf\u7684\u6a21\u578b\u3002 \u529f\u80fd\u548c\u670d\u52a1 \u00b6 OpenAI \u5e73\u53f0\u63d0\u4f9b\u4e86\u51e0\u4e2a API \u548c\u5de5\u5177\uff0c\u5305\u62ec\uff1a GPT API \uff1a\u4e00\u79cd\u80fd\u591f\u751f\u6210\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u6587\u672c\u7684 AI \u6a21\u578b\u3002\u901a\u8fc7\u8f93\u5165\u6587\u672c\u7247\u6bb5\uff0c\u6b64 API \u53ef\u4ee5\u751f\u6210\u7c7b\u4f3c\u4e8e\u8fd9\u4e9b\u7247\u6bb5\u7684\u6587\u672c\u3002 DALL\u00b7E API \uff1a\u4e00\u79cd\u80fd\u591f\u6839\u636e\u6587\u672c\u63cf\u8ff0\u751f\u6210\u56fe\u50cf\u7684 AI \u6a21\u578b\u3002\u6b64 API \u53ef\u4ee5\u5c06\u6587\u672c\u63cf\u8ff0\u8f6c\u6362\u4e3a\u56fe\u50cf\u3002 CLIP API \uff1a\u4e00\u79cd\u901a\u7528\u7684\u89c6\u89c9 AI \u6a21\u578b\uff0c\u7ecf\u8fc7\u5927\u91cf\u6570\u636e\u8bad\u7ec3\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u8303\u56f4\u3002 OpenAI Codex \uff1a\u4e00\u79cd\u57fa\u4e8e GPT \u7684\u6a21\u578b\uff0c\u53ef\u4ee5\u5c06\u81ea\u7136\u8bed\u8a00\u7ffb\u8bd1\u4e3a\u8ba1\u7b97\u673a\u4ee3\u7801\u3002 OpenAI Gym \uff1a\u4e00\u79cd\u5f3a\u5316\u5b66\u4e60\u5de5\u5177\u5305\u3002\u8be5\u5de5\u5177\u5305\u5305\u62ec\u591a\u4e2a\u73af\u5883\uff0c\u53ef\u7528\u4e8e\u8bad\u7ec3\u548c\u6d4b\u8bd5 AI \u6a21\u578b\u3002 \u83b7\u53d6 OpenAI API \u5bc6\u94a5 \u00b6 \u8981\u4f7f\u7528 OpenAI \u5e73\u53f0\u7684 API\uff0c\u60a8\u9700\u8981\u83b7\u53d6 API \u5bc6\u94a5\u3002\u60a8\u53ef\u4ee5\u901a\u8fc7\u8bbf\u95ee https://beta.openai.com/signup/ \u83b7\u53d6 API \u5bc6\u94a5\u3002 \u5f00\u59cb\u4f7f\u7528 OpenAI \u5e73\u53f0 \u00b6 \u8981\u5f00\u59cb\u4f7f\u7528 OpenAI \u5e73\u53f0\uff0c\u8bf7\u6309\u7167\u4ee5\u4e0b\u6b65\u9aa4\u8fdb\u884c\uff1a \u83b7\u53d6 OpenAI API \u5bc6\u94a5\u3002 \u9009\u62e9\u8981\u4f7f\u7528\u7684 API \u6216\u5de5\u5177\u3002 \u9605\u8bfb\u76f8\u5173\u6587\u6863\u4ee5\u4e86\u89e3\u5982\u4f55\u4f7f\u7528 API \u6216\u5de5\u5177\u3002 \u4f7f\u7528 API \u5bc6\u94a5\u8fdb\u884c\u8eab\u4efd\u9a8c\u8bc1\u4ee5\u8bbf\u95ee API\u3002 \u5f00\u59cb\u4f7f\u7528 API \u6216\u5de5\u5177\u3002 \u6709\u5173\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u8bbf\u95ee https://openai.com/platform/ \u3002","title":"OpenAI \u5e73\u53f0"},{"location":"#openai","text":"https://platform.openai.com/docs OpenAI \u5e73\u53f0 \u662f\u4e00\u7ec4\u65e8\u5728\u4f7f AI \u7814\u7a76\u3001\u5f00\u53d1\u548c\u90e8\u7f72\u66f4\u52a0\u6613\u4e8e\u4f7f\u7528\u7684\u5de5\u5177\u548c\u670d\u52a1\u7684\u96c6\u5408\u3002\u8be5\u5e73\u53f0\u63d0\u4f9b\u4e86\u591a\u79cd\u5f3a\u5927\u7684 API\uff0c\u5141\u8bb8\u5f00\u53d1\u4eba\u5458\u5c06\u5c16\u7aef\u7684 AI \u6280\u672f\u96c6\u6210\u5230\u4ed6\u4eec\u7684\u5e94\u7528\u7a0b\u5e8f\u4e2d\u3002\u6b64\u5916\uff0cOpenAI \u8fd8\u63d0\u4f9b\u4e86\u57f9\u8bad AI \u6a21\u578b\u7684\u8d44\u6e90\u548c\u5de5\u5177\uff0c\u4f7f\u7814\u7a76\u4eba\u5458\u548c\u5f00\u53d1\u4eba\u5458\u80fd\u591f\u5feb\u901f\u6784\u5efa\u9ad8\u8d28\u91cf\u7684\u6a21\u578b\u3002","title":"OpenAI \u5e73\u53f0"},{"location":"#_1","text":"OpenAI \u5e73\u53f0\u63d0\u4f9b\u4e86\u51e0\u4e2a API \u548c\u5de5\u5177\uff0c\u5305\u62ec\uff1a GPT API \uff1a\u4e00\u79cd\u80fd\u591f\u751f\u6210\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u6587\u672c\u7684 AI \u6a21\u578b\u3002\u901a\u8fc7\u8f93\u5165\u6587\u672c\u7247\u6bb5\uff0c\u6b64 API \u53ef\u4ee5\u751f\u6210\u7c7b\u4f3c\u4e8e\u8fd9\u4e9b\u7247\u6bb5\u7684\u6587\u672c\u3002 DALL\u00b7E API \uff1a\u4e00\u79cd\u80fd\u591f\u6839\u636e\u6587\u672c\u63cf\u8ff0\u751f\u6210\u56fe\u50cf\u7684 AI \u6a21\u578b\u3002\u6b64 API \u53ef\u4ee5\u5c06\u6587\u672c\u63cf\u8ff0\u8f6c\u6362\u4e3a\u56fe\u50cf\u3002 CLIP API \uff1a\u4e00\u79cd\u901a\u7528\u7684\u89c6\u89c9 AI \u6a21\u578b\uff0c\u7ecf\u8fc7\u5927\u91cf\u6570\u636e\u8bad\u7ec3\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u8303\u56f4\u3002 OpenAI Codex \uff1a\u4e00\u79cd\u57fa\u4e8e GPT \u7684\u6a21\u578b\uff0c\u53ef\u4ee5\u5c06\u81ea\u7136\u8bed\u8a00\u7ffb\u8bd1\u4e3a\u8ba1\u7b97\u673a\u4ee3\u7801\u3002 OpenAI Gym \uff1a\u4e00\u79cd\u5f3a\u5316\u5b66\u4e60\u5de5\u5177\u5305\u3002\u8be5\u5de5\u5177\u5305\u5305\u62ec\u591a\u4e2a\u73af\u5883\uff0c\u53ef\u7528\u4e8e\u8bad\u7ec3\u548c\u6d4b\u8bd5 AI \u6a21\u578b\u3002","title":"\u529f\u80fd\u548c\u670d\u52a1"},{"location":"#openai-api","text":"\u8981\u4f7f\u7528 OpenAI \u5e73\u53f0\u7684 API\uff0c\u60a8\u9700\u8981\u83b7\u53d6 API \u5bc6\u94a5\u3002\u60a8\u53ef\u4ee5\u901a\u8fc7\u8bbf\u95ee https://beta.openai.com/signup/ \u83b7\u53d6 API \u5bc6\u94a5\u3002","title":"\u83b7\u53d6 OpenAI API \u5bc6\u94a5"},{"location":"#openai_1","text":"\u8981\u5f00\u59cb\u4f7f\u7528 OpenAI \u5e73\u53f0\uff0c\u8bf7\u6309\u7167\u4ee5\u4e0b\u6b65\u9aa4\u8fdb\u884c\uff1a \u83b7\u53d6 OpenAI API \u5bc6\u94a5\u3002 \u9009\u62e9\u8981\u4f7f\u7528\u7684 API \u6216\u5de5\u5177\u3002 \u9605\u8bfb\u76f8\u5173\u6587\u6863\u4ee5\u4e86\u89e3\u5982\u4f55\u4f7f\u7528 API \u6216\u5de5\u5177\u3002 \u4f7f\u7528 API \u5bc6\u94a5\u8fdb\u884c\u8eab\u4efd\u9a8c\u8bc1\u4ee5\u8bbf\u95ee API\u3002 \u5f00\u59cb\u4f7f\u7528 API \u6216\u5de5\u5177\u3002 \u6709\u5173\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u8bbf\u95ee https://openai.com/platform/ \u3002","title":"\u5f00\u59cb\u4f7f\u7528 OpenAI \u5e73\u53f0"},{"location":"data-usage-policies/","text":"API data usage policies \u00b6 We\u2019ve recently updated our data usage policies to be clearer and more specific. Updated March 1 st , 2023 Starting on March 1, 2023, we are making two changes to our data usage and retention policies: OpenAI will not use data submitted by customers via our API to train or improve our models, unless you explicitly decide to share your data with us for this purpose. You can opt-in to share data. Any data sent through the API will be retained for abuse and misuse monitoring purposes for a maximum of 30 days, after which it will be deleted (unless otherwise required by law). The OpenAI API processes user prompts and completions, as well as training data submitted to fine-tune models via the Files endpoint. We refer to this data as API data. By default, OpenAI will not use data submitted by customers via our API to train OpenAI models or improve OpenAI\u2019s service offering. Data submitted by the user for fine-tuning will only be used to fine-tune the customer's model. However, OpenAI will allow users to opt-in to share their data to improve model performance. Sharing your data will ensure that future iterations of the model improve for your use cases. Data submitted to the API prior to March 1, 2023 (the effective date of this change) may have been used for improvements if the customer had not previously opted out of sharing data. OpenAI retains API data for 30 days for abuse and misuse monitoring purposes. A limited number of authorized OpenAI employees, as well as specialized third-party contractors that are subject to confidentiality and security obligations, can access this data solely to investigate and verify suspected abuse. Enterprise customers deploying use cases with low likelihood of misuse may request to not have API data stored at all, including for safety monitoring and prevention. OpenAI may still have content classifiers flag when data is suspected to contain platform abuse. Data submitted by the user through the Files endpoint, for instance to fine-tune a model, is retained until the user deletes the file. Note that this data policy does not apply to OpenAI's Non-API consumer services like ChatGPT or DALL\u00b7E Labs. You can learn more about these policies in our data usage for consumer services FAQ. Frequently asked questions What technical protections and security certifications does OpenAI have in place surrounding the public API? OpenAI is SOC 2 Type 1 compliant and has been audited by an independent third-party auditor against the 2017 Trust Services Criteria for Security. Where is API data stored? Content is stored on OpenAI systems and our sub-processors\u2019 systems. We may also send select portions of de-identified content to third-party contractors (subject to confidentiality and security obligations) safety purposes. Our 30-day data retention policy also applies to our sub-processors and contractors. You can view our list of sub-processors and their locations for details. When I call the API, is the data encrypted in transit? The OpenAI API is only available over Transport Layer Security (TLS), and therefore customer-to-OpenAI requests and responses are encrypted. Do you have a European data center? All customer data is processed and stored in the US. We do not currently store data in Europe or in other countries. Can I use the API for HIPAA workloads? We are able to sign Business Associate Agreements in support of customers\u2019 compliance with the Health Insurance Portability and Accountability Act (HIPAA). To qualify for this, you must have an Enterprise Agreement with OpenAI and a qualifying use case. Please reach out to our sales team if you are interested. I\u2019ve received a data deletion request, how do I ask OpenAI to delete data? We only retain data sent through the API for up to 30 days for abuse and monitoring purposes. If you would like to delete your account before then, please follow these steps. Does OpenAI have a Data Processing Addendum (DPA)? Yes. Please complete our DPA form to execute our Data Processing Addendum. Can we self-host? We do not offer on-premise hosting. You may purchase dedicated capacity by reaching out to our sales team.","title":"API data usage policies"},{"location":"data-usage-policies/#api-data-usage-policies","text":"We\u2019ve recently updated our data usage policies to be clearer and more specific. Updated March 1 st , 2023 Starting on March 1, 2023, we are making two changes to our data usage and retention policies: OpenAI will not use data submitted by customers via our API to train or improve our models, unless you explicitly decide to share your data with us for this purpose. You can opt-in to share data. Any data sent through the API will be retained for abuse and misuse monitoring purposes for a maximum of 30 days, after which it will be deleted (unless otherwise required by law). The OpenAI API processes user prompts and completions, as well as training data submitted to fine-tune models via the Files endpoint. We refer to this data as API data. By default, OpenAI will not use data submitted by customers via our API to train OpenAI models or improve OpenAI\u2019s service offering. Data submitted by the user for fine-tuning will only be used to fine-tune the customer's model. However, OpenAI will allow users to opt-in to share their data to improve model performance. Sharing your data will ensure that future iterations of the model improve for your use cases. Data submitted to the API prior to March 1, 2023 (the effective date of this change) may have been used for improvements if the customer had not previously opted out of sharing data. OpenAI retains API data for 30 days for abuse and misuse monitoring purposes. A limited number of authorized OpenAI employees, as well as specialized third-party contractors that are subject to confidentiality and security obligations, can access this data solely to investigate and verify suspected abuse. Enterprise customers deploying use cases with low likelihood of misuse may request to not have API data stored at all, including for safety monitoring and prevention. OpenAI may still have content classifiers flag when data is suspected to contain platform abuse. Data submitted by the user through the Files endpoint, for instance to fine-tune a model, is retained until the user deletes the file. Note that this data policy does not apply to OpenAI's Non-API consumer services like ChatGPT or DALL\u00b7E Labs. You can learn more about these policies in our data usage for consumer services FAQ. Frequently asked questions What technical protections and security certifications does OpenAI have in place surrounding the public API? OpenAI is SOC 2 Type 1 compliant and has been audited by an independent third-party auditor against the 2017 Trust Services Criteria for Security. Where is API data stored? Content is stored on OpenAI systems and our sub-processors\u2019 systems. We may also send select portions of de-identified content to third-party contractors (subject to confidentiality and security obligations) safety purposes. Our 30-day data retention policy also applies to our sub-processors and contractors. You can view our list of sub-processors and their locations for details. When I call the API, is the data encrypted in transit? The OpenAI API is only available over Transport Layer Security (TLS), and therefore customer-to-OpenAI requests and responses are encrypted. Do you have a European data center? All customer data is processed and stored in the US. We do not currently store data in Europe or in other countries. Can I use the API for HIPAA workloads? We are able to sign Business Associate Agreements in support of customers\u2019 compliance with the Health Insurance Portability and Accountability Act (HIPAA). To qualify for this, you must have an Enterprise Agreement with OpenAI and a qualifying use case. Please reach out to our sales team if you are interested. I\u2019ve received a data deletion request, how do I ask OpenAI to delete data? We only retain data sent through the API for up to 30 days for abuse and monitoring purposes. If you would like to delete your account before then, please follow these steps. Does OpenAI have a Data Processing Addendum (DPA)? Yes. Please complete our DPA form to execute our Data Processing Addendum. Can we self-host? We do not offer on-premise hosting. You may purchase dedicated capacity by reaching out to our sales team.","title":"API data usage policies"},{"location":"introduction/","text":"\u4ecb\u7ecd \u00b6 \u6982\u8ff0 \u00b6 OpenAI API \u53ef\u4ee5\u5e94\u7528\u4e8e\u51e0\u4e4e\u4efb\u4f55\u6d89\u53ca\u81ea\u7136\u8bed\u8a00\u6216\u4ee3\u7801\u7406\u89e3\u6216\u751f\u6210\u7684\u4efb\u52a1\u3002 \u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u7cfb\u5217\u4e0d\u540c\u529f\u7387\u7ea7\u522b\u7684\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u7684\u4efb\u52a1\uff0c\u4ee5\u53ca\u5fae\u8c03\u81ea\u5df1\u7684\u81ea\u5b9a\u4e49\u6a21\u578b\u7684\u80fd\u529b\u3002 \u8fd9\u4e9b\u6a21\u578b\u53ef\u7528\u4e8e\u4ece\u5185\u5bb9\u751f\u6210\u5230\u8bed\u4e49\u641c\u7d22\u548c\u5206\u7c7b\u7684\u6240\u6709\u5185\u5bb9\u3002 \u5173\u952e\u6982\u5ff5 \u00b6 \u6211\u4eec\u5efa\u8bae\u5b8c\u6210\u6211\u4eec\u7684\u5feb\u901f\u5165\u95e8\u6559\u7a0b\uff0c\u901a\u8fc7\u5b9e\u9645\u64cd\u4f5c\u548c\u4e92\u52a8\u793a\u4f8b\u4e86\u89e3\u5173\u952e\u6982\u5ff5\u3002 \u5feb\u901f\u5165\u95e8\u6559\u7a0b \u901a\u8fc7\u6784\u5efa\u5feb\u901f\u793a\u4f8b\u5e94\u7528\u7a0b\u5e8f\u8fdb\u884c\u5b66\u4e60\u3002 \u63d0\u793a\u548c\u8865\u5168 \u00b6 \u8865\u5168 \u7aef\u70b9\u662f\u6211\u4eec API \u7684\u6838\u5fc3\u3002 \u5b83\u63d0\u4f9b\u4e86\u4e00\u4e2a\u975e\u5e38\u7075\u6d3b\u548c\u5f3a\u5927\u7684\u6a21\u578b\u63a5\u53e3\u3002 \u60a8\u5c06\u4e00\u4e9b\u6587\u672c\u4f5c\u4e3a\u63d0\u793a\u8f93\u5165\uff0c\u6a21\u578b\u5c06\u751f\u6210\u4e00\u4e2a\u6587\u672c\u5b8c\u6210\uff0c\u8bd5\u56fe\u5339\u914d\u60a8\u7ed9\u5b83\u7684\u4efb\u4f55\u4e0a\u4e0b\u6587\u6216\u6a21\u5f0f\u3002 \u4f8b\u5982\uff0c\u5982\u679c\u60a8\u7ed9 API \u63d0\u4f9b\u63d0\u793a\u201c\u4e3a\u4e00\u4e2a\u51b0\u6dc7\u6dcb\u5e97\u5199\u4e00\u4e2a\u53e3\u53f7\u201d\uff0c\u5b83\u5c06\u8fd4\u56de\u4e00\u4e2a\u5b8c\u6210\uff0c\u5982\u201c\u6211\u4eec\u4e3a\u6bcf\u4e00\u52fa\u51b0\u6fc0\u51cc\u63d0\u4f9b\u7b11\u5bb9\uff01\u201d \u8bbe\u8ba1\u60a8\u7684\u63d0\u793a \u672c\u8d28\u4e0a\u5c31\u662f\u5982\u4f55\u201c\u7f16\u7a0b\u201d\u6a21\u578b\uff0c\u901a\u5e38\u901a\u8fc7\u63d0\u4f9b\u4e00\u4e9b\u8bf4\u660e\u6216\u51e0\u4e2a\u793a\u4f8b\u6765\u5b8c\u6210\u3002 \u8fd9\u4e0e\u5927\u591a\u6570\u5176\u4ed6 NLP \u670d\u52a1\u4e0d\u540c\uff0c\u5b83\u4eec\u4ec5\u8bbe\u8ba1\u7528\u4e8e\u5355\u4e2a\u4efb\u52a1\uff0c\u5982\u60c5\u611f\u5206\u7c7b\u6216\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u3002 \u76f8\u53cd\uff0c\u5b8c\u6210\u7aef\u70b9\u53ef\u7528\u4e8e\u51e0\u4e4e\u4efb\u4f55\u4efb\u52a1\uff0c\u5305\u62ec\u5185\u5bb9\u6216\u4ee3\u7801\u751f\u6210\u3001\u6458\u8981\u3001\u6269\u5c55\u3001\u4f1a\u8bdd\u3001\u521b\u610f\u5199\u4f5c\u3001\u6837\u5f0f\u8f6c\u6362\u7b49\u3002 Tokens \u00b6 \u6211\u4eec\u7684\u6a21\u578b\u901a\u8fc7\u5c06\u6587\u672c\u5206\u89e3\u4e3a\u6807\u8bb0\u6765\u7406\u89e3\u548c\u5904\u7406\u6587\u672c\u3002 \u6807\u8bb0\u53ef\u4ee5\u662f\u5355\u8bcd\u6216\u53ea\u662f\u5b57\u7b26\u5757\u3002 \u4f8b\u5982\uff0c\u201c\u6c49\u5821\u5305\u201d\u8fd9\u4e2a\u8bcd\u88ab\u5206\u89e3\u6210\u201cham\u201d\uff0c\u201cbur\u201d\u548c\u201cger\u201d\u8fd9\u4e9b\u6807\u8bb0\uff0c\u800c\u4e00\u4e2a\u77ed\u800c\u5e38\u89c1\u7684\u5355\u8bcd\u201cpear\u201d\u662f\u4e00\u4e2a\u5355\u72ec\u7684\u6807\u8bb0\u3002 \u8bb8\u591a\u6807\u8bb0\u4ee5\u7a7a\u683c\u5f00\u59cb\uff0c\u4f8b\u5982\u201chello\u201d\u548c\u201cbye\u201d\u3002 \u5728\u7ed9\u5b9a API \u8bf7\u6c42\u4e2d\u5904\u7406\u7684\u6807\u8bb0\u6570\u91cf\u53d6\u51b3\u4e8e\u60a8\u7684\u8f93\u5165\u548c\u8f93\u51fa\u957f\u5ea6\u3002 \u7c97\u7565\u7684\u7ecf\u9a8c\u6cd5\u5219\u662f\uff0c1 \u4e2a\u6807\u8bb0\u5927\u7ea6\u76f8\u5f53\u4e8e 4 \u4e2a\u5b57\u7b26\u6216 0.75 \u4e2a\u5355\u8bcd\uff08\u5bf9\u4e8e\u82f1\u6587\u6587\u672c\uff09\u3002 \u8981\u8bb0\u4f4f\u7684\u4e00\u4e2a\u9650\u5236\u662f\uff0c\u60a8\u7684\u6587\u672c\u63d0\u793a\u548c\u751f\u6210\u7684\u5b8c\u6210\u7ec4\u5408\u5fc5\u987b\u4e0d\u8d85\u8fc7\u6a21\u578b\u7684\u6700\u5927\u4e0a\u4e0b\u6587\u957f\u5ea6\uff08\u5bf9\u4e8e\u5927\u591a\u6570\u6a21\u578b\uff0c\u8fd9\u662f 2048 \u4e2a\u6807\u8bb0\uff0c\u7ea6\u4e3a 1500 \u4e2a\u5355\u8bcd\uff09\u3002 \u67e5\u770b\u6211\u4eec\u7684 \u6807\u8bb0\u5316\u5de5\u5177 \uff0c\u4e86\u89e3\u6709\u5173\u6587\u672c\u5982\u4f55\u8f6c\u6362\u4e3a\u6807\u8bb0\u7684\u66f4\u591a\u4fe1\u606f\u3002 \u6a21\u578b \u00b6 \u4ee5\u4e0b\u4ee3\u7801\u662f API \u6240\u4f9d\u8d56\u7684\u4e00\u7ec4\u5177\u6709\u4e0d\u540c\u80fd\u529b\u548c\u4ef7\u683c\u70b9\u7684\u6a21\u578b\u3002\u6211\u4eec\u7684\u57fa\u7840 GPT-3 \u6a21\u578b\u5305\u62ec Davinci\u3001Curie\u3001Babbage \u548c Ada\u3002\u6211\u4eec\u7684 Codex \u7cfb\u5217\u662f GPT-3 \u7684\u540e\u4ee3\uff0c\u5b83\u7ecf\u8fc7\u4e86\u81ea\u7136\u8bed\u8a00\u548c\u4ee3\u7801\u7684\u8bad\u7ec3\u3002\u4e86\u89e3\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u8bbf\u95ee\u6211\u4eec\u7684 \u6a21\u578b\u6587\u6863 \u3002 \u4e0b\u4e00\u4e2a \u00b6 \u5728\u5f00\u59cb\u6784\u5efa\u5e94\u7528\u7a0b\u5e8f\u65f6\uff0c\u8bf7\u7262\u8bb0\u6211\u4eec\u7684 \u4f7f\u7528\u653f\u7b56 \u3002 \u6d4f\u89c8\u6211\u4eec\u7684 \u793a\u4f8b\u5e93 \uff0c\u83b7\u53d6\u7075\u611f\u3002 \u9605\u8bfb\u6211\u4eec\u7684\u6307\u5357\u4e4b\u4e00\u5f00\u59cb\u6784\u5efa\u3002 \u6307\u5357 \u00b6 \u6587\u672c\u8865\u5168 \uff1a\u5b66\u4e60\u5982\u4f55\u4f7f\u7528\u6211\u4eec\u7684\u6a21\u578b\u751f\u6210\u6216\u7f16\u8f91\u6587\u672c\u3002 \u4ee3\u7801\u8865\u5168 \uff08\u6709\u9650\u6d4b\u8bd5\u7248\uff09 \uff1a\u5b66\u4e60\u5982\u4f55\u751f\u6210\u3001\u7f16\u8f91\u6216\u89e3\u91ca\u4ee3\u7801\u3002 \u56fe\u50cf\u751f\u6210 \uff08\u6d4b\u8bd5\u7248\uff09 \uff1a\u5b66\u4e60\u5982\u4f55\u751f\u6210\u6216\u7f16\u8f91\u56fe\u50cf\u3002 \u5fae\u8c03 \uff1a\u5b66\u4e60\u5982\u4f55\u9488\u5bf9\u60a8\u7684\u7528\u4f8b\u8bad\u7ec3\u6a21\u578b\u3002 \u5d4c\u5165 \uff1a\u5b66\u4e60\u5982\u4f55\u641c\u7d22\u3001\u5206\u7c7b\u548c\u6bd4\u8f83\u6587\u672c\u3002","title":"\u4ecb\u7ecd"},{"location":"introduction/#_1","text":"","title":"\u4ecb\u7ecd"},{"location":"introduction/#_2","text":"OpenAI API \u53ef\u4ee5\u5e94\u7528\u4e8e\u51e0\u4e4e\u4efb\u4f55\u6d89\u53ca\u81ea\u7136\u8bed\u8a00\u6216\u4ee3\u7801\u7406\u89e3\u6216\u751f\u6210\u7684\u4efb\u52a1\u3002 \u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u7cfb\u5217\u4e0d\u540c\u529f\u7387\u7ea7\u522b\u7684\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u7684\u4efb\u52a1\uff0c\u4ee5\u53ca\u5fae\u8c03\u81ea\u5df1\u7684\u81ea\u5b9a\u4e49\u6a21\u578b\u7684\u80fd\u529b\u3002 \u8fd9\u4e9b\u6a21\u578b\u53ef\u7528\u4e8e\u4ece\u5185\u5bb9\u751f\u6210\u5230\u8bed\u4e49\u641c\u7d22\u548c\u5206\u7c7b\u7684\u6240\u6709\u5185\u5bb9\u3002","title":"\u6982\u8ff0"},{"location":"introduction/#_3","text":"\u6211\u4eec\u5efa\u8bae\u5b8c\u6210\u6211\u4eec\u7684\u5feb\u901f\u5165\u95e8\u6559\u7a0b\uff0c\u901a\u8fc7\u5b9e\u9645\u64cd\u4f5c\u548c\u4e92\u52a8\u793a\u4f8b\u4e86\u89e3\u5173\u952e\u6982\u5ff5\u3002 \u5feb\u901f\u5165\u95e8\u6559\u7a0b \u901a\u8fc7\u6784\u5efa\u5feb\u901f\u793a\u4f8b\u5e94\u7528\u7a0b\u5e8f\u8fdb\u884c\u5b66\u4e60\u3002","title":"\u5173\u952e\u6982\u5ff5"},{"location":"introduction/#_4","text":"\u8865\u5168 \u7aef\u70b9\u662f\u6211\u4eec API \u7684\u6838\u5fc3\u3002 \u5b83\u63d0\u4f9b\u4e86\u4e00\u4e2a\u975e\u5e38\u7075\u6d3b\u548c\u5f3a\u5927\u7684\u6a21\u578b\u63a5\u53e3\u3002 \u60a8\u5c06\u4e00\u4e9b\u6587\u672c\u4f5c\u4e3a\u63d0\u793a\u8f93\u5165\uff0c\u6a21\u578b\u5c06\u751f\u6210\u4e00\u4e2a\u6587\u672c\u5b8c\u6210\uff0c\u8bd5\u56fe\u5339\u914d\u60a8\u7ed9\u5b83\u7684\u4efb\u4f55\u4e0a\u4e0b\u6587\u6216\u6a21\u5f0f\u3002 \u4f8b\u5982\uff0c\u5982\u679c\u60a8\u7ed9 API \u63d0\u4f9b\u63d0\u793a\u201c\u4e3a\u4e00\u4e2a\u51b0\u6dc7\u6dcb\u5e97\u5199\u4e00\u4e2a\u53e3\u53f7\u201d\uff0c\u5b83\u5c06\u8fd4\u56de\u4e00\u4e2a\u5b8c\u6210\uff0c\u5982\u201c\u6211\u4eec\u4e3a\u6bcf\u4e00\u52fa\u51b0\u6fc0\u51cc\u63d0\u4f9b\u7b11\u5bb9\uff01\u201d \u8bbe\u8ba1\u60a8\u7684\u63d0\u793a \u672c\u8d28\u4e0a\u5c31\u662f\u5982\u4f55\u201c\u7f16\u7a0b\u201d\u6a21\u578b\uff0c\u901a\u5e38\u901a\u8fc7\u63d0\u4f9b\u4e00\u4e9b\u8bf4\u660e\u6216\u51e0\u4e2a\u793a\u4f8b\u6765\u5b8c\u6210\u3002 \u8fd9\u4e0e\u5927\u591a\u6570\u5176\u4ed6 NLP \u670d\u52a1\u4e0d\u540c\uff0c\u5b83\u4eec\u4ec5\u8bbe\u8ba1\u7528\u4e8e\u5355\u4e2a\u4efb\u52a1\uff0c\u5982\u60c5\u611f\u5206\u7c7b\u6216\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u3002 \u76f8\u53cd\uff0c\u5b8c\u6210\u7aef\u70b9\u53ef\u7528\u4e8e\u51e0\u4e4e\u4efb\u4f55\u4efb\u52a1\uff0c\u5305\u62ec\u5185\u5bb9\u6216\u4ee3\u7801\u751f\u6210\u3001\u6458\u8981\u3001\u6269\u5c55\u3001\u4f1a\u8bdd\u3001\u521b\u610f\u5199\u4f5c\u3001\u6837\u5f0f\u8f6c\u6362\u7b49\u3002","title":"\u63d0\u793a\u548c\u8865\u5168"},{"location":"introduction/#tokens","text":"\u6211\u4eec\u7684\u6a21\u578b\u901a\u8fc7\u5c06\u6587\u672c\u5206\u89e3\u4e3a\u6807\u8bb0\u6765\u7406\u89e3\u548c\u5904\u7406\u6587\u672c\u3002 \u6807\u8bb0\u53ef\u4ee5\u662f\u5355\u8bcd\u6216\u53ea\u662f\u5b57\u7b26\u5757\u3002 \u4f8b\u5982\uff0c\u201c\u6c49\u5821\u5305\u201d\u8fd9\u4e2a\u8bcd\u88ab\u5206\u89e3\u6210\u201cham\u201d\uff0c\u201cbur\u201d\u548c\u201cger\u201d\u8fd9\u4e9b\u6807\u8bb0\uff0c\u800c\u4e00\u4e2a\u77ed\u800c\u5e38\u89c1\u7684\u5355\u8bcd\u201cpear\u201d\u662f\u4e00\u4e2a\u5355\u72ec\u7684\u6807\u8bb0\u3002 \u8bb8\u591a\u6807\u8bb0\u4ee5\u7a7a\u683c\u5f00\u59cb\uff0c\u4f8b\u5982\u201chello\u201d\u548c\u201cbye\u201d\u3002 \u5728\u7ed9\u5b9a API \u8bf7\u6c42\u4e2d\u5904\u7406\u7684\u6807\u8bb0\u6570\u91cf\u53d6\u51b3\u4e8e\u60a8\u7684\u8f93\u5165\u548c\u8f93\u51fa\u957f\u5ea6\u3002 \u7c97\u7565\u7684\u7ecf\u9a8c\u6cd5\u5219\u662f\uff0c1 \u4e2a\u6807\u8bb0\u5927\u7ea6\u76f8\u5f53\u4e8e 4 \u4e2a\u5b57\u7b26\u6216 0.75 \u4e2a\u5355\u8bcd\uff08\u5bf9\u4e8e\u82f1\u6587\u6587\u672c\uff09\u3002 \u8981\u8bb0\u4f4f\u7684\u4e00\u4e2a\u9650\u5236\u662f\uff0c\u60a8\u7684\u6587\u672c\u63d0\u793a\u548c\u751f\u6210\u7684\u5b8c\u6210\u7ec4\u5408\u5fc5\u987b\u4e0d\u8d85\u8fc7\u6a21\u578b\u7684\u6700\u5927\u4e0a\u4e0b\u6587\u957f\u5ea6\uff08\u5bf9\u4e8e\u5927\u591a\u6570\u6a21\u578b\uff0c\u8fd9\u662f 2048 \u4e2a\u6807\u8bb0\uff0c\u7ea6\u4e3a 1500 \u4e2a\u5355\u8bcd\uff09\u3002 \u67e5\u770b\u6211\u4eec\u7684 \u6807\u8bb0\u5316\u5de5\u5177 \uff0c\u4e86\u89e3\u6709\u5173\u6587\u672c\u5982\u4f55\u8f6c\u6362\u4e3a\u6807\u8bb0\u7684\u66f4\u591a\u4fe1\u606f\u3002","title":"Tokens"},{"location":"introduction/#_5","text":"\u4ee5\u4e0b\u4ee3\u7801\u662f API \u6240\u4f9d\u8d56\u7684\u4e00\u7ec4\u5177\u6709\u4e0d\u540c\u80fd\u529b\u548c\u4ef7\u683c\u70b9\u7684\u6a21\u578b\u3002\u6211\u4eec\u7684\u57fa\u7840 GPT-3 \u6a21\u578b\u5305\u62ec Davinci\u3001Curie\u3001Babbage \u548c Ada\u3002\u6211\u4eec\u7684 Codex \u7cfb\u5217\u662f GPT-3 \u7684\u540e\u4ee3\uff0c\u5b83\u7ecf\u8fc7\u4e86\u81ea\u7136\u8bed\u8a00\u548c\u4ee3\u7801\u7684\u8bad\u7ec3\u3002\u4e86\u89e3\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u8bbf\u95ee\u6211\u4eec\u7684 \u6a21\u578b\u6587\u6863 \u3002","title":"\u6a21\u578b"},{"location":"introduction/#_6","text":"\u5728\u5f00\u59cb\u6784\u5efa\u5e94\u7528\u7a0b\u5e8f\u65f6\uff0c\u8bf7\u7262\u8bb0\u6211\u4eec\u7684 \u4f7f\u7528\u653f\u7b56 \u3002 \u6d4f\u89c8\u6211\u4eec\u7684 \u793a\u4f8b\u5e93 \uff0c\u83b7\u53d6\u7075\u611f\u3002 \u9605\u8bfb\u6211\u4eec\u7684\u6307\u5357\u4e4b\u4e00\u5f00\u59cb\u6784\u5efa\u3002","title":"\u4e0b\u4e00\u4e2a"},{"location":"introduction/#_7","text":"\u6587\u672c\u8865\u5168 \uff1a\u5b66\u4e60\u5982\u4f55\u4f7f\u7528\u6211\u4eec\u7684\u6a21\u578b\u751f\u6210\u6216\u7f16\u8f91\u6587\u672c\u3002 \u4ee3\u7801\u8865\u5168 \uff08\u6709\u9650\u6d4b\u8bd5\u7248\uff09 \uff1a\u5b66\u4e60\u5982\u4f55\u751f\u6210\u3001\u7f16\u8f91\u6216\u89e3\u91ca\u4ee3\u7801\u3002 \u56fe\u50cf\u751f\u6210 \uff08\u6d4b\u8bd5\u7248\uff09 \uff1a\u5b66\u4e60\u5982\u4f55\u751f\u6210\u6216\u7f16\u8f91\u56fe\u50cf\u3002 \u5fae\u8c03 \uff1a\u5b66\u4e60\u5982\u4f55\u9488\u5bf9\u60a8\u7684\u7528\u4f8b\u8bad\u7ec3\u6a21\u578b\u3002 \u5d4c\u5165 \uff1a\u5b66\u4e60\u5982\u4f55\u641c\u7d22\u3001\u5206\u7c7b\u548c\u6bd4\u8f83\u6587\u672c\u3002","title":"\u6307\u5357"},{"location":"libraries/","text":"\u5e93 \u00b6 Python\u5e93 \u00b6 \u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e2aPython\u5e93\uff0c\u60a8\u53ef\u4ee5\u6309\u5982\u4e0b\u65b9\u5f0f\u5b89\u88c5: 1 $ pip install openai \u5b89\u88c5\u5b8c\u6210\u540e\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u7ed1\u5b9a\u548c\u5bc6\u94a5\u6765\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: 1 2 3 4 5 6 import os import openai # Load your API key from an environment variable or secret management service openai . api_key = os . getenv ( \"OPENAI_API_KEY\" ) response = openai . Completion . create ( model = \"text-davinci-003\" , prompt = \"Say this is a test\" , temperature = 0 , max_tokens = 7 ) \u7ed1\u5b9a\u8fd8\u5c06\u5b89\u88c5\u4e00\u4e2a\u547d\u4ee4\u884c\u5b9e\u7528\u7a0b\u5e8f\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u5982\u4e0b: 1 $ openai api completions . create - m text - davinci - 003 - p \"Say this is a test\" - t 0 - M 7 -- stream Node.js\u5e93 \u00b6 \u6211\u4eec\u8fd8\u6709\u4e00\u4e2aNode.js\u5e93\uff0c\u4f60\u53ef\u4ee5\u5728\u4f60\u7684Node.js\u9879\u76ee\u76ee\u5f55\u4e0b\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6765\u5b89\u88c5\u5b83: 1 npm install openai \u5b89\u88c5\u540e\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u5e93\u548c\u5bc6\u94a5\u6765\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: 1 2 3 4 5 6 7 8 9 10 11 const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . createCompletion ({ model : \"text-davinci-003\" , prompt : \"Say this is a test\" , temperature : 0 , max_tokens : 7 , }); \u793e\u533a\u5e93 \u00b6 \u4e0b\u9762\u7684\u5e93\u7531\u66f4\u5e7f\u6cdb\u7684\u5f00\u53d1\u4eba\u5458\u793e\u533a\u6784\u5efa\u548c\u7ef4\u62a4\u3002\u5982\u679c\u60a8\u60f3\u5728\u8fd9\u91cc\u6dfb\u52a0\u4e00\u4e2a\u65b0\u5e93\uff0c\u8bf7\u6309\u7167\u6211\u4eec\u5e2e\u52a9\u4e2d\u5fc3\u6587\u7ae0\u4e2d\u5173\u4e8e\u6dfb\u52a0\u793e\u533a\u5e93\u7684\u8bf4\u660e\u8fdb\u884c\u64cd\u4f5c\u3002 \u8bf7\u6ce8\u610f\uff0cOpenAI\u4e0d\u4f1a\u9a8c\u8bc1\u8fd9\u4e9b\u9879\u76ee\u7684\u6b63\u786e\u6027\u6216\u5b89\u5168\u6027\u3002 C# / .NET Betalgo.OpenAI.GPT3 by Betalgo Crystal openai-crystal by sferik Go go-gpt3 by sashabaranov Java openai-java by Theo Kanning Kotlin openai-kotlin by Mouaad Aallam Node.js openai-api by Njerschow openai-api-node by erlapso gpt-x by ceifa gpt3 by poteat gpts by thencc dalenguyen/openai by dalenguyen tectalic/openai by tectalic PHP orhanerday/open-ai by orhanerday tectalic/openai by tectalic Python chronology by OthersideAI R rgpt3 by ben-aaron188 Ruby openai by nileshtrivedi ruby-openai by alexrudall Scala openai-scala-client by cequence-io Swift OpenAIKit by dylanshine Unity OpenAi-Api-Unity by hexthedev Unreal Engine OpenAI-Api-Unreal by KellanM","title":"\u5e93"},{"location":"libraries/#_1","text":"","title":"\u5e93"},{"location":"libraries/#python","text":"\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e2aPython\u5e93\uff0c\u60a8\u53ef\u4ee5\u6309\u5982\u4e0b\u65b9\u5f0f\u5b89\u88c5: 1 $ pip install openai \u5b89\u88c5\u5b8c\u6210\u540e\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u7ed1\u5b9a\u548c\u5bc6\u94a5\u6765\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: 1 2 3 4 5 6 import os import openai # Load your API key from an environment variable or secret management service openai . api_key = os . getenv ( \"OPENAI_API_KEY\" ) response = openai . Completion . create ( model = \"text-davinci-003\" , prompt = \"Say this is a test\" , temperature = 0 , max_tokens = 7 ) \u7ed1\u5b9a\u8fd8\u5c06\u5b89\u88c5\u4e00\u4e2a\u547d\u4ee4\u884c\u5b9e\u7528\u7a0b\u5e8f\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u5982\u4e0b: 1 $ openai api completions . create - m text - davinci - 003 - p \"Say this is a test\" - t 0 - M 7 -- stream","title":"Python\u5e93"},{"location":"libraries/#nodejs","text":"\u6211\u4eec\u8fd8\u6709\u4e00\u4e2aNode.js\u5e93\uff0c\u4f60\u53ef\u4ee5\u5728\u4f60\u7684Node.js\u9879\u76ee\u76ee\u5f55\u4e0b\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6765\u5b89\u88c5\u5b83: 1 npm install openai \u5b89\u88c5\u540e\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u5e93\u548c\u5bc6\u94a5\u6765\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: 1 2 3 4 5 6 7 8 9 10 11 const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . createCompletion ({ model : \"text-davinci-003\" , prompt : \"Say this is a test\" , temperature : 0 , max_tokens : 7 , });","title":"Node.js\u5e93"},{"location":"libraries/#_2","text":"\u4e0b\u9762\u7684\u5e93\u7531\u66f4\u5e7f\u6cdb\u7684\u5f00\u53d1\u4eba\u5458\u793e\u533a\u6784\u5efa\u548c\u7ef4\u62a4\u3002\u5982\u679c\u60a8\u60f3\u5728\u8fd9\u91cc\u6dfb\u52a0\u4e00\u4e2a\u65b0\u5e93\uff0c\u8bf7\u6309\u7167\u6211\u4eec\u5e2e\u52a9\u4e2d\u5fc3\u6587\u7ae0\u4e2d\u5173\u4e8e\u6dfb\u52a0\u793e\u533a\u5e93\u7684\u8bf4\u660e\u8fdb\u884c\u64cd\u4f5c\u3002 \u8bf7\u6ce8\u610f\uff0cOpenAI\u4e0d\u4f1a\u9a8c\u8bc1\u8fd9\u4e9b\u9879\u76ee\u7684\u6b63\u786e\u6027\u6216\u5b89\u5168\u6027\u3002 C# / .NET Betalgo.OpenAI.GPT3 by Betalgo Crystal openai-crystal by sferik Go go-gpt3 by sashabaranov Java openai-java by Theo Kanning Kotlin openai-kotlin by Mouaad Aallam Node.js openai-api by Njerschow openai-api-node by erlapso gpt-x by ceifa gpt3 by poteat gpts by thencc dalenguyen/openai by dalenguyen tectalic/openai by tectalic PHP orhanerday/open-ai by orhanerday tectalic/openai by tectalic Python chronology by OthersideAI R rgpt3 by ben-aaron188 Ruby openai by nileshtrivedi ruby-openai by alexrudall Scala openai-scala-client by cequence-io Swift OpenAIKit by dylanshine Unity OpenAi-Api-Unity by hexthedev Unreal Engine OpenAI-Api-Unreal by KellanM","title":"\u793e\u533a\u5e93"},{"location":"models/","text":"\u6a21\u578b \u00b6 \u6982\u8ff0 \u00b6 OpenAI API \u7531\u4e00\u7ec4\u5177\u6709\u4e0d\u540c\u529f\u80fd\u548c\u4ef7\u683c\u70b9\u7684\u4e0d\u540c\u6a21\u578b\u63d0\u4f9b\u652f\u6301\u3002\u60a8\u8fd8\u53ef\u4ee5\u901a\u8fc7\u5fae\u8c03\u5bf9\u6211\u4eec\u7684\u539f\u59cb\u57fa\u672c\u6a21\u578b\u8fdb\u884c\u6709\u9650\u7684\u5b9a\u5236\u3002 MODELS DESCRIPTION GPT-4 Limited beta \u4e00\u7ec4\u6539\u8fdb GPT-3.5 \u7684\u6a21\u578b\uff0c\u53ef\u4ee5\u7406\u89e3\u548c\u751f\u6210\u81ea\u7136\u8bed\u8a00\u6216\u4ee3\u7801 GPT-3.5 \u4e00\u7ec4\u6539\u8fdb GPT-3 \u7684\u6a21\u578b\uff0c\u53ef\u4ee5\u7406\u89e3\u548c\u751f\u6210\u81ea\u7136\u8bed\u8a00\u6216\u4ee3\u7801 DALL\u00b7E \u4e00\u4e2a\u6a21\u578b\uff0c\u53ef\u4ee5\u751f\u6210\u548c\u7f16\u8f91\u56fe\u50cf\u7ed9\u5b9a\u7684\u81ea\u7136\u8bed\u8a00\u63d0\u793a Whisper \u53ef\u4ee5\u5c06\u97f3\u9891\u8f6c\u6362\u4e3a\u6587\u672c\u7684\u6a21\u578b Embeddings \u4e00\u7ec4\u53ef\u4ee5\u5c06\u6587\u672c\u8f6c\u6362\u4e3a\u6570\u503c\u5f62\u5f0f\u7684\u6a21\u578b Codex Limited beta \u4e00\u7ec4\u80fd\u591f\u7406\u89e3\u548c\u751f\u6210\u4ee3\u7801\u7684\u6a21\u578b\uff0c\u5305\u62ec\u5c06\u81ea\u7136\u8bed\u8a00\u8f6c\u6362\u4e3a\u4ee3\u7801 Moderation \u4e00\u4e2a\u7ecf\u8fc7\u5fae\u8c03\u7684\u6a21\u578b\uff0c\u53ef\u4ee5\u68c0\u6d4b\u6587\u672c\u662f\u5426\u654f\u611f\u6216\u4e0d\u5b89\u5168 GPT-3 \u4e00\u7ec4\u80fd\u591f\u7406\u89e3\u548c\u751f\u6210\u81ea\u7136\u8bed\u8a00\u7684\u6a21\u578b \u6211\u4eec\u8fd8\u53d1\u5e03\u4e86\u5305\u62ec Point-E \u3001 Whisper \u3001 Jukebox \u548c CLIP \u5728\u5185\u7684\u5f00\u6e90\u6a21\u578b\u3002 \u8bbf\u95ee\u6211\u4eec\u7684 \u6a21\u578b\u7d22\u5f15 \uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u4e86\u89e3\u66f4\u591a\u5173\u4e8e\u54ea\u4e9b\u6a21\u578b\u5728\u6211\u4eec\u7684\u7814\u7a76\u8bba\u6587\u4e2d\u6709\u7279\u8272\uff0c\u4ee5\u53ca\u50cf InstructGPT \u548c GPT-3.5 \u8fd9\u6837\u7684\u6a21\u578b\u7cfb\u5217\u4e4b\u95f4\u7684\u5dee\u5f02\u3002 GPT-4 Limited beta \u00b6 GPT-4 \u662f\u4e00\u4e2a\u5927\u578b\u7684\u591a\u6a21\u6001\u6a21\u578b(\u76ee\u524d\u63a5\u53d7\u6587\u672c\u8f93\u5165\u5e76\u8f93\u51fa\u6587\u672c\uff0c\u672a\u6765\u5c06\u6709\u56fe\u50cf\u8f93\u5165)\uff0c\u7531\u4e8e\u5176\u66f4\u5e7f\u6cdb\u7684\u5e38\u8bc6\u548c\u5148\u8fdb\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5b83\u53ef\u4ee5\u6bd4\u6211\u4eec\u4e4b\u524d\u7684\u4efb\u4f55\u6a21\u578b\u66f4\u51c6\u786e\u5730\u89e3\u51b3\u96be\u9898\u3002\u4e0e gpt-3.5-turbo \u4e00\u6837\uff0cGPT-4 \u9488\u5bf9\u804a\u5929\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u4f46\u4e5f\u9002\u7528\u4e8e\u4f20\u7edf\u7684\u5b8c\u6210\u4efb\u52a1\u3002 GPT-4 \u76ee\u524d\u5904\u4e8e\u6709\u9650\u7684\u6d4b\u8bd5\u9636\u6bb5\uff0c\u53ea\u6709\u88ab\u6388\u4e88\u8bbf\u95ee\u6743\u9650\u7684\u4eba\u624d\u80fd\u8bbf\u95ee\u3002\u5f53\u5bb9\u91cf\u53ef\u7528\u65f6\uff0c\u8bf7\u52a0\u5165\u7b49\u5f85\u5217\u8868\u4ee5\u83b7\u5f97\u8bbf\u95ee\u3002 LATEST MODEL DESCRIPTION MAX TOKENS TRAINING DATA gpt-4 \u6bd4\u4efb\u4f55 GPT-3.5 \u6a21\u578b\u66f4\u5f3a\u5927\uff0c\u80fd\u591f\u5b8c\u6210\u66f4\u590d\u6742\u7684\u4efb\u52a1\uff0c\u5e76\u4e3a\u804a\u5929\u8fdb\u884c\u4e86\u4f18\u5316\u3002\u5c06\u4e0e\u6211\u4eec\u6700\u65b0\u7684\u6a21\u578b\u8fed\u4ee3\u66f4\u65b0\u3002 8,192 tokens Up to Sep 2021 gpt-4-0314 2023 \u5e74 3 \u6708 14 \u65e5\u7684 gpt-4 \u5feb\u7167\u3002\u4e0e gpt-4 \u4e0d\u540c\u7684\u662f\uff0c\u8be5\u6a21\u578b\u5c06\u4e0d\u4f1a\u63a5\u53d7\u66f4\u65b0\uff0c\u5e76\u4e14\u53ea\u4f1a\u5728 2023 \u5e74 6 \u6708 14 \u65e5\u7ed3\u675f\u7684\u4e09\u4e2a\u6708\u5185\u5f97\u5230\u652f\u6301\u3002 8,192 tokens Up to Sep 2021 gpt-4-32k \u4e0e\u57fa\u7840 gpt-4 \u6a21\u5f0f\u76f8\u540c\u7684\u529f\u80fd\uff0c\u4f46\u4e0a\u4e0b\u6587\u957f\u5ea6\u662f\u5b83\u7684 4 \u500d\u3002\u5c06\u4e0e\u6211\u4eec\u6700\u65b0\u7684\u6a21\u578b\u8fed\u4ee3\u66f4\u65b0\u3002 32,768 tokens Up to Sep 2021 gpt-4-32k-0314 2023 \u5e74 3 \u6708 14 \u65e5 gpt-4-32 \u7684\u5feb\u7167\u3002\u4e0e gpt-4-32k \u4e0d\u540c\u7684\u662f\uff0c\u8be5\u578b\u53f7\u5c06\u4e0d\u63a5\u53d7\u66f4\u65b0\uff0c\u5e76\u4e14\u53ea\u652f\u6301\u4e09\u4e2a\u6708\u7684\u65f6\u95f4\uff0c\u622a\u6b62 2023 \u5e74 6 \u6708 14 \u65e5\u3002 32,768 tokens Up to Sep 2021 \u5bf9\u4e8e\u8bb8\u591a\u57fa\u672c\u4efb\u52a1\uff0cGPT-4 \u548c GPT-3.5 \u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u5f02\u5e76\u4e0d\u663e\u8457\u3002\u7136\u800c\uff0c\u5728\u66f4\u590d\u6742\u7684\u63a8\u7406\u60c5\u51b5\u4e0b\uff0cGPT-4 \u6bd4\u6211\u4eec\u4e4b\u524d\u7684\u4efb\u4f55\u6a21\u578b\u90fd\u66f4\u6709\u80fd\u529b\u3002 GPT-3.5 \u00b6 GPT-3.5 \u6a21\u578b\u53ef\u4ee5\u7406\u89e3\u548c\u751f\u6210\u81ea\u7136\u8bed\u8a00\u6216\u4ee3\u7801\u3002\u6211\u4eec\u6700\u6709\u80fd\u529b\u548c\u6700\u5177\u6210\u672c\u6548\u76ca\u7684\u6a21\u578b\u662f gpt-3.5-turbo\uff0c\u5b83\u9488\u5bf9\u804a\u5929\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u4f46\u4e5f\u9002\u7528\u4e8e\u4f20\u7edf\u7684\u5b8c\u6210\u4efb\u52a1\u3002 LATEST MODEL DESCRIPTION MAX REQUEST TRAINING DATA gpt-3.5-turbo \u529f\u80fd\u6700\u5f3a\u5927\u7684 GPT-3.5 \u6a21\u578b\u548c\u804a\u5929\u4f18\u5316\uff0c\u6210\u672c\u4e3a\u6587\u672c davinci-003 \u7684\u5341\u5206\u4e4b\u4e00\u3002\u5c06\u4e0e\u6211\u4eec\u6700\u65b0\u7684\u6a21\u578b\u8fed\u4ee3\u66f4\u65b0\u3002 4,096 tokens Up to Sep 2021 gpt-3.5-turbo-0301 2023 \u5e74 3 \u6708 1 \u65e5 gpt-3.5-turbo \u7684\u5feb\u7167\u3002\u4e0e gpt-3.5-turbo \u4e0d\u540c\u7684\u662f\uff0c\u8be5\u578b\u53f7\u5c06\u4e0d\u63a5\u53d7\u66f4\u65b0\uff0c\u5e76\u4e14\u53ea\u652f\u6301\u4e09\u4e2a\u6708\u7684\u65f6\u95f4\uff0c\u622a\u6b62\u5230 2023 \u5e74 6 \u6708 1 \u65e5\u3002 4,096 tokens Up to Sep 2021 text-davinci-003 \u4e0e curie, babbage \u6216 ada \u6a21\u578b\u76f8\u6bd4\uff0c\u53ef\u4ee5\u4ee5\u66f4\u597d\u7684\u8d28\u91cf\uff0c\u66f4\u957f\u7684\u8f93\u51fa\u548c\u4e00\u81f4\u7684\u6307\u4ee4\u9075\u5faa\u6765\u5b8c\u6210\u4efb\u4f55\u8bed\u8a00\u4efb\u52a1\u3002\u8fd8\u652f\u6301\u5728\u6587\u672c\u4e2d\u63d2\u5165\u8865\u5168\u3002 4,000 tokens Up to Jun 2021 text-davinci-002 \u4e0e text-davinci-003 \u529f\u80fd\u7c7b\u4f3c\uff0c\u4f46\u4f7f\u7528\u76d1\u7763\u5fae\u8c03\u800c\u4e0d\u662f\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u8bad\u7ec3 4,000 tokens Up to Jun 2021 code-davinci-002 \u4f18\u5316\u4ee3\u7801\u5b8c\u6210\u4efb\u52a1 4,000 tokens Up to Jun 2021 \u6211\u4eec\u5efa\u8bae\u5728\u5b9e\u9a8c\u65f6\u4f7f\u7528 gpt-3.5-turbo\uff0c\u56e0\u4e3a\u5b83\u4f1a\u4ea7\u751f\u6700\u597d\u7684\u7ed3\u679c\u3002\u4e00\u65e6\u4f60\u6709\u4e86\u5de5\u4f5c\uff0c\u6211\u4eec\u9f13\u52b1\u5c1d\u8bd5\u5176\u4ed6\u6a21\u578b\uff0c\u770b\u770b\u4f60\u662f\u5426\u80fd\u4ee5\u66f4\u4f4e\u7684\u5ef6\u8fdf\u6216\u6210\u672c\u83b7\u5f97\u76f8\u540c\u7684\u7ed3\u679c\u3002 Note OpenAI\u6a21\u578b\u662f\u975e\u786e\u5b9a\u6027\u7684\uff0c\u8fd9\u610f\u5473\u7740\u76f8\u540c\u7684\u8f93\u5165\u53ef\u4ee5\u4ea7\u751f\u4e0d\u540c\u7684\u8f93\u51fa\u3002\u5c06\u6e29\u5ea6\u8bbe\u7f6e\u4e3a0\u5c06\u4f7f\u8f93\u51fa\u5927\u90e8\u5206\u662f\u786e\u5b9a\u7684\uff0c\u4f46\u53ef\u80fd\u4f1a\u4fdd\u7559\u5c11\u91cf\u7684\u53ef\u53d8\u6027\u3002 Feature-specific \u58a8\u9999 \u00b6 \u867d\u7136\u65b0\u7684 gpt-3.5-turbo \u6a21\u578b\u9488\u5bf9\u804a\u5929\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u4f46\u5b83\u5bf9\u4f20\u7edf\u7684\u5b8c\u6210\u4efb\u52a1\u975e\u5e38\u6709\u6548\u3002\u539f\u59cb\u7684 GPT-3.5 \u6a21\u578b\u9488\u5bf9\u6587\u672c\u8865\u5168\u8fdb\u884c\u4e86\u4f18\u5316\u3002 \u6211\u4eec\u7528\u4e8e \u521b\u5efa\u5d4c\u5165 \u548c \u7f16\u8f91\u6587\u672c \u7684\u7aef\u70b9\u4f7f\u7528\u5b83\u4eec\u81ea\u5df1\u7684\u4e13\u7528\u6a21\u578b\u96c6\u3002 \u627e\u5230\u6b63\u786e\u7684\u6a21\u5f0f \u00b6 \u8bd5\u9a8c gpt-3.5-turbo \u662f\u4e86\u89e3 API \u529f\u80fd\u7684\u597d\u65b9\u6cd5\u3002 \u5728\u4f60\u77e5\u9053\u4f60\u60f3\u8981\u5b8c\u6210\u4ec0\u4e48\u4e4b\u540e\uff0c\u4f60\u53ef\u4ee5\u7ee7\u7eed\u4f7f\u7528 gpt-3.5-turbo \u6216\u5176\u4ed6\u6a21\u578b\uff0c\u5e76\u5c1d\u8bd5\u56f4\u7ed5\u5b83\u7684\u529f\u80fd\u8fdb\u884c\u4f18\u5316\u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528 GPT \u6bd4\u8f83\u5de5\u5177 \uff0c\u8be5\u5de5\u5177\u5141\u8bb8\u60a8\u5e76\u6392\u8fd0\u884c\u4e0d\u540c\u7684\u6a21\u578b\u6765\u6bd4\u8f83\u8f93\u51fa\u3001\u8bbe\u7f6e\u548c\u54cd\u5e94\u65f6\u95f4\uff0c\u7136\u540e\u5c06\u6570\u636e\u4e0b\u8f7d\u5230 Excel \u7535\u5b50\u8868\u683c\u4e2d\u3002 Turbo \u00b6 Turbo \u662f\u652f\u6301 ChatGPT \u7684\u540c\u4e00\u4e2a\u6a21\u578b\u5bb6\u65cf\u3002 \u5b83\u9488\u5bf9\u4f1a\u8bdd\u804a\u5929\u8f93\u5165\u548c\u8f93\u51fa\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u4f46\u4e0e\u8fbe\u82ac\u5947\u6a21\u578b\u5bb6\u65cf\u76f8\u6bd4\uff0c\u5b83\u5728\u8865\u5168\u65b9\u9762\u8868\u73b0\u540c\u6837\u51fa\u8272\u3002 \u4efb\u4f55\u5728 ChatGPT \u4e2d\u53ef\u4ee5\u5f88\u597d\u5730\u5b8c\u6210\u7684\u7528\u4f8b\u90fd\u5e94\u8be5\u5728 API \u4e2d\u7684 Turbo \u6a21\u578b\u5bb6\u65cf\u4e2d\u8868\u73b0\u826f\u597d\u3002 Turbo \u6a21\u578b\u5bb6\u65cf\u4e5f\u662f\u7b2c\u4e00\u4e2a\u63a5\u53d7\u5b9a\u671f\u66f4\u65b0\u7684\u6a21\u578b\uff0c\u5982 ChatGPT\u3002 \u64c5\u957f:\u4f1a\u8bdd\u548c\u6587\u672c\u751f\u6210 Davinci \u00b6 Davinci is the most capable model family and can perform any task the other models (ada, curie, and babbage) can perform and often with less instruction. For applications requiring a lot of understanding of the content, like summarization for a specific audience and creative content generation, Davinci will produce the best results. These increased capabilities require more compute resources, so Davinci costs more per API call and is not as fast as the other models. Another area where Davinci shines is in understanding the intent of text. Davinci is quite good at solving many kinds of logic problems and explaining the motives of characters. Davinci has been able to solve some of the most challenging AI problems involving cause and effect. Good at: Complex intent, cause and effect, summarization for audience Curie \u00b6 Curie is extremely powerful, yet very fast. While Davinci is stronger when it comes to analyzing complicated text, Curie is quite capable for many nuanced tasks like sentiment classification and summarization. Curie is also quite good at answering questions and performing Q&A and as a general service chatbot. Good at: Language translation, complex classification, text sentiment, summarization Babbage \u00b6 Babbage can perform straightforward tasks like simple classification. It\u2019s also quite capable when it comes to Semantic Search ranking how well documents match up with search queries. Good at: Moderate classification, semantic search classification Ada \u00b6 Ada is usually the fastest model and can perform tasks like parsing text, address correction and certain kinds of classification tasks that don\u2019t require too much nuance. Ada\u2019s performance can often be improved by providing more context. Good at: Parsing text, simple classification, address correction, keywords Note: Any task performed by a faster model like Ada can be performed by a more powerful model like Curie or Davinci. DALL\u00b7E \u00b6 DALL\u00b7E is a AI system that can create realistic images and art from a description in natural language. We currently support the ability, given a prommpt, to create a new image with a certain size, edit an existing image, or create variations of a user provided image. The current DALL\u00b7E model available through our API is the 2 nd iteration of DALL\u00b7E with more realistic, accurate, and 4x greater resolution images than the original model. You can try it through the our Labs interface or via the API. Whisper \u00b6 Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multi-task model that can perform multilingual speech recognition as well as speech translation and language identification. The Whisper v2-large model is currently available through our API with the whisper-1 model name. Currently, there is no difference between the open source version of Whisper and the version available through our API. However, through our API, we offer an optimized inference process which makes running Whisper through our API much faster than doing it through other means. For more technical details on Whisper, you can read the paper. Embeddings \u00b6 \u5d4c\u5165\u662f\u4e00\u79cd\u6587\u672c\u7684\u6570\u5b57\u8868\u793a\uff0c\u53ef\u7528\u4e8e\u6d4b\u91cf\u4e24\u6bb5\u6587\u672c\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002 \u6211\u4eec\u7684\u7b2c\u4e8c\u4ee3\u5d4c\u5165\u6a21\u578b\uff0ctext-embedding-ada-002 \u662f\u4e00\u79cd\u8bbe\u8ba1\u6765\u53d6\u4ee3\u4e4b\u524d\u7684 16 \u4e2a\u7b2c\u4e00\u4ee3\u5d4c\u5165\u6a21\u578b\u7684\u6210\u672c\u7684\u4e00\u5c0f\u90e8\u5206\u3002 \u5d4c\u5165\u5bf9\u4e8e\u641c\u7d22\u3001\u805a\u7c7b\u3001\u63a8\u8350\u3001\u5f02\u5e38\u68c0\u6d4b\u548c\u5206\u7c7b\u4efb\u52a1\u975e\u5e38\u6709\u7528\u3002 \u60a8\u53ef\u4ee5\u5728\u516c\u544a\u535a\u5ba2\u6587\u7ae0\u4e2d\u9605\u8bfb\u66f4\u591a\u5173\u4e8e\u6211\u4eec\u6700\u65b0\u7684\u5d4c\u5165\u6a21\u578b\u3002 Codex Limited beta \u00b6 Codex \u6a21\u578b\u662f GPT-3 \u6a21\u578b\u7684\u540e\u4ee3\uff0c\u53ef\u4ee5\u7406\u89e3\u548c\u751f\u6210\u4ee3\u7801\u3002 \u4ed6\u4eec\u7684\u8bad\u7ec3\u6570\u636e\u65e2\u5305\u542b\u81ea\u7136\u8bed\u8a00\uff0c\u4e5f\u5305\u542b\u6765\u81ea GitHub \u7684\u6570\u5341\u4ebf\u884c\u516c\u5171\u4ee3\u7801\u3002\u5b66\u4e60\u66f4\u591a\u7684\u77e5\u8bc6\u3002 \u4ed6\u4eec\u6700\u64c5\u957f Python\uff0c\u7cbe\u901a\u5341\u591a\u79cd\u8bed\u8a00\uff0c\u5305\u62ec JavaScript\u3001Go\u3001Perl\u3001PHP\u3001Ruby\u3001Swift\u3001TypeScript\u3001SQL \u751a\u81f3 Shell\u3002 \u6211\u4eec\u76ee\u524d\u63d0\u4f9b\u4e24\u79cd\u98df\u5178\u6a21\u5f0f: LATEST MODEL DESCRIPTION MAX REQUEST TRAINING DATA code-davinci-002 \u6700\u80fd\u5e72\u7684\u6cd5\u5178\u6a21\u578b\u3002\u7279\u522b\u64c5\u957f\u5c06\u81ea\u7136\u8bed\u8a00\u8f6c\u6362\u4e3a\u4ee3\u7801\u3002\u9664\u4e86\u8865\u5168\u4ee3\u7801\uff0c\u8fd8\u652f\u6301\u5728\u4ee3\u7801\u4e2d\u63d2\u5165\u8865\u5168\u3002 8,000 tokens Up to Jun 2021 code-cushman-001 \u51e0\u4e4e\u548c\u300a\u8fbe\u82ac\u5947\u6284\u672c\u300b\u4e00\u6837\uff0c\u4f46\u901f\u5ea6\u7a0d\u5feb\u3002\u8fd9\u79cd\u901f\u5ea6\u4f18\u52bf\u53ef\u80fd\u4f7f\u5b83\u66f4\u9002\u5408\u4e8e\u5b9e\u65f6\u5e94\u7528\u7a0b\u5e8f\u3002 Up to 2,048 tokens For more, visit our guide to working with Codex. The Codex models are free to use during the limited beta, and are subject to reduced rate limits. As we learn about use, we'll look to offer pricing to enable a broad set of applications. During this period, you're welcome to go live with your application as long as it follows our usage policies. We welcome any feedback on these models while in early use and look forward to engaging with the community. Feature-specific models The main Codex models are meant to be used with the text completion endpoint. We also offer models that are specifically meant to be used with our endpoints for creating embeddings and editing code. Moderation \u00b6 \u5ba1\u6838\u6a21\u578b\u7528\u4e8e\u68c0\u67e5\u5185\u5bb9\u662f\u5426\u7b26\u5408 OpenAI \u7684\u4f7f\u7528\u7b56\u7565\u3002 \u8fd9\u4e9b\u6a21\u578b\u63d0\u4f9b\u4e86\u5206\u7c7b\u529f\u80fd\uff0c\u53ef\u4ee5\u5728\u4ee5\u4e0b\u7c7b\u522b\u4e2d\u67e5\u627e\u5185\u5bb9:\u4ec7\u6068\u3001\u4ec7\u6068/\u5a01\u80c1\u3001\u81ea\u6b8b\u3001\u6027\u3001\u6027/\u672a\u6210\u5e74\u4eba\u3001\u66b4\u529b\u548c\u66b4\u529b/\u56fe\u5f62\u3002 \u4f60\u53ef\u4ee5\u5728\u6211\u4eec\u7684\u9002\u5ea6\u6307\u5357\u4e2d\u627e\u5230\u66f4\u591a\u4fe1\u606f\u3002 MODEL DESCRIPTION text-moderation-latest \u6700\u6709\u80fd\u529b\u7684\u8c03\u8282\u6a21\u5f0f\u3002\u7cbe\u5ea6\u5c06\u7565\u9ad8\u4e8e\u7a33\u5b9a\u6a21\u578b text-moderation-stable \u6027\u80fd\u51e0\u4e4e\u548c\u6700\u65b0\u578b\u53f7\u4e00\u6837\uff0c\u53ea\u662f\u7a0d\u5fae\u8001\u4e86\u4e00\u4e9b\u3002 GPT-3 \u00b6 GPT-3 models can understand and generate natural language. These models were superceded by the more powerful GPT-3.5 generation models. However, the original GPT-3 base models (davinci, curie, ada, and babbage) are current the only models that are available to fine-tune. LATEST MODEL DESCRIPTION MAX REQUEST TRAINING DATA text-curie-001 Very capable, faster and lower cost than Davinci. 2,048 tokens Up to Oct 2019 text-babbage-001 Capable of straightforward tasks, very fast, and lower cost. 2,048 tokens Up to Oct 2019 text-ada-001 Capable of very simple tasks, usually the fastest model in the GPT-3 series, and lowest cost. 2,048 tokens Up to Oct 2019 davinci Most capable GPT-3 model. Can do any task the other models can do, often with higher quality. 2,048 tokens Up to Oct 2019 curie Very capable, but faster and lower cost than Davinci. 2,048 tokens Up to Oct 2019 babbage Capable of straightforward tasks, very fast, and lower cost. 2,048 tokens Up to Oct 2019 ada Capable of very simple tasks, usually the fastest model in the GPT-3 series, and lowest cost. 2,048 tokens Up to Oct 2019","title":"\u6a21\u578b"},{"location":"models/#_1","text":"","title":"\u6a21\u578b"},{"location":"models/#_2","text":"OpenAI API \u7531\u4e00\u7ec4\u5177\u6709\u4e0d\u540c\u529f\u80fd\u548c\u4ef7\u683c\u70b9\u7684\u4e0d\u540c\u6a21\u578b\u63d0\u4f9b\u652f\u6301\u3002\u60a8\u8fd8\u53ef\u4ee5\u901a\u8fc7\u5fae\u8c03\u5bf9\u6211\u4eec\u7684\u539f\u59cb\u57fa\u672c\u6a21\u578b\u8fdb\u884c\u6709\u9650\u7684\u5b9a\u5236\u3002 MODELS DESCRIPTION GPT-4 Limited beta \u4e00\u7ec4\u6539\u8fdb GPT-3.5 \u7684\u6a21\u578b\uff0c\u53ef\u4ee5\u7406\u89e3\u548c\u751f\u6210\u81ea\u7136\u8bed\u8a00\u6216\u4ee3\u7801 GPT-3.5 \u4e00\u7ec4\u6539\u8fdb GPT-3 \u7684\u6a21\u578b\uff0c\u53ef\u4ee5\u7406\u89e3\u548c\u751f\u6210\u81ea\u7136\u8bed\u8a00\u6216\u4ee3\u7801 DALL\u00b7E \u4e00\u4e2a\u6a21\u578b\uff0c\u53ef\u4ee5\u751f\u6210\u548c\u7f16\u8f91\u56fe\u50cf\u7ed9\u5b9a\u7684\u81ea\u7136\u8bed\u8a00\u63d0\u793a Whisper \u53ef\u4ee5\u5c06\u97f3\u9891\u8f6c\u6362\u4e3a\u6587\u672c\u7684\u6a21\u578b Embeddings \u4e00\u7ec4\u53ef\u4ee5\u5c06\u6587\u672c\u8f6c\u6362\u4e3a\u6570\u503c\u5f62\u5f0f\u7684\u6a21\u578b Codex Limited beta \u4e00\u7ec4\u80fd\u591f\u7406\u89e3\u548c\u751f\u6210\u4ee3\u7801\u7684\u6a21\u578b\uff0c\u5305\u62ec\u5c06\u81ea\u7136\u8bed\u8a00\u8f6c\u6362\u4e3a\u4ee3\u7801 Moderation \u4e00\u4e2a\u7ecf\u8fc7\u5fae\u8c03\u7684\u6a21\u578b\uff0c\u53ef\u4ee5\u68c0\u6d4b\u6587\u672c\u662f\u5426\u654f\u611f\u6216\u4e0d\u5b89\u5168 GPT-3 \u4e00\u7ec4\u80fd\u591f\u7406\u89e3\u548c\u751f\u6210\u81ea\u7136\u8bed\u8a00\u7684\u6a21\u578b \u6211\u4eec\u8fd8\u53d1\u5e03\u4e86\u5305\u62ec Point-E \u3001 Whisper \u3001 Jukebox \u548c CLIP \u5728\u5185\u7684\u5f00\u6e90\u6a21\u578b\u3002 \u8bbf\u95ee\u6211\u4eec\u7684 \u6a21\u578b\u7d22\u5f15 \uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u4e86\u89e3\u66f4\u591a\u5173\u4e8e\u54ea\u4e9b\u6a21\u578b\u5728\u6211\u4eec\u7684\u7814\u7a76\u8bba\u6587\u4e2d\u6709\u7279\u8272\uff0c\u4ee5\u53ca\u50cf InstructGPT \u548c GPT-3.5 \u8fd9\u6837\u7684\u6a21\u578b\u7cfb\u5217\u4e4b\u95f4\u7684\u5dee\u5f02\u3002","title":"\u6982\u8ff0"},{"location":"models/#gpt-4-limited-beta","text":"GPT-4 \u662f\u4e00\u4e2a\u5927\u578b\u7684\u591a\u6a21\u6001\u6a21\u578b(\u76ee\u524d\u63a5\u53d7\u6587\u672c\u8f93\u5165\u5e76\u8f93\u51fa\u6587\u672c\uff0c\u672a\u6765\u5c06\u6709\u56fe\u50cf\u8f93\u5165)\uff0c\u7531\u4e8e\u5176\u66f4\u5e7f\u6cdb\u7684\u5e38\u8bc6\u548c\u5148\u8fdb\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5b83\u53ef\u4ee5\u6bd4\u6211\u4eec\u4e4b\u524d\u7684\u4efb\u4f55\u6a21\u578b\u66f4\u51c6\u786e\u5730\u89e3\u51b3\u96be\u9898\u3002\u4e0e gpt-3.5-turbo \u4e00\u6837\uff0cGPT-4 \u9488\u5bf9\u804a\u5929\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u4f46\u4e5f\u9002\u7528\u4e8e\u4f20\u7edf\u7684\u5b8c\u6210\u4efb\u52a1\u3002 GPT-4 \u76ee\u524d\u5904\u4e8e\u6709\u9650\u7684\u6d4b\u8bd5\u9636\u6bb5\uff0c\u53ea\u6709\u88ab\u6388\u4e88\u8bbf\u95ee\u6743\u9650\u7684\u4eba\u624d\u80fd\u8bbf\u95ee\u3002\u5f53\u5bb9\u91cf\u53ef\u7528\u65f6\uff0c\u8bf7\u52a0\u5165\u7b49\u5f85\u5217\u8868\u4ee5\u83b7\u5f97\u8bbf\u95ee\u3002 LATEST MODEL DESCRIPTION MAX TOKENS TRAINING DATA gpt-4 \u6bd4\u4efb\u4f55 GPT-3.5 \u6a21\u578b\u66f4\u5f3a\u5927\uff0c\u80fd\u591f\u5b8c\u6210\u66f4\u590d\u6742\u7684\u4efb\u52a1\uff0c\u5e76\u4e3a\u804a\u5929\u8fdb\u884c\u4e86\u4f18\u5316\u3002\u5c06\u4e0e\u6211\u4eec\u6700\u65b0\u7684\u6a21\u578b\u8fed\u4ee3\u66f4\u65b0\u3002 8,192 tokens Up to Sep 2021 gpt-4-0314 2023 \u5e74 3 \u6708 14 \u65e5\u7684 gpt-4 \u5feb\u7167\u3002\u4e0e gpt-4 \u4e0d\u540c\u7684\u662f\uff0c\u8be5\u6a21\u578b\u5c06\u4e0d\u4f1a\u63a5\u53d7\u66f4\u65b0\uff0c\u5e76\u4e14\u53ea\u4f1a\u5728 2023 \u5e74 6 \u6708 14 \u65e5\u7ed3\u675f\u7684\u4e09\u4e2a\u6708\u5185\u5f97\u5230\u652f\u6301\u3002 8,192 tokens Up to Sep 2021 gpt-4-32k \u4e0e\u57fa\u7840 gpt-4 \u6a21\u5f0f\u76f8\u540c\u7684\u529f\u80fd\uff0c\u4f46\u4e0a\u4e0b\u6587\u957f\u5ea6\u662f\u5b83\u7684 4 \u500d\u3002\u5c06\u4e0e\u6211\u4eec\u6700\u65b0\u7684\u6a21\u578b\u8fed\u4ee3\u66f4\u65b0\u3002 32,768 tokens Up to Sep 2021 gpt-4-32k-0314 2023 \u5e74 3 \u6708 14 \u65e5 gpt-4-32 \u7684\u5feb\u7167\u3002\u4e0e gpt-4-32k \u4e0d\u540c\u7684\u662f\uff0c\u8be5\u578b\u53f7\u5c06\u4e0d\u63a5\u53d7\u66f4\u65b0\uff0c\u5e76\u4e14\u53ea\u652f\u6301\u4e09\u4e2a\u6708\u7684\u65f6\u95f4\uff0c\u622a\u6b62 2023 \u5e74 6 \u6708 14 \u65e5\u3002 32,768 tokens Up to Sep 2021 \u5bf9\u4e8e\u8bb8\u591a\u57fa\u672c\u4efb\u52a1\uff0cGPT-4 \u548c GPT-3.5 \u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u5f02\u5e76\u4e0d\u663e\u8457\u3002\u7136\u800c\uff0c\u5728\u66f4\u590d\u6742\u7684\u63a8\u7406\u60c5\u51b5\u4e0b\uff0cGPT-4 \u6bd4\u6211\u4eec\u4e4b\u524d\u7684\u4efb\u4f55\u6a21\u578b\u90fd\u66f4\u6709\u80fd\u529b\u3002","title":"GPT-4 Limited beta"},{"location":"models/#gpt-35","text":"GPT-3.5 \u6a21\u578b\u53ef\u4ee5\u7406\u89e3\u548c\u751f\u6210\u81ea\u7136\u8bed\u8a00\u6216\u4ee3\u7801\u3002\u6211\u4eec\u6700\u6709\u80fd\u529b\u548c\u6700\u5177\u6210\u672c\u6548\u76ca\u7684\u6a21\u578b\u662f gpt-3.5-turbo\uff0c\u5b83\u9488\u5bf9\u804a\u5929\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u4f46\u4e5f\u9002\u7528\u4e8e\u4f20\u7edf\u7684\u5b8c\u6210\u4efb\u52a1\u3002 LATEST MODEL DESCRIPTION MAX REQUEST TRAINING DATA gpt-3.5-turbo \u529f\u80fd\u6700\u5f3a\u5927\u7684 GPT-3.5 \u6a21\u578b\u548c\u804a\u5929\u4f18\u5316\uff0c\u6210\u672c\u4e3a\u6587\u672c davinci-003 \u7684\u5341\u5206\u4e4b\u4e00\u3002\u5c06\u4e0e\u6211\u4eec\u6700\u65b0\u7684\u6a21\u578b\u8fed\u4ee3\u66f4\u65b0\u3002 4,096 tokens Up to Sep 2021 gpt-3.5-turbo-0301 2023 \u5e74 3 \u6708 1 \u65e5 gpt-3.5-turbo \u7684\u5feb\u7167\u3002\u4e0e gpt-3.5-turbo \u4e0d\u540c\u7684\u662f\uff0c\u8be5\u578b\u53f7\u5c06\u4e0d\u63a5\u53d7\u66f4\u65b0\uff0c\u5e76\u4e14\u53ea\u652f\u6301\u4e09\u4e2a\u6708\u7684\u65f6\u95f4\uff0c\u622a\u6b62\u5230 2023 \u5e74 6 \u6708 1 \u65e5\u3002 4,096 tokens Up to Sep 2021 text-davinci-003 \u4e0e curie, babbage \u6216 ada \u6a21\u578b\u76f8\u6bd4\uff0c\u53ef\u4ee5\u4ee5\u66f4\u597d\u7684\u8d28\u91cf\uff0c\u66f4\u957f\u7684\u8f93\u51fa\u548c\u4e00\u81f4\u7684\u6307\u4ee4\u9075\u5faa\u6765\u5b8c\u6210\u4efb\u4f55\u8bed\u8a00\u4efb\u52a1\u3002\u8fd8\u652f\u6301\u5728\u6587\u672c\u4e2d\u63d2\u5165\u8865\u5168\u3002 4,000 tokens Up to Jun 2021 text-davinci-002 \u4e0e text-davinci-003 \u529f\u80fd\u7c7b\u4f3c\uff0c\u4f46\u4f7f\u7528\u76d1\u7763\u5fae\u8c03\u800c\u4e0d\u662f\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u8bad\u7ec3 4,000 tokens Up to Jun 2021 code-davinci-002 \u4f18\u5316\u4ee3\u7801\u5b8c\u6210\u4efb\u52a1 4,000 tokens Up to Jun 2021 \u6211\u4eec\u5efa\u8bae\u5728\u5b9e\u9a8c\u65f6\u4f7f\u7528 gpt-3.5-turbo\uff0c\u56e0\u4e3a\u5b83\u4f1a\u4ea7\u751f\u6700\u597d\u7684\u7ed3\u679c\u3002\u4e00\u65e6\u4f60\u6709\u4e86\u5de5\u4f5c\uff0c\u6211\u4eec\u9f13\u52b1\u5c1d\u8bd5\u5176\u4ed6\u6a21\u578b\uff0c\u770b\u770b\u4f60\u662f\u5426\u80fd\u4ee5\u66f4\u4f4e\u7684\u5ef6\u8fdf\u6216\u6210\u672c\u83b7\u5f97\u76f8\u540c\u7684\u7ed3\u679c\u3002 Note OpenAI\u6a21\u578b\u662f\u975e\u786e\u5b9a\u6027\u7684\uff0c\u8fd9\u610f\u5473\u7740\u76f8\u540c\u7684\u8f93\u5165\u53ef\u4ee5\u4ea7\u751f\u4e0d\u540c\u7684\u8f93\u51fa\u3002\u5c06\u6e29\u5ea6\u8bbe\u7f6e\u4e3a0\u5c06\u4f7f\u8f93\u51fa\u5927\u90e8\u5206\u662f\u786e\u5b9a\u7684\uff0c\u4f46\u53ef\u80fd\u4f1a\u4fdd\u7559\u5c11\u91cf\u7684\u53ef\u53d8\u6027\u3002","title":"GPT-3.5"},{"location":"models/#feature-specific","text":"\u867d\u7136\u65b0\u7684 gpt-3.5-turbo \u6a21\u578b\u9488\u5bf9\u804a\u5929\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u4f46\u5b83\u5bf9\u4f20\u7edf\u7684\u5b8c\u6210\u4efb\u52a1\u975e\u5e38\u6709\u6548\u3002\u539f\u59cb\u7684 GPT-3.5 \u6a21\u578b\u9488\u5bf9\u6587\u672c\u8865\u5168\u8fdb\u884c\u4e86\u4f18\u5316\u3002 \u6211\u4eec\u7528\u4e8e \u521b\u5efa\u5d4c\u5165 \u548c \u7f16\u8f91\u6587\u672c \u7684\u7aef\u70b9\u4f7f\u7528\u5b83\u4eec\u81ea\u5df1\u7684\u4e13\u7528\u6a21\u578b\u96c6\u3002","title":"Feature-specific \u58a8\u9999"},{"location":"models/#_3","text":"\u8bd5\u9a8c gpt-3.5-turbo \u662f\u4e86\u89e3 API \u529f\u80fd\u7684\u597d\u65b9\u6cd5\u3002 \u5728\u4f60\u77e5\u9053\u4f60\u60f3\u8981\u5b8c\u6210\u4ec0\u4e48\u4e4b\u540e\uff0c\u4f60\u53ef\u4ee5\u7ee7\u7eed\u4f7f\u7528 gpt-3.5-turbo \u6216\u5176\u4ed6\u6a21\u578b\uff0c\u5e76\u5c1d\u8bd5\u56f4\u7ed5\u5b83\u7684\u529f\u80fd\u8fdb\u884c\u4f18\u5316\u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528 GPT \u6bd4\u8f83\u5de5\u5177 \uff0c\u8be5\u5de5\u5177\u5141\u8bb8\u60a8\u5e76\u6392\u8fd0\u884c\u4e0d\u540c\u7684\u6a21\u578b\u6765\u6bd4\u8f83\u8f93\u51fa\u3001\u8bbe\u7f6e\u548c\u54cd\u5e94\u65f6\u95f4\uff0c\u7136\u540e\u5c06\u6570\u636e\u4e0b\u8f7d\u5230 Excel \u7535\u5b50\u8868\u683c\u4e2d\u3002","title":"\u627e\u5230\u6b63\u786e\u7684\u6a21\u5f0f"},{"location":"models/#turbo","text":"Turbo \u662f\u652f\u6301 ChatGPT \u7684\u540c\u4e00\u4e2a\u6a21\u578b\u5bb6\u65cf\u3002 \u5b83\u9488\u5bf9\u4f1a\u8bdd\u804a\u5929\u8f93\u5165\u548c\u8f93\u51fa\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u4f46\u4e0e\u8fbe\u82ac\u5947\u6a21\u578b\u5bb6\u65cf\u76f8\u6bd4\uff0c\u5b83\u5728\u8865\u5168\u65b9\u9762\u8868\u73b0\u540c\u6837\u51fa\u8272\u3002 \u4efb\u4f55\u5728 ChatGPT \u4e2d\u53ef\u4ee5\u5f88\u597d\u5730\u5b8c\u6210\u7684\u7528\u4f8b\u90fd\u5e94\u8be5\u5728 API \u4e2d\u7684 Turbo \u6a21\u578b\u5bb6\u65cf\u4e2d\u8868\u73b0\u826f\u597d\u3002 Turbo \u6a21\u578b\u5bb6\u65cf\u4e5f\u662f\u7b2c\u4e00\u4e2a\u63a5\u53d7\u5b9a\u671f\u66f4\u65b0\u7684\u6a21\u578b\uff0c\u5982 ChatGPT\u3002 \u64c5\u957f:\u4f1a\u8bdd\u548c\u6587\u672c\u751f\u6210","title":"Turbo"},{"location":"models/#davinci","text":"Davinci is the most capable model family and can perform any task the other models (ada, curie, and babbage) can perform and often with less instruction. For applications requiring a lot of understanding of the content, like summarization for a specific audience and creative content generation, Davinci will produce the best results. These increased capabilities require more compute resources, so Davinci costs more per API call and is not as fast as the other models. Another area where Davinci shines is in understanding the intent of text. Davinci is quite good at solving many kinds of logic problems and explaining the motives of characters. Davinci has been able to solve some of the most challenging AI problems involving cause and effect. Good at: Complex intent, cause and effect, summarization for audience","title":"Davinci"},{"location":"models/#curie","text":"Curie is extremely powerful, yet very fast. While Davinci is stronger when it comes to analyzing complicated text, Curie is quite capable for many nuanced tasks like sentiment classification and summarization. Curie is also quite good at answering questions and performing Q&A and as a general service chatbot. Good at: Language translation, complex classification, text sentiment, summarization","title":"Curie"},{"location":"models/#babbage","text":"Babbage can perform straightforward tasks like simple classification. It\u2019s also quite capable when it comes to Semantic Search ranking how well documents match up with search queries. Good at: Moderate classification, semantic search classification","title":"Babbage"},{"location":"models/#ada","text":"Ada is usually the fastest model and can perform tasks like parsing text, address correction and certain kinds of classification tasks that don\u2019t require too much nuance. Ada\u2019s performance can often be improved by providing more context. Good at: Parsing text, simple classification, address correction, keywords Note: Any task performed by a faster model like Ada can be performed by a more powerful model like Curie or Davinci.","title":"Ada"},{"location":"models/#dalle","text":"DALL\u00b7E is a AI system that can create realistic images and art from a description in natural language. We currently support the ability, given a prommpt, to create a new image with a certain size, edit an existing image, or create variations of a user provided image. The current DALL\u00b7E model available through our API is the 2 nd iteration of DALL\u00b7E with more realistic, accurate, and 4x greater resolution images than the original model. You can try it through the our Labs interface or via the API.","title":"DALL\u00b7E"},{"location":"models/#whisper","text":"Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multi-task model that can perform multilingual speech recognition as well as speech translation and language identification. The Whisper v2-large model is currently available through our API with the whisper-1 model name. Currently, there is no difference between the open source version of Whisper and the version available through our API. However, through our API, we offer an optimized inference process which makes running Whisper through our API much faster than doing it through other means. For more technical details on Whisper, you can read the paper.","title":"Whisper"},{"location":"models/#embeddings","text":"\u5d4c\u5165\u662f\u4e00\u79cd\u6587\u672c\u7684\u6570\u5b57\u8868\u793a\uff0c\u53ef\u7528\u4e8e\u6d4b\u91cf\u4e24\u6bb5\u6587\u672c\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002 \u6211\u4eec\u7684\u7b2c\u4e8c\u4ee3\u5d4c\u5165\u6a21\u578b\uff0ctext-embedding-ada-002 \u662f\u4e00\u79cd\u8bbe\u8ba1\u6765\u53d6\u4ee3\u4e4b\u524d\u7684 16 \u4e2a\u7b2c\u4e00\u4ee3\u5d4c\u5165\u6a21\u578b\u7684\u6210\u672c\u7684\u4e00\u5c0f\u90e8\u5206\u3002 \u5d4c\u5165\u5bf9\u4e8e\u641c\u7d22\u3001\u805a\u7c7b\u3001\u63a8\u8350\u3001\u5f02\u5e38\u68c0\u6d4b\u548c\u5206\u7c7b\u4efb\u52a1\u975e\u5e38\u6709\u7528\u3002 \u60a8\u53ef\u4ee5\u5728\u516c\u544a\u535a\u5ba2\u6587\u7ae0\u4e2d\u9605\u8bfb\u66f4\u591a\u5173\u4e8e\u6211\u4eec\u6700\u65b0\u7684\u5d4c\u5165\u6a21\u578b\u3002","title":"Embeddings"},{"location":"models/#codex-limited-beta","text":"Codex \u6a21\u578b\u662f GPT-3 \u6a21\u578b\u7684\u540e\u4ee3\uff0c\u53ef\u4ee5\u7406\u89e3\u548c\u751f\u6210\u4ee3\u7801\u3002 \u4ed6\u4eec\u7684\u8bad\u7ec3\u6570\u636e\u65e2\u5305\u542b\u81ea\u7136\u8bed\u8a00\uff0c\u4e5f\u5305\u542b\u6765\u81ea GitHub \u7684\u6570\u5341\u4ebf\u884c\u516c\u5171\u4ee3\u7801\u3002\u5b66\u4e60\u66f4\u591a\u7684\u77e5\u8bc6\u3002 \u4ed6\u4eec\u6700\u64c5\u957f Python\uff0c\u7cbe\u901a\u5341\u591a\u79cd\u8bed\u8a00\uff0c\u5305\u62ec JavaScript\u3001Go\u3001Perl\u3001PHP\u3001Ruby\u3001Swift\u3001TypeScript\u3001SQL \u751a\u81f3 Shell\u3002 \u6211\u4eec\u76ee\u524d\u63d0\u4f9b\u4e24\u79cd\u98df\u5178\u6a21\u5f0f: LATEST MODEL DESCRIPTION MAX REQUEST TRAINING DATA code-davinci-002 \u6700\u80fd\u5e72\u7684\u6cd5\u5178\u6a21\u578b\u3002\u7279\u522b\u64c5\u957f\u5c06\u81ea\u7136\u8bed\u8a00\u8f6c\u6362\u4e3a\u4ee3\u7801\u3002\u9664\u4e86\u8865\u5168\u4ee3\u7801\uff0c\u8fd8\u652f\u6301\u5728\u4ee3\u7801\u4e2d\u63d2\u5165\u8865\u5168\u3002 8,000 tokens Up to Jun 2021 code-cushman-001 \u51e0\u4e4e\u548c\u300a\u8fbe\u82ac\u5947\u6284\u672c\u300b\u4e00\u6837\uff0c\u4f46\u901f\u5ea6\u7a0d\u5feb\u3002\u8fd9\u79cd\u901f\u5ea6\u4f18\u52bf\u53ef\u80fd\u4f7f\u5b83\u66f4\u9002\u5408\u4e8e\u5b9e\u65f6\u5e94\u7528\u7a0b\u5e8f\u3002 Up to 2,048 tokens For more, visit our guide to working with Codex. The Codex models are free to use during the limited beta, and are subject to reduced rate limits. As we learn about use, we'll look to offer pricing to enable a broad set of applications. During this period, you're welcome to go live with your application as long as it follows our usage policies. We welcome any feedback on these models while in early use and look forward to engaging with the community. Feature-specific models The main Codex models are meant to be used with the text completion endpoint. We also offer models that are specifically meant to be used with our endpoints for creating embeddings and editing code.","title":"Codex Limited beta"},{"location":"models/#moderation","text":"\u5ba1\u6838\u6a21\u578b\u7528\u4e8e\u68c0\u67e5\u5185\u5bb9\u662f\u5426\u7b26\u5408 OpenAI \u7684\u4f7f\u7528\u7b56\u7565\u3002 \u8fd9\u4e9b\u6a21\u578b\u63d0\u4f9b\u4e86\u5206\u7c7b\u529f\u80fd\uff0c\u53ef\u4ee5\u5728\u4ee5\u4e0b\u7c7b\u522b\u4e2d\u67e5\u627e\u5185\u5bb9:\u4ec7\u6068\u3001\u4ec7\u6068/\u5a01\u80c1\u3001\u81ea\u6b8b\u3001\u6027\u3001\u6027/\u672a\u6210\u5e74\u4eba\u3001\u66b4\u529b\u548c\u66b4\u529b/\u56fe\u5f62\u3002 \u4f60\u53ef\u4ee5\u5728\u6211\u4eec\u7684\u9002\u5ea6\u6307\u5357\u4e2d\u627e\u5230\u66f4\u591a\u4fe1\u606f\u3002 MODEL DESCRIPTION text-moderation-latest \u6700\u6709\u80fd\u529b\u7684\u8c03\u8282\u6a21\u5f0f\u3002\u7cbe\u5ea6\u5c06\u7565\u9ad8\u4e8e\u7a33\u5b9a\u6a21\u578b text-moderation-stable \u6027\u80fd\u51e0\u4e4e\u548c\u6700\u65b0\u578b\u53f7\u4e00\u6837\uff0c\u53ea\u662f\u7a0d\u5fae\u8001\u4e86\u4e00\u4e9b\u3002","title":"Moderation"},{"location":"models/#gpt-3","text":"GPT-3 models can understand and generate natural language. These models were superceded by the more powerful GPT-3.5 generation models. However, the original GPT-3 base models (davinci, curie, ada, and babbage) are current the only models that are available to fine-tune. LATEST MODEL DESCRIPTION MAX REQUEST TRAINING DATA text-curie-001 Very capable, faster and lower cost than Davinci. 2,048 tokens Up to Oct 2019 text-babbage-001 Capable of straightforward tasks, very fast, and lower cost. 2,048 tokens Up to Oct 2019 text-ada-001 Capable of very simple tasks, usually the fastest model in the GPT-3 series, and lowest cost. 2,048 tokens Up to Oct 2019 davinci Most capable GPT-3 model. Can do any task the other models can do, often with higher quality. 2,048 tokens Up to Oct 2019 curie Very capable, but faster and lower cost than Davinci. 2,048 tokens Up to Oct 2019 babbage Capable of straightforward tasks, very fast, and lower cost. 2,048 tokens Up to Oct 2019 ada Capable of very simple tasks, usually the fastest model in the GPT-3 series, and lowest cost. 2,048 tokens Up to Oct 2019","title":"GPT-3"},{"location":"price/","text":"\u5b9a\u4ef7 \u00b6 \u7b80\u5355\u7075\u6d3b\u3002\u53ea\u6309\u4f7f\u7528\u4ed8\u8d39\u3002 \u5feb\u901f\u94fe\u63a5\u8054\u7cfb\u9500\u552e\u4e86\u89e3\u66f4\u591a\u4fe1\u606f \u8bed\u8a00\u6a21\u578b \u00b6 \u591a\u79cd\u578b\u53f7\uff0c\u6bcf\u4e2a\u578b\u53f7\u90fd\u6709\u4e0d\u540c\u7684\u529f\u80fd\u548c\u4ef7\u4f4d\u3002\u4ef7\u683c\u4e3a\u6bcf 1000 \u4e2a tokens\u3002 \u4f60\u53ef\u4ee5\u628a tokens \u60f3\u8c61\u6210\u4e00\u4e2a\u4e2a\u5355\u8bcd\uff0c1000 \u4e2a tokens \u5927\u7ea6\u662f 750 \u4e2a\u5355\u8bcd\u3002 \u8fd9\u6bb5\u662f 35 \u4e2a tokens\u3002 \u7c7b\u578b \u6a21\u578b \u8bad\u7ec3 \u4f7f\u7528 \u804a\u5929\u6a21\u578b-Chat gpt-3.5-turbo $0.002 / 1K tokens \u6307\u4ee4\u6a21\u578b- InstructGPT Ada (\u6700\u5feb\u7684) $0.0004 / 1K tokens Babbage $0.0005 / 1K tokens Curie $0.0020 / 1K tokens Davinci (\u6700\u6709\u5f71\u54cd\u529b) $0.0200 / 1K tokens \u5fae\u8c03\u6a21\u578b- Fine-tuning models Ada $0.0004 / 1K tokens $0.0016 / 1K tokens Babbage $0.0006 / 1K tokens $0.0024 / 1K tokens Curie $0.0030 / 1K tokens $0.0120 / 1K tokens Davinci $0.0300 / 1K tokens $0.1200 / 1K tokens \u5d4c\u5165\u6a21\u578b- Embedding models Ada $0.0004 / 1K tokens \u591a\u901a\u9053\u6a21\u578b \u00b6 \u6a21\u578b \u6a21\u578b/\u51b3\u8bae \u4ef7\u683c \u56fe\u50cf\u6a21\u578b 1024\u00d71024 $0.020 / image 512\u00d7512 $0.018 / image 256\u00d7256 $0.016 / image \u58f0\u97f3\u6a21\u578b Whisper $0.006 / minute (\u56db\u820d\u4e94\u5165\u5230\u6700\u8fd1\u7684\u79d2) \u4f7f\u7528\u914d\u989d \u00b6 \u5f53\u4f60\u6ce8\u518c\u65f6\uff0c\u4f60\u5c06\u83b7\u5f97\u4e00\u4e2a\u521d\u59cb\u6d88\u8d39\u9650\u989d\u6216\u914d\u989d\uff0c\u968f\u7740\u65f6\u95f4\u7684\u63a8\u79fb\uff0c\u6211\u4eec\u4f1a\u968f\u7740\u4f60\u7684\u7533\u8bf7\u5efa\u7acb\u4e00\u4e2a\u8ddf\u8e2a\u8bb0\u5f55\u800c\u589e\u52a0\u8fd9\u4e2a\u9650\u989d\u3002 \u5982\u679c\u60a8\u9700\u8981\u66f4\u591a\u7684\u4ee4\u724c\uff0c\u60a8\u603b\u662f\u53ef\u4ee5\u8bf7\u6c42\u589e\u52a0\u914d\u989d\u3002","title":"\u5b9a\u4ef7"},{"location":"price/#_1","text":"\u7b80\u5355\u7075\u6d3b\u3002\u53ea\u6309\u4f7f\u7528\u4ed8\u8d39\u3002 \u5feb\u901f\u94fe\u63a5\u8054\u7cfb\u9500\u552e\u4e86\u89e3\u66f4\u591a\u4fe1\u606f","title":"\u5b9a\u4ef7"},{"location":"price/#_2","text":"\u591a\u79cd\u578b\u53f7\uff0c\u6bcf\u4e2a\u578b\u53f7\u90fd\u6709\u4e0d\u540c\u7684\u529f\u80fd\u548c\u4ef7\u4f4d\u3002\u4ef7\u683c\u4e3a\u6bcf 1000 \u4e2a tokens\u3002 \u4f60\u53ef\u4ee5\u628a tokens \u60f3\u8c61\u6210\u4e00\u4e2a\u4e2a\u5355\u8bcd\uff0c1000 \u4e2a tokens \u5927\u7ea6\u662f 750 \u4e2a\u5355\u8bcd\u3002 \u8fd9\u6bb5\u662f 35 \u4e2a tokens\u3002 \u7c7b\u578b \u6a21\u578b \u8bad\u7ec3 \u4f7f\u7528 \u804a\u5929\u6a21\u578b-Chat gpt-3.5-turbo $0.002 / 1K tokens \u6307\u4ee4\u6a21\u578b- InstructGPT Ada (\u6700\u5feb\u7684) $0.0004 / 1K tokens Babbage $0.0005 / 1K tokens Curie $0.0020 / 1K tokens Davinci (\u6700\u6709\u5f71\u54cd\u529b) $0.0200 / 1K tokens \u5fae\u8c03\u6a21\u578b- Fine-tuning models Ada $0.0004 / 1K tokens $0.0016 / 1K tokens Babbage $0.0006 / 1K tokens $0.0024 / 1K tokens Curie $0.0030 / 1K tokens $0.0120 / 1K tokens Davinci $0.0300 / 1K tokens $0.1200 / 1K tokens \u5d4c\u5165\u6a21\u578b- Embedding models Ada $0.0004 / 1K tokens","title":"\u8bed\u8a00\u6a21\u578b"},{"location":"price/#_3","text":"\u6a21\u578b \u6a21\u578b/\u51b3\u8bae \u4ef7\u683c \u56fe\u50cf\u6a21\u578b 1024\u00d71024 $0.020 / image 512\u00d7512 $0.018 / image 256\u00d7256 $0.016 / image \u58f0\u97f3\u6a21\u578b Whisper $0.006 / minute (\u56db\u820d\u4e94\u5165\u5230\u6700\u8fd1\u7684\u79d2)","title":"\u591a\u901a\u9053\u6a21\u578b"},{"location":"price/#_4","text":"\u5f53\u4f60\u6ce8\u518c\u65f6\uff0c\u4f60\u5c06\u83b7\u5f97\u4e00\u4e2a\u521d\u59cb\u6d88\u8d39\u9650\u989d\u6216\u914d\u989d\uff0c\u968f\u7740\u65f6\u95f4\u7684\u63a8\u79fb\uff0c\u6211\u4eec\u4f1a\u968f\u7740\u4f60\u7684\u7533\u8bf7\u5efa\u7acb\u4e00\u4e2a\u8ddf\u8e2a\u8bb0\u5f55\u800c\u589e\u52a0\u8fd9\u4e2a\u9650\u989d\u3002 \u5982\u679c\u60a8\u9700\u8981\u66f4\u591a\u7684\u4ee4\u724c\uff0c\u60a8\u603b\u662f\u53ef\u4ee5\u8bf7\u6c42\u589e\u52a0\u914d\u989d\u3002","title":"\u4f7f\u7528\u914d\u989d"},{"location":"prompts/","text":"\u63d0\u793a\u8bed \u00b6 \u8f6c\u4e49\u63d0\u793a\u8bed \u00b6 1 \u8f6c\u4e49 https://platform.openai.com \u4e3a Markdown Code\uff0c\u5e76\u7ffb\u8bd1","title":"\u63d0\u793a\u8bed"},{"location":"prompts/#_1","text":"","title":"\u63d0\u793a\u8bed"},{"location":"prompts/#_2","text":"1 \u8f6c\u4e49 https://platform.openai.com \u4e3a Markdown Code\uff0c\u5e76\u7ffb\u8bd1","title":"\u8f6c\u4e49\u63d0\u793a\u8bed"},{"location":"quickstart/","text":"\u5feb\u901f\u5165\u95e8 \u00b6 OpenAI \u8bad\u7ec3\u4e86\u4e00\u4e9b\u5c16\u7aef\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u5b83\u4eec\u975e\u5e38\u64c5\u957f\u7406\u89e3\u548c\u751f\u6210\u6587\u672c\u3002\u6211\u4eec\u7684 API \u63d0\u4f9b\u4e86\u5bf9\u8fd9\u4e9b\u6a21\u578b\u7684\u8bbf\u95ee\uff0c\u5e76\u53ef\u4ee5\u7528\u4e8e\u89e3\u51b3\u51e0\u4e4e\u6d89\u53ca\u5904\u7406\u8bed\u8a00\u7684\u4efb\u4f55\u4efb\u52a1\u3002 \u5728\u8fd9\u4e2a\u5feb\u901f\u5165\u95e8\u6559\u7a0b\u4e2d\uff0c\u60a8\u5c06\u6784\u5efa\u4e00\u4e2a\u7b80\u5355\u7684\u793a\u4f8b\u5e94\u7528\u7a0b\u5e8f\u3002\u5728\u6b64\u8fc7\u7a0b\u4e2d\uff0c\u60a8\u5c06\u5b66\u4e60\u5230\u5bf9\u4e8e\u4efb\u4f55\u4efb\u52a1\u4f7f\u7528 API \u7684\u57fa\u672c\u6982\u5ff5\u548c\u6280\u672f\uff0c\u5305\u62ec\uff1a \u5185\u5bb9\u751f\u6210 \u6458\u8981 \u5206\u7c7b\u3001\u5206\u7c7b\u548c\u60c5\u611f\u5206\u6790 \u6570\u636e\u63d0\u53d6 \u7ffb\u8bd1 \u8fd8\u6709\u66f4\u591a\uff01 \u4ecb\u7ecd \u00b6 \u8865\u5168 \u7aef\u70b9\u662f\u6211\u4eec API \u7684\u6838\u5fc3\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u975e\u5e38\u7075\u6d3b\u548c\u5f3a\u5927\u7684\u7b80\u5355\u63a5\u53e3\u3002 \u60a8\u5c06\u4e00\u4e9b\u6587\u672c\u4f5c\u4e3a\u63d0\u793a\u8f93\u5165\uff0cAPI \u5c06\u8fd4\u56de\u4e00\u4e2a\u6587\u672c\u8865\u5168\uff0c\u8bd5\u56fe\u5339\u914d\u60a8\u7ed9\u51fa\u7684\u4efb\u4f55\u6307\u4ee4\u6216\u4e0a\u4e0b\u6587\u3002 Prompt-\u63d0\u793a \u4e3a\u51b0\u6fc0\u51cc\u5e97\u5199\u4e00\u4e2a\u6807\u8bed\u3002 Completion-\u8865\u5168 \u6211\u4eec\u6bcf\u676f\u90fd\u63d0\u4f9b\u5fae\u7b11\uff01 \u60a8\u53ef\u4ee5\u5c06\u5176\u89c6\u4e3a\u975e\u5e38\u9ad8\u7ea7\u7684\u81ea\u52a8\u8865\u5168 - \u6a21\u578b\u5904\u7406\u60a8\u7684\u6587\u672c\u63d0\u793a\uff0c\u5e76\u5c1d\u8bd5\u9884\u6d4b\u6700\u6709\u53ef\u80fd\u51fa\u73b0\u7684\u5185\u5bb9\u3002 1. \u4ece\u4e00\u6761\u6307\u4ee4\u5f00\u59cb \u00b6 \u60f3\u8c61\u4e00\u4e0b\u60a8\u60f3\u521b\u5efa\u4e00\u4e2a\u5ba0\u7269\u540d\u5b57\u751f\u6210\u5668\u3002\u4ece\u96f6\u5f00\u59cb\u60f3\u51fa\u540d\u5b57\u5f88\u96be\uff01 \u9996\u5148\uff0c\u60a8\u9700\u8981\u4e00\u4e2a\u660e\u786e\u60a8\u60f3\u8981\u7684\u63d0\u793a\u3002\u8ba9\u6211\u4eec\u4ece\u4e00\u6761\u6307\u4ee4\u5f00\u59cb\u3002 **\u5c06\u6b64\u63d0\u793a\u63d0\u4ea4**\u4ee5\u751f\u6210\u60a8\u7684\u7b2c\u4e00\u4e2a\u8865\u5168\u3002 1 \u4e3a\u9a6c\u63d0\u4f9b\u4e00\u4e2a\u540d\u5b57\u3002 \u4e0d\u9519\uff01\u73b0\u5728\uff0c\u8bd5\u7740\u8ba9\u60a8\u7684\u6307\u4ee4\u66f4\u5177\u4f53\u3002 \u4e3a\u4e00\u5339\u9ed1\u9a6c\u5efa\u8bae\u4e00\u4e2a\u540d\u5b57\u3002 \u6b63\u5982\u60a8\u6240\u89c1\uff0c\u5411\u6211\u4eec\u7684\u63d0\u793a\u6dfb\u52a0\u4e00\u4e2a\u7b80\u5355\u7684\u5f62\u5bb9\u8bcd\u4f1a\u6539\u53d8\u7ed3\u679c\u7684\u8865\u5168\u3002\u8bbe\u8ba1\u63d0\u793a\u672c\u8d28\u4e0a\u5c31\u662f\u5982\u4f55\u201c\u7f16\u7a0b\u201d\u6a21\u578b\u3002 2. \u6dfb\u52a0\u4e00\u4e9b\u793a\u4f8b \u00b6 \u8bbe\u8ba1\u826f\u597d\u7684\u6307\u4ee4\u5bf9\u4e8e\u83b7\u5f97\u826f\u597d\u7684\u7ed3\u679c\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u6709\u65f6\u5b83\u4eec\u8fd8\u4e0d\u591f\u3002\u8ba9\u6211\u4eec\u5c1d\u8bd5\u4f7f\u60a8\u7684\u6307\u4ee4\u66f4\u52a0\u590d\u6742\u3002 1 \u4e3a\u4e00\u4f4d\u8d85\u7ea7\u82f1\u96c4\u7684\u9a6c\u5efa\u8bae\u4e09\u4e2a\u540d\u5b57\u3002 \u8fd9\u4e2a\u8865\u5168\u5e76\u4e0d\u662f\u6211\u4eec\u60f3\u8981\u7684\u3002\u8fd9\u4e9b\u540d\u79f0\u975e\u5e38\u901a\u7528\uff0c\u5e76\u4e14\u4f3c\u4e4e\u6a21\u578b\u6ca1\u6709\u6ce8\u610f\u5230\u6211\u4eec\u6307\u4ee4\u4e2d\u7684\u9a6c\u3002\u8ba9\u6211\u4eec\u770b\u770b\u662f\u5426\u53ef\u4ee5\u8ba9\u5b83\u63d0\u51fa\u66f4\u76f8\u5173\u7684\u5efa\u8bae\u3002 \u5728\u8bb8\u591a\u60c5\u51b5\u4e0b\uff0c\u5411\u6a21\u578b\u5c55\u793a\u548c\u544a\u8bc9\u60a8\u60f3\u8981\u7684\u662f\u6709\u5e2e\u52a9\u7684\u3002\u5c06\u793a\u4f8b\u6dfb\u52a0\u5230\u60a8\u7684\u63d0\u793a\u4e2d\u53ef\u4ee5\u5e2e\u52a9\u4f20\u8fbe\u6a21\u5f0f\u6216\u7ec6\u5fae\u5dee\u522b\u3002\u5c1d\u8bd5\u63d0\u4ea4\u5305\u542b\u4e00\u4e9b\u793a\u4f8b\u7684\u6b64\u63d0\u793a\u3002 1 2 3 4 5 6 7 8 \u4e3a\u4e00\u4e2a\u8d85\u7ea7\u82f1\u96c4\u7684\u52a8\u7269\u5efa\u8bae\u4e09\u4e2a\u540d\u5b57\u3002 \u52a8\u7269\uff1a\u732b \u540d\u79f0\uff1aSharpclaw \u961f\u957f\uff0cFluffball \u7279\u5de5\uff0cIncredible Feline \u52a8\u7269\uff1a\u72d7 \u540d\u79f0\uff1aRuff the Protector\uff0cWonder Canine\uff0cSir Barks-a-Lot \u52a8\u7269\uff1a\u9a6c \u540d\u79f0\uff1a \u4e0d\u9519\uff01\u4e3a\u7ed9\u5b9a\u7684\u8f93\u5165\u6dfb\u52a0\u8f93\u51fa\u793a\u4f8b\u53ef\u4ee5\u5e2e\u52a9\u6a21\u578b\u63d0\u4f9b\u6211\u4eec\u6240\u5bfb\u627e\u7684\u540d\u79f0\u7c7b\u578b\u3002 3. \u8c03\u6574\u60a8\u7684\u8bbe\u7f6e \u00b6 \u63d0\u793a\u8bbe\u8ba1\u5e76\u4e0d\u662f\u60a8\u62e5\u6709\u7684\u552f\u4e00\u5de5\u5177\u3002\u60a8\u8fd8\u53ef\u4ee5\u901a\u8fc7\u8c03\u6574\u8bbe\u7f6e\u6765\u63a7\u5236\u8865\u5168\u3002\u5176\u4e2d\u6700\u91cd\u8981\u7684\u8bbe\u7f6e\u4e4b\u4e00\u79f0\u4e3a**\u6e29\u5ea6**\u3002 \u60a8\u53ef\u80fd\u5df2\u7ecf\u6ce8\u610f\u5230\uff0c\u5982\u679c\u5728\u4e0a\u9762\u7684\u793a\u4f8b\u4e2d\u591a\u6b21\u63d0\u4ea4\u76f8\u540c\u7684\u63d0\u793a\uff0c\u6a21\u578b\u5c06\u59cb\u7ec8\u8fd4\u56de\u76f8\u540c\u6216\u975e\u5e38\u76f8\u4f3c\u7684\u8865\u5168\u3002\u8fd9\u662f\u56e0\u4e3a\u60a8\u7684\u6e29\u5ea6\u8bbe\u7f6e\u4e3a 0 \u3002 \u5c1d\u8bd5\u4f7f\u7528\u6e29\u5ea6\u8bbe\u7f6e\u4e3a 1 \u91cd\u65b0\u63d0\u4ea4\u76f8\u540c\u7684\u63d0\u793a\u51e0\u6b21\u3002 1 2 3 4 5 6 7 8 9 10 \u4e3a\u4e00\u4e2a\u8d85\u7ea7\u82f1\u96c4\u7684\u52a8\u7269\u63d0\u51fa\u4e09\u4e2a\u540d\u5b57\u3002 \u52a8\u7269\uff1a\u732b \u540d\u79f0\uff1aSharpclaw \u961f\u957f\u3001Fluffball \u7279\u5de5\u3001Incredible Feline \u52a8\u7269\uff1a\u72d7 \u540d\u79f0\uff1a\u4fdd\u62a4\u8005\u62c9\u592b\u3001Wonder Canine\u3001Sir Barks-a-Lot \u52a8\u7269\uff1a\u9a6c \u540d\u79f0\uff1a \u6e29\u5ea6 0-1 \u770b\u770b\u53d1\u751f\u4e86\u4ec0\u4e48\uff1f\u5f53\u6e29\u5ea6\u9ad8\u4e8e 0 \u65f6\uff0c\u591a\u6b21\u63d0\u4ea4\u76f8\u540c\u7684\u63d0\u793a\u4f1a\u5bfc\u81f4\u4e0d\u540c\u7684\u8865\u5168\u7ed3\u679c\u3002 \u8bf7\u8bb0\u4f4f\uff0c\u6a21\u578b\u9884\u6d4b\u5728\u5176\u4e4b\u524d\u7684\u6587\u672c\u4e4b\u540e\u6700\u53ef\u80fd\u51fa\u73b0\u7684\u6587\u672c\u3002 \u6e29\u5ea6\u662f\u4e00\u4e2a\u4ecb\u4e8e 0 \u548c 1 \u4e4b\u95f4\u7684\u503c\uff0c\u57fa\u672c\u4e0a\u8ba9\u60a8\u63a7\u5236\u6a21\u578b\u5728\u8fdb\u884c\u8fd9\u4e9b\u9884\u6d4b\u65f6\u5e94\u8be5\u6709\u591a\u81ea\u4fe1\u3002 \u964d\u4f4e\u6e29\u5ea6\u610f\u5473\u7740\u5b83\u5c06\u5192\u66f4\u5c11\u7684\u98ce\u9669\uff0c\u8865\u5168\u5c06\u66f4\u51c6\u786e\u548c\u786e\u5b9a\u6027\u3002\u589e\u52a0\u6e29\u5ea6\u5c06\u5bfc\u81f4\u66f4\u591a\u7684\u591a\u6837\u5316\u8865\u5168\u3002 \u6df1\u5165\u6316\u6398->\u7406\u89e3\u6807\u8bb0\u548c\u6982\u7387 \u6211\u4eec\u7684\u6a21\u578b\u901a\u8fc7\u5c06\u6587\u672c\u62c6\u5206\u6210\u79f0\u4e3a\u6807\u8bb0\u7684\u8f83\u5c0f\u5355\u4f4d\u6765\u5904\u7406\u6587\u672c\u3002 \u6807\u8bb0\u53ef\u4ee5\u662f\u5355\u8bcd\u3001\u5355\u8bcd\u5757\u6216\u5355\u4e2a\u5b57\u7b26\u3002\u7f16\u8f91\u4e0b\u9762\u7684\u6587\u672c\u4ee5\u67e5\u770b\u5982\u4f55\u5c06\u5176\u6807\u8bb0\u5316\u3002 1 I have an orange cat named Butterscotch. 1 I have an orange cat named Butterscotch. \u50cf\u201ccat\u201d\u8fd9\u6837\u7684\u5e38\u7528\u8bcd\u662f\u4e00\u4e2a\u6807\u8bb0\uff0c\u800c\u4e0d\u5e38\u7528\u7684\u8bcd\u901a\u5e38\u4f1a\u88ab\u62c6\u5206\u6210\u591a\u4e2a\u6807\u8bb0\u3002 \u4f8b\u5982\uff0c\u201cButterscotch\u201d\u53ef\u4ee5\u7ffb\u8bd1\u6210\u56db\u4e2a\u6807\u8bb0\uff1a\u201cBut\u201d\u3001\u201cters\u201d\u3001\u201ccot\u201d\u548c\u201cch\u201d\u3002 \u8bb8\u591a\u6807\u8bb0\u4ee5\u7a7a\u683c\u5f00\u5934\uff0c\u4f8b\u5982\u201c hello\u201d\u548c\u201c bye\u201d\u3002 \u5bf9\u4e8e\u4e00\u4e9b\u6587\u672c\uff0c\u6a21\u578b\u786e\u5b9a\u4e0b\u4e00\u4e2a\u6700\u53ef\u80fd\u51fa\u73b0\u7684\u6807\u8bb0\u662f\u54ea\u4e2a\u3002 \u4f8b\u5982\uff0c\u6587\u672c\u201cHorses are my favorite\u201d\u6700\u6709\u53ef\u80fd\u540e\u8ddf\u6807\u8bb0\u201canimal\u201d\u3002 1 2 3 4 5 6 Horses are my favorite animal 49.65% animals 42.58% \\n 3.49% ! 0.91% \u5982\u679c\u5c06\u6e29\u5ea6\u8bbe\u7f6e\u4e3a 0 \u5e76\u63d0\u4ea4\u6b64\u63d0\u793a 4 \u6b21\uff0c\u6a21\u578b\u5c06\u59cb\u7ec8\u8fd4\u56de\u201canimal\u201d\u4f5c\u4e3a\u4e0b\u4e00\u4e2a\u6807\u8bb0\uff0c\u56e0\u4e3a\u5b83\u5177\u6709\u6700\u9ad8\u7684\u6982\u7387\u3002 \u5982\u679c\u589e\u52a0\u6e29\u5ea6\uff0c\u5b83\u5c06\u66f4\u5192\u9669\uff0c\u8003\u8651\u6982\u7387\u8f83\u4f4e\u7684\u6807\u8bb0\u3002 \u5982\u679c\u6e29\u5ea6\u4e3a 0 1 2 3 4 Horses are my favorite animal Horses are my favorite animal Horses are my favorite animal Horses are my favorite animal \u5982\u679c\u6e29\u5ea6\u4e3a 1 1 2 3 4 Horses are my favorite animal Horses are my favorite animals Horses are my favorite ! Horses are my favorite animal \u901a\u5e38\u6700\u597d\u4e3a\u5df2\u5b9a\u4e49\u671f\u671b\u8f93\u51fa\u7684\u4efb\u52a1\u8bbe\u7f6e\u8f83\u4f4e\u7684\u6e29\u5ea6\u3002 \u5bf9\u4e8e\u9700\u8981\u591a\u6837\u6027\u6216\u521b\u9020\u529b\u7684\u4efb\u52a1\uff0c\u8f83\u9ad8\u7684\u6e29\u5ea6\u53ef\u80fd\u4f1a\u6709\u7528\uff0c\u6216\u8005\u5982\u679c\u60a8\u60f3\u751f\u6210\u4e00\u4e9b\u53d8\u4f53\u4f9b\u6700\u7ec8\u7528\u6237\u6216\u4eba\u7c7b\u4e13\u5bb6\u9009\u62e9\u3002 \u5bf9\u4e8e\u60a8\u7684\u5ba0\u7269\u540d\u79f0\u751f\u6210\u5668\uff0c\u60a8\u53ef\u80fd\u5e0c\u671b\u80fd\u591f\u751f\u6210\u8bb8\u591a\u540d\u79f0\u60f3\u6cd5\u3002\u9002\u5ea6\u7684\u6e29\u5ea6 0.6 \u5e94\u8be5\u5f88\u597d\u7528\u3002 4. \u6784\u5efa\u5e94\u7528\u7a0b\u5e8f \u00b6 NODE.JS PYTHON (FLASK) \u73b0\u5728\uff0c\u60a8\u5df2\u7ecf\u627e\u5230\u4e86\u4e00\u4e2a\u597d\u7684\u63d0\u793a\u548c\u8bbe\u7f6e\uff0c\u51c6\u5907\u6784\u5efa\u60a8\u7684\u5ba0\u7269\u540d\u5b57\u751f\u6210\u5668\uff01\u6211\u4eec\u7f16\u5199\u4e86\u4e00\u4e9b\u4ee3\u7801\u6765\u5e2e\u52a9\u60a8\u5165\u95e8\u2014\u2014\u8bf7\u6309\u7167\u4e0b\u9762\u7684\u8bf4\u660e\u4e0b\u8f7d\u4ee3\u7801\u5e76\u8fd0\u884c\u5e94\u7528\u7a0b\u5e8f\u3002 \u8bbe\u7f6e \u00b6 \u5982\u679c\u60a8\u8fd8\u6ca1\u6709\u5b89\u88c5 Node.js\uff0c\u8bf7\u4ece \u6b64\u5904\u5b89\u88c5 \u3002\u7136\u540e\u901a\u8fc7\u514b\u9686\u8be5 \u5b58\u50a8\u5e93 \u4e0b\u8f7d\u4ee3\u7801\u3002 1 git clone https://github.com/openai/openai-quickstart-node.git \u5982\u679c\u60a8\u4e0d\u60f3\u4f7f\u7528 git\uff0c\u60a8\u8fd8\u53ef\u4ee5\u4f7f\u7528\u6b64 zip \u6587\u4ef6 \u4e0b\u8f7d\u4ee3\u7801\u3002 \u6dfb\u52a0 API \u5bc6\u94a5 \u00b6 \u8fdb\u5165\u9879\u76ee\u76ee\u5f55\u5e76\u590d\u5236\u793a\u4f8b\u73af\u5883\u53d8\u91cf\u6587\u4ef6\u3002 1 2 cd openai-quickstart-node cp .env.example .env \u5c06\u60a8\u7684\u79d8\u5bc6 API \u5bc6\u94a5\u590d\u5236\u5e76\u8bbe\u7f6e\u4e3a\u60a8\u65b0\u521b\u5efa\u7684.env \u6587\u4ef6\u4e2d\u7684 OPENAI_API_KEY \u3002 \u5982\u679c\u60a8\u5c1a\u672a\u521b\u5efa\u79d8\u5bc6\u5bc6\u94a5\uff0c\u53ef\u4ee5\u5728\u4e0b\u9762\u8fdb\u884c\u521b\u5efa\u3002 \u79d8\u5bc6\u5bc6\u94a5 \u521b\u5efa\u65e5\u671f \u6700\u8fd1\u4f7f\u7528\u65e5\u671f 1 2 3 4 5 sk-...MdZi 2023 \u5e74 2 \u6708 9 \u65e5 2023 \u5e74 2 \u6708 28 \u65e5 sk-...p9m5 2023 \u5e74 2 \u6708 27 \u65e5 2023 \u5e74 2 \u6708 28 \u65e5 sk-...PwEC 2023 \u5e74 3 \u6708 3 \u65e5 2023 \u5e74 3 \u6708 5 \u65e5 + Create new secret key \u91cd\u8981\u63d0\u793a \u5f53\u4f7f\u7528 Javascript \u65f6\uff0c\u6240\u6709 API \u8c03\u7528\u5e94\u4ec5\u5728\u670d\u52a1\u5668\u7aef\u8fdb\u884c\uff0c\u56e0\u4e3a\u5728\u5ba2\u6237\u7aef\u6d4f\u89c8\u5668\u4ee3\u7801\u4e2d\u8fdb\u884c\u8c03\u7528\u5c06\u66b4\u9732\u60a8\u7684 API \u5bc6\u94a5\u3002\u6709\u5173\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1\u6b64\u5904\u3002 \u8fd0\u884c\u5e94\u7528\u7a0b\u5e8f \u00b6 \u5728\u9879\u76ee\u76ee\u5f55\u4e2d\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u4ee5\u5b89\u88c5\u4f9d\u8d56\u9879\u5e76\u8fd0\u884c\u5e94\u7528\u7a0b\u5e8f\u3002 1 2 npm install npm run dev \u5728\u6d4f\u89c8\u5668\u4e2d\u6253\u5f00 http://localhost:3000 \uff0c\u60a8\u5e94\u8be5\u4f1a\u770b\u5230\u5ba0\u7269\u540d\u5b57\u751f\u6210\u5668\uff01 \u4e86\u89e3\u4ee3\u7801 \u00b6 \u5728 openai-quickstart-node/pages/api \u6587\u4ef6\u5939\u4e2d\u6253\u5f00 generate.js \u3002 \u5728\u5e95\u90e8\uff0c\u60a8\u5c06\u770b\u5230\u751f\u6210\u6211\u4eec\u4e0a\u9762\u4f7f\u7528\u7684\u63d0\u793a\u7684\u51fd\u6570\u3002\u7531\u4e8e\u7528\u6237\u5c06\u8f93\u5165\u5176\u5ba0\u7269\u7684\u52a8\u7269\u7c7b\u578b\uff0c\u56e0\u6b64\u5b83\u4f1a\u52a8\u6001\u66ff\u6362\u6307\u5b9a\u52a8\u7269\u7684\u90e8\u5206\u3002 1 2 3 4 5 6 7 8 9 10 11 function generatePrompt ( animal ) { const capitalizedAnimal = animal [ 0 ]. toUpperCase () + animal . slice ( 1 ). toLowerCase (); return `Suggest three names for an animal that is a superhero. Animal: Cat Names: Captain Sharpclaw, Agent Fluffball, The Incredible Feline Animal: Dog Names: Ruff the Protector, Wonder Canine, Sir Barks-a-Lot Animal: ${ capitalizedAnimal } Names:` ; } \u5728 generate.js \u7684\u7b2c 9 \u884c\uff0c\u60a8\u5c06\u770b\u5230\u53d1\u9001\u5b9e\u9645 API \u8bf7\u6c42\u7684\u4ee3\u7801\u3002\u5982\u4e0a\u6240\u8ff0\uff0c\u5b83\u4f7f\u7528\u6e29\u5ea6\u4e3a 0.6 \u7684 \u8865\u5168 \u7aef\u70b9\u3002 1 2 3 4 5 const completion = await openai . createCompletion ({ model : \"text-davinci-003\" , prompt : generatePrompt ( req . body . animal ), temperature : 0.6 , }); \u5230\u6b64\u4e3a\u6b62\uff01\u60a8\u73b0\u5728\u5e94\u8be5\u5df2\u7ecf\u5b8c\u5168\u4e86\u89e3\u4e86\u5982\u4f55\u4f7f\u7528 OpenAI API \u6784\u5efa\uff08\u8d85\u7ea7\u82f1\u96c4\uff09\u5ba0\u7269\u540d\u5b57\u751f\u6210\u5668\uff01 \u7ed3\u8bed \u00b6 \u8fd9\u4e9b\u6982\u5ff5\u548c\u6280\u672f\u5c06\u6709\u52a9\u4e8e\u60a8\u6784\u5efa\u81ea\u5df1\u7684\u5e94\u7528\u7a0b\u5e8f\u3002\u4e0d\u8fc7\uff0c\u8fd9\u4e2a\u7b80\u5355\u7684\u4f8b\u5b50\u53ea\u662f\u5c55\u793a\u4e86\u53ef\u80fd\u6027\u7684\u4e00\u5c0f\u90e8\u5206\uff01\u8865\u5168\u7aef\u70b9\u8db3\u591f\u7075\u6d3b\uff0c\u53ef\u4ee5\u89e3\u51b3\u51e0\u4e4e\u4efb\u4f55\u8bed\u8a00\u5904\u7406\u4efb\u52a1\uff0c\u5305\u62ec\u5185\u5bb9\u751f\u6210\u3001\u6458\u8981\u3001\u8bed\u4e49\u641c\u7d22\u3001\u4e3b\u9898\u6807\u8bb0\u3001\u60c5\u611f\u5206\u6790\u7b49\u7b49\u3002 \u9700\u8981\u8bb0\u4f4f\u7684\u4e00\u4e2a\u9650\u5236\u662f\uff0c\u5bf9\u4e8e\u5927\u591a\u6570\u6a21\u578b\u800c\u8a00\uff0c\u5355\u4e2a API \u8bf7\u6c42\u53ea\u80fd\u5904\u7406\u5728\u63d0\u793a\u548c\u8865\u5168\u4e4b\u95f4\u6700\u591a 2,048 \u4e2a\u6807\u8bb0\uff08\u5927\u7ea6 1,500 \u4e2a\u5355\u8bcd\uff09\u3002 \u6df1\u5ea6\u6316\u6398->\u6a21\u578b\u548c\u5b9a\u4ef7 \u6211\u4eec\u63d0\u4f9b\u5177\u6709\u4e0d\u540c\u529f\u80fd\u548c \u4ef7\u683c \u7684\u4e00\u7cfb\u5217 \u6a21\u578b \u3002 \u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528\u4e86text-davinci-003\uff0c\u6211\u4eec\u6700\u5f3a\u5927\u7684\u81ea\u7136\u8bed\u8a00\u6a21\u578b\u3002 \u6211\u4eec\u5efa\u8bae\u5728\u5b9e\u9a8c\u65f6\u4f7f\u7528\u6b64\u6a21\u578b\uff0c\u56e0\u4e3a\u5b83\u5c06\u4ea7\u751f\u6700\u597d\u7684\u7ed3\u679c\u3002 \u4e00\u65e6\u60a8\u7684\u5de5\u4f5c\u6b63\u5e38\u8fd0\u884c\uff0c\u60a8\u53ef\u4ee5\u67e5\u770b\u5176\u4ed6\u6a21\u578b\u662f\u5426\u53ef\u4ee5\u4ee5\u66f4\u4f4e\u7684\u5ef6\u8fdf\u548c\u6210\u672c\u4ea7\u751f\u76f8\u540c\u7684\u7ed3\u679c\u3002 \u5355\u4e2a\u8bf7\u6c42\uff08\u63d0\u793a\u548c\u5b8c\u6210\uff09\u5904\u7406\u7684\u6807\u8bb0\u603b\u6570\u4e0d\u80fd\u8d85\u8fc7\u6a21\u578b\u7684\u6700\u5927\u4e0a\u4e0b\u6587\u957f\u5ea6\u3002 \u5bf9\u4e8e\u5927\u591a\u6570\u6a21\u578b\uff0c\u8fd9\u662f2,048\u4e2a\u6807\u8bb0\u6216\u5927\u7ea61,500\u4e2a\u5355\u8bcd\u3002 \u7c97\u7565\u7684\u7ecf\u9a8c\u6cd5\u5219\u662f\uff0c\u5bf9\u4e8e\u82f1\u6587\u6587\u672c\uff0c1\u4e2a\u6807\u8bb0\u5927\u7ea6\u76f8\u5f53\u4e8e4\u4e2a\u5b57\u7b26\u62160.75\u4e2a\u5355\u8bcd\u3002 \u6309\u7167\u6bcf1,000\u4e2a\u6807\u8bb0\u7684\u4f7f\u7528\u91cf\u4ed8\u8d39\uff0c\u524d3\u4e2a\u6708\u53ef\u83b7\u5f975\u7f8e\u5143\u7684\u514d\u8d39\u4fe1\u7528\u989d\u3002 \u4e86\u89e3\u66f4\u591a \u3002 \u5bf9\u4e8e\u66f4\u9ad8\u7ea7\u7684\u4efb\u52a1\uff0c\u60a8\u53ef\u80fd\u5e0c\u671b\u80fd\u591f\u63d0\u4f9b\u6bd4\u5355\u4e2a\u63d0\u793a\u4e2d\u53ef\u4ee5\u5bb9\u7eb3\u7684\u66f4\u591a\u7684\u793a\u4f8b\u6216\u4e0a\u4e0b\u6587\u3002 \u5fae\u8c03 API \u662f\u66f4\u9ad8\u7ea7\u4efb\u52a1\u7684\u4e00\u4e2a\u5f88\u597d\u7684\u9009\u62e9\u3002 **\u5fae\u8c03**\u5141\u8bb8\u60a8\u63d0\u4f9b\u6570\u767e\u751a\u81f3\u6570\u5343\u4e2a\u793a\u4f8b\u6765\u4e3a\u7279\u5b9a\u7528\u4f8b\u5b9a\u5236\u6a21\u578b\u3002 \u4e0b\u4e00\u6b65 \u00b6 \u4e3a\u4e86\u83b7\u5f97\u7075\u611f\u5e76\u4e86\u89e3\u6709\u5173\u4e3a\u4e0d\u540c\u4efb\u52a1\u8bbe\u8ba1\u63d0\u793a\u7684\u66f4\u591a\u4fe1\u606f\uff1a \u9605\u8bfb\u6211\u4eec\u7684 \u8865\u5168 \u6307\u5357\u3002 \u63a2\u7d22\u6211\u4eec\u7684 \u793a\u4f8b\u63d0\u793a\u5e93 \u3002 \u5728 Playground \u4e2d\u5f00\u59cb\u5b9e\u9a8c\u3002 \u5728\u5f00\u59cb\u6784\u5efa\u4e4b\u524d\u8bf7\u7262\u8bb0\u6211\u4eec\u7684 \u4f7f\u7528\u653f\u7b56 \u3002","title":"\u5feb\u901f\u5165\u95e8"},{"location":"quickstart/#_1","text":"OpenAI \u8bad\u7ec3\u4e86\u4e00\u4e9b\u5c16\u7aef\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u5b83\u4eec\u975e\u5e38\u64c5\u957f\u7406\u89e3\u548c\u751f\u6210\u6587\u672c\u3002\u6211\u4eec\u7684 API \u63d0\u4f9b\u4e86\u5bf9\u8fd9\u4e9b\u6a21\u578b\u7684\u8bbf\u95ee\uff0c\u5e76\u53ef\u4ee5\u7528\u4e8e\u89e3\u51b3\u51e0\u4e4e\u6d89\u53ca\u5904\u7406\u8bed\u8a00\u7684\u4efb\u4f55\u4efb\u52a1\u3002 \u5728\u8fd9\u4e2a\u5feb\u901f\u5165\u95e8\u6559\u7a0b\u4e2d\uff0c\u60a8\u5c06\u6784\u5efa\u4e00\u4e2a\u7b80\u5355\u7684\u793a\u4f8b\u5e94\u7528\u7a0b\u5e8f\u3002\u5728\u6b64\u8fc7\u7a0b\u4e2d\uff0c\u60a8\u5c06\u5b66\u4e60\u5230\u5bf9\u4e8e\u4efb\u4f55\u4efb\u52a1\u4f7f\u7528 API \u7684\u57fa\u672c\u6982\u5ff5\u548c\u6280\u672f\uff0c\u5305\u62ec\uff1a \u5185\u5bb9\u751f\u6210 \u6458\u8981 \u5206\u7c7b\u3001\u5206\u7c7b\u548c\u60c5\u611f\u5206\u6790 \u6570\u636e\u63d0\u53d6 \u7ffb\u8bd1 \u8fd8\u6709\u66f4\u591a\uff01","title":"\u5feb\u901f\u5165\u95e8"},{"location":"quickstart/#_2","text":"\u8865\u5168 \u7aef\u70b9\u662f\u6211\u4eec API \u7684\u6838\u5fc3\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u975e\u5e38\u7075\u6d3b\u548c\u5f3a\u5927\u7684\u7b80\u5355\u63a5\u53e3\u3002 \u60a8\u5c06\u4e00\u4e9b\u6587\u672c\u4f5c\u4e3a\u63d0\u793a\u8f93\u5165\uff0cAPI \u5c06\u8fd4\u56de\u4e00\u4e2a\u6587\u672c\u8865\u5168\uff0c\u8bd5\u56fe\u5339\u914d\u60a8\u7ed9\u51fa\u7684\u4efb\u4f55\u6307\u4ee4\u6216\u4e0a\u4e0b\u6587\u3002 Prompt-\u63d0\u793a \u4e3a\u51b0\u6fc0\u51cc\u5e97\u5199\u4e00\u4e2a\u6807\u8bed\u3002 Completion-\u8865\u5168 \u6211\u4eec\u6bcf\u676f\u90fd\u63d0\u4f9b\u5fae\u7b11\uff01 \u60a8\u53ef\u4ee5\u5c06\u5176\u89c6\u4e3a\u975e\u5e38\u9ad8\u7ea7\u7684\u81ea\u52a8\u8865\u5168 - \u6a21\u578b\u5904\u7406\u60a8\u7684\u6587\u672c\u63d0\u793a\uff0c\u5e76\u5c1d\u8bd5\u9884\u6d4b\u6700\u6709\u53ef\u80fd\u51fa\u73b0\u7684\u5185\u5bb9\u3002","title":"\u4ecb\u7ecd"},{"location":"quickstart/#1","text":"\u60f3\u8c61\u4e00\u4e0b\u60a8\u60f3\u521b\u5efa\u4e00\u4e2a\u5ba0\u7269\u540d\u5b57\u751f\u6210\u5668\u3002\u4ece\u96f6\u5f00\u59cb\u60f3\u51fa\u540d\u5b57\u5f88\u96be\uff01 \u9996\u5148\uff0c\u60a8\u9700\u8981\u4e00\u4e2a\u660e\u786e\u60a8\u60f3\u8981\u7684\u63d0\u793a\u3002\u8ba9\u6211\u4eec\u4ece\u4e00\u6761\u6307\u4ee4\u5f00\u59cb\u3002 **\u5c06\u6b64\u63d0\u793a\u63d0\u4ea4**\u4ee5\u751f\u6210\u60a8\u7684\u7b2c\u4e00\u4e2a\u8865\u5168\u3002 1 \u4e3a\u9a6c\u63d0\u4f9b\u4e00\u4e2a\u540d\u5b57\u3002 \u4e0d\u9519\uff01\u73b0\u5728\uff0c\u8bd5\u7740\u8ba9\u60a8\u7684\u6307\u4ee4\u66f4\u5177\u4f53\u3002 \u4e3a\u4e00\u5339\u9ed1\u9a6c\u5efa\u8bae\u4e00\u4e2a\u540d\u5b57\u3002 \u6b63\u5982\u60a8\u6240\u89c1\uff0c\u5411\u6211\u4eec\u7684\u63d0\u793a\u6dfb\u52a0\u4e00\u4e2a\u7b80\u5355\u7684\u5f62\u5bb9\u8bcd\u4f1a\u6539\u53d8\u7ed3\u679c\u7684\u8865\u5168\u3002\u8bbe\u8ba1\u63d0\u793a\u672c\u8d28\u4e0a\u5c31\u662f\u5982\u4f55\u201c\u7f16\u7a0b\u201d\u6a21\u578b\u3002","title":"1. \u4ece\u4e00\u6761\u6307\u4ee4\u5f00\u59cb"},{"location":"quickstart/#2","text":"\u8bbe\u8ba1\u826f\u597d\u7684\u6307\u4ee4\u5bf9\u4e8e\u83b7\u5f97\u826f\u597d\u7684\u7ed3\u679c\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u6709\u65f6\u5b83\u4eec\u8fd8\u4e0d\u591f\u3002\u8ba9\u6211\u4eec\u5c1d\u8bd5\u4f7f\u60a8\u7684\u6307\u4ee4\u66f4\u52a0\u590d\u6742\u3002 1 \u4e3a\u4e00\u4f4d\u8d85\u7ea7\u82f1\u96c4\u7684\u9a6c\u5efa\u8bae\u4e09\u4e2a\u540d\u5b57\u3002 \u8fd9\u4e2a\u8865\u5168\u5e76\u4e0d\u662f\u6211\u4eec\u60f3\u8981\u7684\u3002\u8fd9\u4e9b\u540d\u79f0\u975e\u5e38\u901a\u7528\uff0c\u5e76\u4e14\u4f3c\u4e4e\u6a21\u578b\u6ca1\u6709\u6ce8\u610f\u5230\u6211\u4eec\u6307\u4ee4\u4e2d\u7684\u9a6c\u3002\u8ba9\u6211\u4eec\u770b\u770b\u662f\u5426\u53ef\u4ee5\u8ba9\u5b83\u63d0\u51fa\u66f4\u76f8\u5173\u7684\u5efa\u8bae\u3002 \u5728\u8bb8\u591a\u60c5\u51b5\u4e0b\uff0c\u5411\u6a21\u578b\u5c55\u793a\u548c\u544a\u8bc9\u60a8\u60f3\u8981\u7684\u662f\u6709\u5e2e\u52a9\u7684\u3002\u5c06\u793a\u4f8b\u6dfb\u52a0\u5230\u60a8\u7684\u63d0\u793a\u4e2d\u53ef\u4ee5\u5e2e\u52a9\u4f20\u8fbe\u6a21\u5f0f\u6216\u7ec6\u5fae\u5dee\u522b\u3002\u5c1d\u8bd5\u63d0\u4ea4\u5305\u542b\u4e00\u4e9b\u793a\u4f8b\u7684\u6b64\u63d0\u793a\u3002 1 2 3 4 5 6 7 8 \u4e3a\u4e00\u4e2a\u8d85\u7ea7\u82f1\u96c4\u7684\u52a8\u7269\u5efa\u8bae\u4e09\u4e2a\u540d\u5b57\u3002 \u52a8\u7269\uff1a\u732b \u540d\u79f0\uff1aSharpclaw \u961f\u957f\uff0cFluffball \u7279\u5de5\uff0cIncredible Feline \u52a8\u7269\uff1a\u72d7 \u540d\u79f0\uff1aRuff the Protector\uff0cWonder Canine\uff0cSir Barks-a-Lot \u52a8\u7269\uff1a\u9a6c \u540d\u79f0\uff1a \u4e0d\u9519\uff01\u4e3a\u7ed9\u5b9a\u7684\u8f93\u5165\u6dfb\u52a0\u8f93\u51fa\u793a\u4f8b\u53ef\u4ee5\u5e2e\u52a9\u6a21\u578b\u63d0\u4f9b\u6211\u4eec\u6240\u5bfb\u627e\u7684\u540d\u79f0\u7c7b\u578b\u3002","title":"2. \u6dfb\u52a0\u4e00\u4e9b\u793a\u4f8b"},{"location":"quickstart/#3","text":"\u63d0\u793a\u8bbe\u8ba1\u5e76\u4e0d\u662f\u60a8\u62e5\u6709\u7684\u552f\u4e00\u5de5\u5177\u3002\u60a8\u8fd8\u53ef\u4ee5\u901a\u8fc7\u8c03\u6574\u8bbe\u7f6e\u6765\u63a7\u5236\u8865\u5168\u3002\u5176\u4e2d\u6700\u91cd\u8981\u7684\u8bbe\u7f6e\u4e4b\u4e00\u79f0\u4e3a**\u6e29\u5ea6**\u3002 \u60a8\u53ef\u80fd\u5df2\u7ecf\u6ce8\u610f\u5230\uff0c\u5982\u679c\u5728\u4e0a\u9762\u7684\u793a\u4f8b\u4e2d\u591a\u6b21\u63d0\u4ea4\u76f8\u540c\u7684\u63d0\u793a\uff0c\u6a21\u578b\u5c06\u59cb\u7ec8\u8fd4\u56de\u76f8\u540c\u6216\u975e\u5e38\u76f8\u4f3c\u7684\u8865\u5168\u3002\u8fd9\u662f\u56e0\u4e3a\u60a8\u7684\u6e29\u5ea6\u8bbe\u7f6e\u4e3a 0 \u3002 \u5c1d\u8bd5\u4f7f\u7528\u6e29\u5ea6\u8bbe\u7f6e\u4e3a 1 \u91cd\u65b0\u63d0\u4ea4\u76f8\u540c\u7684\u63d0\u793a\u51e0\u6b21\u3002 1 2 3 4 5 6 7 8 9 10 \u4e3a\u4e00\u4e2a\u8d85\u7ea7\u82f1\u96c4\u7684\u52a8\u7269\u63d0\u51fa\u4e09\u4e2a\u540d\u5b57\u3002 \u52a8\u7269\uff1a\u732b \u540d\u79f0\uff1aSharpclaw \u961f\u957f\u3001Fluffball \u7279\u5de5\u3001Incredible Feline \u52a8\u7269\uff1a\u72d7 \u540d\u79f0\uff1a\u4fdd\u62a4\u8005\u62c9\u592b\u3001Wonder Canine\u3001Sir Barks-a-Lot \u52a8\u7269\uff1a\u9a6c \u540d\u79f0\uff1a \u6e29\u5ea6 0-1 \u770b\u770b\u53d1\u751f\u4e86\u4ec0\u4e48\uff1f\u5f53\u6e29\u5ea6\u9ad8\u4e8e 0 \u65f6\uff0c\u591a\u6b21\u63d0\u4ea4\u76f8\u540c\u7684\u63d0\u793a\u4f1a\u5bfc\u81f4\u4e0d\u540c\u7684\u8865\u5168\u7ed3\u679c\u3002 \u8bf7\u8bb0\u4f4f\uff0c\u6a21\u578b\u9884\u6d4b\u5728\u5176\u4e4b\u524d\u7684\u6587\u672c\u4e4b\u540e\u6700\u53ef\u80fd\u51fa\u73b0\u7684\u6587\u672c\u3002 \u6e29\u5ea6\u662f\u4e00\u4e2a\u4ecb\u4e8e 0 \u548c 1 \u4e4b\u95f4\u7684\u503c\uff0c\u57fa\u672c\u4e0a\u8ba9\u60a8\u63a7\u5236\u6a21\u578b\u5728\u8fdb\u884c\u8fd9\u4e9b\u9884\u6d4b\u65f6\u5e94\u8be5\u6709\u591a\u81ea\u4fe1\u3002 \u964d\u4f4e\u6e29\u5ea6\u610f\u5473\u7740\u5b83\u5c06\u5192\u66f4\u5c11\u7684\u98ce\u9669\uff0c\u8865\u5168\u5c06\u66f4\u51c6\u786e\u548c\u786e\u5b9a\u6027\u3002\u589e\u52a0\u6e29\u5ea6\u5c06\u5bfc\u81f4\u66f4\u591a\u7684\u591a\u6837\u5316\u8865\u5168\u3002 \u6df1\u5165\u6316\u6398->\u7406\u89e3\u6807\u8bb0\u548c\u6982\u7387 \u6211\u4eec\u7684\u6a21\u578b\u901a\u8fc7\u5c06\u6587\u672c\u62c6\u5206\u6210\u79f0\u4e3a\u6807\u8bb0\u7684\u8f83\u5c0f\u5355\u4f4d\u6765\u5904\u7406\u6587\u672c\u3002 \u6807\u8bb0\u53ef\u4ee5\u662f\u5355\u8bcd\u3001\u5355\u8bcd\u5757\u6216\u5355\u4e2a\u5b57\u7b26\u3002\u7f16\u8f91\u4e0b\u9762\u7684\u6587\u672c\u4ee5\u67e5\u770b\u5982\u4f55\u5c06\u5176\u6807\u8bb0\u5316\u3002 1 I have an orange cat named Butterscotch. 1 I have an orange cat named Butterscotch. \u50cf\u201ccat\u201d\u8fd9\u6837\u7684\u5e38\u7528\u8bcd\u662f\u4e00\u4e2a\u6807\u8bb0\uff0c\u800c\u4e0d\u5e38\u7528\u7684\u8bcd\u901a\u5e38\u4f1a\u88ab\u62c6\u5206\u6210\u591a\u4e2a\u6807\u8bb0\u3002 \u4f8b\u5982\uff0c\u201cButterscotch\u201d\u53ef\u4ee5\u7ffb\u8bd1\u6210\u56db\u4e2a\u6807\u8bb0\uff1a\u201cBut\u201d\u3001\u201cters\u201d\u3001\u201ccot\u201d\u548c\u201cch\u201d\u3002 \u8bb8\u591a\u6807\u8bb0\u4ee5\u7a7a\u683c\u5f00\u5934\uff0c\u4f8b\u5982\u201c hello\u201d\u548c\u201c bye\u201d\u3002 \u5bf9\u4e8e\u4e00\u4e9b\u6587\u672c\uff0c\u6a21\u578b\u786e\u5b9a\u4e0b\u4e00\u4e2a\u6700\u53ef\u80fd\u51fa\u73b0\u7684\u6807\u8bb0\u662f\u54ea\u4e2a\u3002 \u4f8b\u5982\uff0c\u6587\u672c\u201cHorses are my favorite\u201d\u6700\u6709\u53ef\u80fd\u540e\u8ddf\u6807\u8bb0\u201canimal\u201d\u3002 1 2 3 4 5 6 Horses are my favorite animal 49.65% animals 42.58% \\n 3.49% ! 0.91% \u5982\u679c\u5c06\u6e29\u5ea6\u8bbe\u7f6e\u4e3a 0 \u5e76\u63d0\u4ea4\u6b64\u63d0\u793a 4 \u6b21\uff0c\u6a21\u578b\u5c06\u59cb\u7ec8\u8fd4\u56de\u201canimal\u201d\u4f5c\u4e3a\u4e0b\u4e00\u4e2a\u6807\u8bb0\uff0c\u56e0\u4e3a\u5b83\u5177\u6709\u6700\u9ad8\u7684\u6982\u7387\u3002 \u5982\u679c\u589e\u52a0\u6e29\u5ea6\uff0c\u5b83\u5c06\u66f4\u5192\u9669\uff0c\u8003\u8651\u6982\u7387\u8f83\u4f4e\u7684\u6807\u8bb0\u3002 \u5982\u679c\u6e29\u5ea6\u4e3a 0 1 2 3 4 Horses are my favorite animal Horses are my favorite animal Horses are my favorite animal Horses are my favorite animal \u5982\u679c\u6e29\u5ea6\u4e3a 1 1 2 3 4 Horses are my favorite animal Horses are my favorite animals Horses are my favorite ! Horses are my favorite animal \u901a\u5e38\u6700\u597d\u4e3a\u5df2\u5b9a\u4e49\u671f\u671b\u8f93\u51fa\u7684\u4efb\u52a1\u8bbe\u7f6e\u8f83\u4f4e\u7684\u6e29\u5ea6\u3002 \u5bf9\u4e8e\u9700\u8981\u591a\u6837\u6027\u6216\u521b\u9020\u529b\u7684\u4efb\u52a1\uff0c\u8f83\u9ad8\u7684\u6e29\u5ea6\u53ef\u80fd\u4f1a\u6709\u7528\uff0c\u6216\u8005\u5982\u679c\u60a8\u60f3\u751f\u6210\u4e00\u4e9b\u53d8\u4f53\u4f9b\u6700\u7ec8\u7528\u6237\u6216\u4eba\u7c7b\u4e13\u5bb6\u9009\u62e9\u3002 \u5bf9\u4e8e\u60a8\u7684\u5ba0\u7269\u540d\u79f0\u751f\u6210\u5668\uff0c\u60a8\u53ef\u80fd\u5e0c\u671b\u80fd\u591f\u751f\u6210\u8bb8\u591a\u540d\u79f0\u60f3\u6cd5\u3002\u9002\u5ea6\u7684\u6e29\u5ea6 0.6 \u5e94\u8be5\u5f88\u597d\u7528\u3002","title":"3. \u8c03\u6574\u60a8\u7684\u8bbe\u7f6e"},{"location":"quickstart/#4","text":"NODE.JS PYTHON (FLASK) \u73b0\u5728\uff0c\u60a8\u5df2\u7ecf\u627e\u5230\u4e86\u4e00\u4e2a\u597d\u7684\u63d0\u793a\u548c\u8bbe\u7f6e\uff0c\u51c6\u5907\u6784\u5efa\u60a8\u7684\u5ba0\u7269\u540d\u5b57\u751f\u6210\u5668\uff01\u6211\u4eec\u7f16\u5199\u4e86\u4e00\u4e9b\u4ee3\u7801\u6765\u5e2e\u52a9\u60a8\u5165\u95e8\u2014\u2014\u8bf7\u6309\u7167\u4e0b\u9762\u7684\u8bf4\u660e\u4e0b\u8f7d\u4ee3\u7801\u5e76\u8fd0\u884c\u5e94\u7528\u7a0b\u5e8f\u3002","title":"4. \u6784\u5efa\u5e94\u7528\u7a0b\u5e8f"},{"location":"quickstart/#_3","text":"\u5982\u679c\u60a8\u8fd8\u6ca1\u6709\u5b89\u88c5 Node.js\uff0c\u8bf7\u4ece \u6b64\u5904\u5b89\u88c5 \u3002\u7136\u540e\u901a\u8fc7\u514b\u9686\u8be5 \u5b58\u50a8\u5e93 \u4e0b\u8f7d\u4ee3\u7801\u3002 1 git clone https://github.com/openai/openai-quickstart-node.git \u5982\u679c\u60a8\u4e0d\u60f3\u4f7f\u7528 git\uff0c\u60a8\u8fd8\u53ef\u4ee5\u4f7f\u7528\u6b64 zip \u6587\u4ef6 \u4e0b\u8f7d\u4ee3\u7801\u3002","title":"\u8bbe\u7f6e"},{"location":"quickstart/#api","text":"\u8fdb\u5165\u9879\u76ee\u76ee\u5f55\u5e76\u590d\u5236\u793a\u4f8b\u73af\u5883\u53d8\u91cf\u6587\u4ef6\u3002 1 2 cd openai-quickstart-node cp .env.example .env \u5c06\u60a8\u7684\u79d8\u5bc6 API \u5bc6\u94a5\u590d\u5236\u5e76\u8bbe\u7f6e\u4e3a\u60a8\u65b0\u521b\u5efa\u7684.env \u6587\u4ef6\u4e2d\u7684 OPENAI_API_KEY \u3002 \u5982\u679c\u60a8\u5c1a\u672a\u521b\u5efa\u79d8\u5bc6\u5bc6\u94a5\uff0c\u53ef\u4ee5\u5728\u4e0b\u9762\u8fdb\u884c\u521b\u5efa\u3002 \u79d8\u5bc6\u5bc6\u94a5 \u521b\u5efa\u65e5\u671f \u6700\u8fd1\u4f7f\u7528\u65e5\u671f 1 2 3 4 5 sk-...MdZi 2023 \u5e74 2 \u6708 9 \u65e5 2023 \u5e74 2 \u6708 28 \u65e5 sk-...p9m5 2023 \u5e74 2 \u6708 27 \u65e5 2023 \u5e74 2 \u6708 28 \u65e5 sk-...PwEC 2023 \u5e74 3 \u6708 3 \u65e5 2023 \u5e74 3 \u6708 5 \u65e5 + Create new secret key \u91cd\u8981\u63d0\u793a \u5f53\u4f7f\u7528 Javascript \u65f6\uff0c\u6240\u6709 API \u8c03\u7528\u5e94\u4ec5\u5728\u670d\u52a1\u5668\u7aef\u8fdb\u884c\uff0c\u56e0\u4e3a\u5728\u5ba2\u6237\u7aef\u6d4f\u89c8\u5668\u4ee3\u7801\u4e2d\u8fdb\u884c\u8c03\u7528\u5c06\u66b4\u9732\u60a8\u7684 API \u5bc6\u94a5\u3002\u6709\u5173\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1\u6b64\u5904\u3002","title":"\u6dfb\u52a0 API \u5bc6\u94a5"},{"location":"quickstart/#_4","text":"\u5728\u9879\u76ee\u76ee\u5f55\u4e2d\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u4ee5\u5b89\u88c5\u4f9d\u8d56\u9879\u5e76\u8fd0\u884c\u5e94\u7528\u7a0b\u5e8f\u3002 1 2 npm install npm run dev \u5728\u6d4f\u89c8\u5668\u4e2d\u6253\u5f00 http://localhost:3000 \uff0c\u60a8\u5e94\u8be5\u4f1a\u770b\u5230\u5ba0\u7269\u540d\u5b57\u751f\u6210\u5668\uff01","title":"\u8fd0\u884c\u5e94\u7528\u7a0b\u5e8f"},{"location":"quickstart/#_5","text":"\u5728 openai-quickstart-node/pages/api \u6587\u4ef6\u5939\u4e2d\u6253\u5f00 generate.js \u3002 \u5728\u5e95\u90e8\uff0c\u60a8\u5c06\u770b\u5230\u751f\u6210\u6211\u4eec\u4e0a\u9762\u4f7f\u7528\u7684\u63d0\u793a\u7684\u51fd\u6570\u3002\u7531\u4e8e\u7528\u6237\u5c06\u8f93\u5165\u5176\u5ba0\u7269\u7684\u52a8\u7269\u7c7b\u578b\uff0c\u56e0\u6b64\u5b83\u4f1a\u52a8\u6001\u66ff\u6362\u6307\u5b9a\u52a8\u7269\u7684\u90e8\u5206\u3002 1 2 3 4 5 6 7 8 9 10 11 function generatePrompt ( animal ) { const capitalizedAnimal = animal [ 0 ]. toUpperCase () + animal . slice ( 1 ). toLowerCase (); return `Suggest three names for an animal that is a superhero. Animal: Cat Names: Captain Sharpclaw, Agent Fluffball, The Incredible Feline Animal: Dog Names: Ruff the Protector, Wonder Canine, Sir Barks-a-Lot Animal: ${ capitalizedAnimal } Names:` ; } \u5728 generate.js \u7684\u7b2c 9 \u884c\uff0c\u60a8\u5c06\u770b\u5230\u53d1\u9001\u5b9e\u9645 API \u8bf7\u6c42\u7684\u4ee3\u7801\u3002\u5982\u4e0a\u6240\u8ff0\uff0c\u5b83\u4f7f\u7528\u6e29\u5ea6\u4e3a 0.6 \u7684 \u8865\u5168 \u7aef\u70b9\u3002 1 2 3 4 5 const completion = await openai . createCompletion ({ model : \"text-davinci-003\" , prompt : generatePrompt ( req . body . animal ), temperature : 0.6 , }); \u5230\u6b64\u4e3a\u6b62\uff01\u60a8\u73b0\u5728\u5e94\u8be5\u5df2\u7ecf\u5b8c\u5168\u4e86\u89e3\u4e86\u5982\u4f55\u4f7f\u7528 OpenAI API \u6784\u5efa\uff08\u8d85\u7ea7\u82f1\u96c4\uff09\u5ba0\u7269\u540d\u5b57\u751f\u6210\u5668\uff01","title":"\u4e86\u89e3\u4ee3\u7801"},{"location":"quickstart/#_6","text":"\u8fd9\u4e9b\u6982\u5ff5\u548c\u6280\u672f\u5c06\u6709\u52a9\u4e8e\u60a8\u6784\u5efa\u81ea\u5df1\u7684\u5e94\u7528\u7a0b\u5e8f\u3002\u4e0d\u8fc7\uff0c\u8fd9\u4e2a\u7b80\u5355\u7684\u4f8b\u5b50\u53ea\u662f\u5c55\u793a\u4e86\u53ef\u80fd\u6027\u7684\u4e00\u5c0f\u90e8\u5206\uff01\u8865\u5168\u7aef\u70b9\u8db3\u591f\u7075\u6d3b\uff0c\u53ef\u4ee5\u89e3\u51b3\u51e0\u4e4e\u4efb\u4f55\u8bed\u8a00\u5904\u7406\u4efb\u52a1\uff0c\u5305\u62ec\u5185\u5bb9\u751f\u6210\u3001\u6458\u8981\u3001\u8bed\u4e49\u641c\u7d22\u3001\u4e3b\u9898\u6807\u8bb0\u3001\u60c5\u611f\u5206\u6790\u7b49\u7b49\u3002 \u9700\u8981\u8bb0\u4f4f\u7684\u4e00\u4e2a\u9650\u5236\u662f\uff0c\u5bf9\u4e8e\u5927\u591a\u6570\u6a21\u578b\u800c\u8a00\uff0c\u5355\u4e2a API \u8bf7\u6c42\u53ea\u80fd\u5904\u7406\u5728\u63d0\u793a\u548c\u8865\u5168\u4e4b\u95f4\u6700\u591a 2,048 \u4e2a\u6807\u8bb0\uff08\u5927\u7ea6 1,500 \u4e2a\u5355\u8bcd\uff09\u3002 \u6df1\u5ea6\u6316\u6398->\u6a21\u578b\u548c\u5b9a\u4ef7 \u6211\u4eec\u63d0\u4f9b\u5177\u6709\u4e0d\u540c\u529f\u80fd\u548c \u4ef7\u683c \u7684\u4e00\u7cfb\u5217 \u6a21\u578b \u3002 \u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528\u4e86text-davinci-003\uff0c\u6211\u4eec\u6700\u5f3a\u5927\u7684\u81ea\u7136\u8bed\u8a00\u6a21\u578b\u3002 \u6211\u4eec\u5efa\u8bae\u5728\u5b9e\u9a8c\u65f6\u4f7f\u7528\u6b64\u6a21\u578b\uff0c\u56e0\u4e3a\u5b83\u5c06\u4ea7\u751f\u6700\u597d\u7684\u7ed3\u679c\u3002 \u4e00\u65e6\u60a8\u7684\u5de5\u4f5c\u6b63\u5e38\u8fd0\u884c\uff0c\u60a8\u53ef\u4ee5\u67e5\u770b\u5176\u4ed6\u6a21\u578b\u662f\u5426\u53ef\u4ee5\u4ee5\u66f4\u4f4e\u7684\u5ef6\u8fdf\u548c\u6210\u672c\u4ea7\u751f\u76f8\u540c\u7684\u7ed3\u679c\u3002 \u5355\u4e2a\u8bf7\u6c42\uff08\u63d0\u793a\u548c\u5b8c\u6210\uff09\u5904\u7406\u7684\u6807\u8bb0\u603b\u6570\u4e0d\u80fd\u8d85\u8fc7\u6a21\u578b\u7684\u6700\u5927\u4e0a\u4e0b\u6587\u957f\u5ea6\u3002 \u5bf9\u4e8e\u5927\u591a\u6570\u6a21\u578b\uff0c\u8fd9\u662f2,048\u4e2a\u6807\u8bb0\u6216\u5927\u7ea61,500\u4e2a\u5355\u8bcd\u3002 \u7c97\u7565\u7684\u7ecf\u9a8c\u6cd5\u5219\u662f\uff0c\u5bf9\u4e8e\u82f1\u6587\u6587\u672c\uff0c1\u4e2a\u6807\u8bb0\u5927\u7ea6\u76f8\u5f53\u4e8e4\u4e2a\u5b57\u7b26\u62160.75\u4e2a\u5355\u8bcd\u3002 \u6309\u7167\u6bcf1,000\u4e2a\u6807\u8bb0\u7684\u4f7f\u7528\u91cf\u4ed8\u8d39\uff0c\u524d3\u4e2a\u6708\u53ef\u83b7\u5f975\u7f8e\u5143\u7684\u514d\u8d39\u4fe1\u7528\u989d\u3002 \u4e86\u89e3\u66f4\u591a \u3002 \u5bf9\u4e8e\u66f4\u9ad8\u7ea7\u7684\u4efb\u52a1\uff0c\u60a8\u53ef\u80fd\u5e0c\u671b\u80fd\u591f\u63d0\u4f9b\u6bd4\u5355\u4e2a\u63d0\u793a\u4e2d\u53ef\u4ee5\u5bb9\u7eb3\u7684\u66f4\u591a\u7684\u793a\u4f8b\u6216\u4e0a\u4e0b\u6587\u3002 \u5fae\u8c03 API \u662f\u66f4\u9ad8\u7ea7\u4efb\u52a1\u7684\u4e00\u4e2a\u5f88\u597d\u7684\u9009\u62e9\u3002 **\u5fae\u8c03**\u5141\u8bb8\u60a8\u63d0\u4f9b\u6570\u767e\u751a\u81f3\u6570\u5343\u4e2a\u793a\u4f8b\u6765\u4e3a\u7279\u5b9a\u7528\u4f8b\u5b9a\u5236\u6a21\u578b\u3002","title":"\u7ed3\u8bed"},{"location":"quickstart/#_7","text":"\u4e3a\u4e86\u83b7\u5f97\u7075\u611f\u5e76\u4e86\u89e3\u6709\u5173\u4e3a\u4e0d\u540c\u4efb\u52a1\u8bbe\u8ba1\u63d0\u793a\u7684\u66f4\u591a\u4fe1\u606f\uff1a \u9605\u8bfb\u6211\u4eec\u7684 \u8865\u5168 \u6307\u5357\u3002 \u63a2\u7d22\u6211\u4eec\u7684 \u793a\u4f8b\u63d0\u793a\u5e93 \u3002 \u5728 Playground \u4e2d\u5f00\u59cb\u5b9e\u9a8c\u3002 \u5728\u5f00\u59cb\u6784\u5efa\u4e4b\u524d\u8bf7\u7262\u8bb0\u6211\u4eec\u7684 \u4f7f\u7528\u653f\u7b56 \u3002","title":"\u4e0b\u4e00\u6b65"},{"location":"tags/","text":"\u6807\u7b7e \u00b6","title":"\u6807\u7b7e"},{"location":"tags/#_1","text":"","title":"\u6807\u7b7e"},{"location":"tutorials/","text":"\u6559\u7a0b \u00b6 \u901a\u8fc7\u9010\u6b65\u6784\u5efa\u771f\u6b63\u7684 AI \u5e94\u7528\u7a0b\u5e8f\uff0c\u5f00\u59cb\u4f7f\u7528 OpenAI API\u3002 Website Q&A with Embeddings \u00b6 \u5b66\u4e60\u5982\u4f55\u5efa\u7acb\u4e00\u4e2a AI\uff0c\u53ef\u4ee5\u56de\u7b54\u5173\u4e8e\u4f60\u7684\u7f51\u7ad9\u7684\u95ee\u9898 Coming soon \u00b6 \u4e86\u89e3\u5982\u4f55\u6784\u5efa\u548c\u90e8\u7f72\u53ef\u4ee5\u56de\u7b54\u6709\u5173\u672c\u5730\u6587\u4ef6\u7684\u95ee\u9898\u7684 AI Coming Soon \u00b6 \u4e86\u89e3\u5982\u4f55\u6784\u5efa\u548c\u90e8\u7f72\u4e86\u89e3\u591a\u4e2a\u77e5\u8bc6\u5e93\u7684 AI \u804a\u5929\u673a\u5668\u4eba \u60f3\u8981\u66f4\u591a\u7684\u70b9\u5b50\u5417?\u67e5\u770b\u6211\u4eec\u7684 \u793a\u4f8b\u5e93 \u6216 GitHub \u4e0a\u7684 OpenAI \u70f9\u996a\u4e66 \u3002","title":"\u6559\u7a0b"},{"location":"tutorials/#_1","text":"\u901a\u8fc7\u9010\u6b65\u6784\u5efa\u771f\u6b63\u7684 AI \u5e94\u7528\u7a0b\u5e8f\uff0c\u5f00\u59cb\u4f7f\u7528 OpenAI API\u3002","title":"\u6559\u7a0b"},{"location":"tutorials/#website-qa-with-embeddings","text":"\u5b66\u4e60\u5982\u4f55\u5efa\u7acb\u4e00\u4e2a AI\uff0c\u53ef\u4ee5\u56de\u7b54\u5173\u4e8e\u4f60\u7684\u7f51\u7ad9\u7684\u95ee\u9898","title":"Website Q&amp;A with Embeddings"},{"location":"tutorials/#coming-soon","text":"\u4e86\u89e3\u5982\u4f55\u6784\u5efa\u548c\u90e8\u7f72\u53ef\u4ee5\u56de\u7b54\u6709\u5173\u672c\u5730\u6587\u4ef6\u7684\u95ee\u9898\u7684 AI","title":"Coming soon"},{"location":"tutorials/#coming-soon_1","text":"\u4e86\u89e3\u5982\u4f55\u6784\u5efa\u548c\u90e8\u7f72\u4e86\u89e3\u591a\u4e2a\u77e5\u8bc6\u5e93\u7684 AI \u804a\u5929\u673a\u5668\u4eba \u60f3\u8981\u66f4\u591a\u7684\u70b9\u5b50\u5417?\u67e5\u770b\u6211\u4eec\u7684 \u793a\u4f8b\u5e93 \u6216 GitHub \u4e0a\u7684 OpenAI \u70f9\u996a\u4e66 \u3002","title":"Coming Soon"},{"location":"usage-policies/","text":"Usage policies \u00b6 Updated Feb 15 th , 2023 We want everyone to use our tools safely and responsibly. That\u2019s why we've created usage policies that apply to all users of OpenAI\u2019s models, tools, and services. By following them, you'll ensure that our technology is used for good. If we discover that your product or usage doesn't follow these policies, we may ask you to make necessary changes. Repeated or serious violations may result in further action, including suspending or terminating your account. Our policies may change as we learn more about use and abuse of our models. Platform policy Our API is being used to power businesses across many sectors and technology platforms. From iOS Apps to websites to Slack, the simplicity of our API makes it possible to integrate into a wide array of use cases. Subject to the use case restrictions mentioned below, we allow the integration of our API into products on all major technology platforms, app stores, and beyond. Disallowed usage We don\u2019t allow the use of our models for the following: Illegal activity Child Sexual Abuse Material or any content that exploits or harms children Generation of hateful, harassing, or violent content Generation of malware Activity that has high risk of physical harm Activity that has high risk of economic harm Fraudulent or deceptive activity Adult content, adult industries, and dating apps Political campaigning or lobbying Activity that violates people\u2019s privacy Engaging in the unauthorized practice of law, or offering tailored legal advice without a qualified person reviewing the information Offering tailored financial advice without a qualified person reviewing the information Telling someone that they have or do not have a certain health condition, or providing instructions on how to cure or treat a health condition High risk government decision-making We have further requirements for certain uses of our models: Consumer-facing uses of our models in medical, financial, and legal industries; in news generation or news summarization; and where else warranted, must provide a disclaimer to users informing them that AI is being used and of its potential limitations. Automated systems (including conversational AI and chatbots) must disclose to users that they are interacting with an AI system. With the exception of chatbots that depict historical public figures, products that simulate another person must either have that person's explicit consent or be clearly labeled as \"simulated\" or \"parody.\" Use of model outputs in livestreams, demonstrations, and research are subject to our Sharing & Publication Policy. You can use our free moderation endpoint and safety best practices to help you keep your app safe. Changelog 2023-02-15: We\u2019ve combined our use case and content policies into a single set of usage policies, and have provided more specific guidance on what activity we disallow in industries we\u2019ve considered high risk. 2022-11-09: We no longer require you to register your applications with OpenAI. Instead, we'll be using a combination of automated and manual methods to monitor for policy violations. 2022-10-25: Updated App Review process (devs no longer need to wait for approval after submitting as long as they comply with our policies). Moved to an outcomes-based approach and updated Safety Best Practices. 2022-06-07: Refactored into categories of applications and corresponding requirements 2022-03-09: Refactored into \"App Review\" 2022-01-19: Simplified copywriting and article writing/editing guidelines 2021-11-15: Addition of \"Content guidelines\" section; changes to bullets on almost always approved uses and disallowed uses; renaming document from \"Use case guidelines\" to \"Usage guidelines\". 2021-08-04: Updated with information related to code generation 2021-03-12: Added detailed case-by-case requirements; small copy and ordering edits 2021-02-26: Clarified the impermissibility of Tweet / Instagram generators","title":"Usage policies"},{"location":"usage-policies/#usage-policies","text":"Updated Feb 15 th , 2023 We want everyone to use our tools safely and responsibly. That\u2019s why we've created usage policies that apply to all users of OpenAI\u2019s models, tools, and services. By following them, you'll ensure that our technology is used for good. If we discover that your product or usage doesn't follow these policies, we may ask you to make necessary changes. Repeated or serious violations may result in further action, including suspending or terminating your account. Our policies may change as we learn more about use and abuse of our models. Platform policy Our API is being used to power businesses across many sectors and technology platforms. From iOS Apps to websites to Slack, the simplicity of our API makes it possible to integrate into a wide array of use cases. Subject to the use case restrictions mentioned below, we allow the integration of our API into products on all major technology platforms, app stores, and beyond. Disallowed usage We don\u2019t allow the use of our models for the following: Illegal activity Child Sexual Abuse Material or any content that exploits or harms children Generation of hateful, harassing, or violent content Generation of malware Activity that has high risk of physical harm Activity that has high risk of economic harm Fraudulent or deceptive activity Adult content, adult industries, and dating apps Political campaigning or lobbying Activity that violates people\u2019s privacy Engaging in the unauthorized practice of law, or offering tailored legal advice without a qualified person reviewing the information Offering tailored financial advice without a qualified person reviewing the information Telling someone that they have or do not have a certain health condition, or providing instructions on how to cure or treat a health condition High risk government decision-making We have further requirements for certain uses of our models: Consumer-facing uses of our models in medical, financial, and legal industries; in news generation or news summarization; and where else warranted, must provide a disclaimer to users informing them that AI is being used and of its potential limitations. Automated systems (including conversational AI and chatbots) must disclose to users that they are interacting with an AI system. With the exception of chatbots that depict historical public figures, products that simulate another person must either have that person's explicit consent or be clearly labeled as \"simulated\" or \"parody.\" Use of model outputs in livestreams, demonstrations, and research are subject to our Sharing & Publication Policy. You can use our free moderation endpoint and safety best practices to help you keep your app safe. Changelog 2023-02-15: We\u2019ve combined our use case and content policies into a single set of usage policies, and have provided more specific guidance on what activity we disallow in industries we\u2019ve considered high risk. 2022-11-09: We no longer require you to register your applications with OpenAI. Instead, we'll be using a combination of automated and manual methods to monitor for policy violations. 2022-10-25: Updated App Review process (devs no longer need to wait for approval after submitting as long as they comply with our policies). Moved to an outcomes-based approach and updated Safety Best Practices. 2022-06-07: Refactored into categories of applications and corresponding requirements 2022-03-09: Refactored into \"App Review\" 2022-01-19: Simplified copywriting and article writing/editing guidelines 2021-11-15: Addition of \"Content guidelines\" section; changes to bullets on almost always approved uses and disallowed uses; renaming document from \"Use case guidelines\" to \"Usage guidelines\". 2021-08-04: Updated with information related to code generation 2021-03-12: Added detailed case-by-case requirements; small copy and ordering edits 2021-02-26: Clarified the impermissibility of Tweet / Instagram generators","title":"Usage policies"},{"location":"api-reference/","text":"api \u53c2\u8003 \u00b6 introduction authentication making-requests models completions chat edits images embeddings audio files fine-tunes moderations engines parameter-details","title":"api \u53c2\u8003"},{"location":"api-reference/#api","text":"introduction authentication making-requests models completions chat edits images embeddings audio files fine-tunes moderations engines parameter-details","title":"api \u53c2\u8003"},{"location":"api-reference/audio/","text":"\u97f3\u9891 \u00b6 \u5b66\u4e60\u5982\u4f55\u5c06\u97f3\u9891\u8f6c\u6362\u4e3a\u6587\u672c\u3002 \u76f8\u5173\u6307\u5357: Speech to text \u521b\u5efa\u8f6c\u5f55 Beta \u00b6 POST : https://api.openai.com/v1/audio/transcriptions Transcribes audio into the input language. \u8bf7\u6c42\u4f53 : file string Required The audio file to transcribe, in one of these formats: mp3, mp4, mpeg, mpga, m4a, wav, or webm. model string Required ID of the model to use. Only whisper-1 is currently available. prompt : string Optional An optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language. response_format string Optional Defaults to json The format of the transcript output, in one of these options: json, text, srt, verbose_json, or vtt. temperature number Optional Defaults to 0 The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit. language string Optional The language of the input audio. Supplying the input language in ISO-639-1 format will improve accuracy and latency. \u793a\u4f8b : 1 2 3 4 5 6 curl https://api.openai.com/v1/audio/transcriptions \\ -X POST \\ -H 'Authorization: Bearer TOKEN' \\ -H 'Content-Type: multipart/form-data' \\ -F file = @/path/to/file/audio.mp3 \\ -F model = whisper-1 \u53c2\u6570 : 1 2 3 4 { \"file\" : \"audio.mp3\" , \"model\" : \"whisper-1\" } \u8fd4\u56de : 1 2 3 { \"text\" : \"Imagine the wildest idea that you've ever had, and you're curious about how it might scale to something that's a 100, a 1,000 times bigger. This is a place where you can get to do that.\" } \u521b\u5efa\u7ffb\u8bd1 Beta \u00b6 POST : https://api.openai.com/v1/audio/translations \u5c06\u97f3\u9891\u7ffb\u8bd1\u6210\u82f1\u8bed\u3002 \u8bf7\u6c42\u4f53 : file string Required The audio file to translate, in one of these formats: mp3, mp4, mpeg, mpga, m4a, wav, or webm. model string Required ID of the model to use. Only whisper-1 is currently available. prompt : string Optional An optional text to guide the model's style or continue a previous audio segment. The prompt should be in English. response_format string Optional Defaults to json The format of the transcript output, in one of these options: json, text, srt, verbose_json, or vtt. temperature number Optional Defaults to 0 The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit. \u793a\u4f8b : 1 2 3 4 5 6 curl https://api.openai.com/v1/audio/translations \\ -X POST \\ -H 'Authorization: Bearer TOKEN' \\ -H 'Content-Type: multipart/form-data' \\ -F file = @/path/to/file/german.m4a \\ -F model = whisper-1 \u53c2\u6570 : 1 2 3 4 { \"file\" : \"german.m4a\" , \"model\" : \"whisper-1\" } \u8fd4\u56de : 1 2 3 { \"text\" : \"Hello, my name is Wolfgang and I come from Germany. Where are you heading today?\" }","title":"\u97f3\u9891"},{"location":"api-reference/audio/#_1","text":"\u5b66\u4e60\u5982\u4f55\u5c06\u97f3\u9891\u8f6c\u6362\u4e3a\u6587\u672c\u3002 \u76f8\u5173\u6307\u5357: Speech to text","title":"\u97f3\u9891"},{"location":"api-reference/audio/#beta","text":"POST : https://api.openai.com/v1/audio/transcriptions Transcribes audio into the input language. \u8bf7\u6c42\u4f53 : file string Required The audio file to transcribe, in one of these formats: mp3, mp4, mpeg, mpga, m4a, wav, or webm. model string Required ID of the model to use. Only whisper-1 is currently available. prompt : string Optional An optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language. response_format string Optional Defaults to json The format of the transcript output, in one of these options: json, text, srt, verbose_json, or vtt. temperature number Optional Defaults to 0 The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit. language string Optional The language of the input audio. Supplying the input language in ISO-639-1 format will improve accuracy and latency. \u793a\u4f8b : 1 2 3 4 5 6 curl https://api.openai.com/v1/audio/transcriptions \\ -X POST \\ -H 'Authorization: Bearer TOKEN' \\ -H 'Content-Type: multipart/form-data' \\ -F file = @/path/to/file/audio.mp3 \\ -F model = whisper-1 \u53c2\u6570 : 1 2 3 4 { \"file\" : \"audio.mp3\" , \"model\" : \"whisper-1\" } \u8fd4\u56de : 1 2 3 { \"text\" : \"Imagine the wildest idea that you've ever had, and you're curious about how it might scale to something that's a 100, a 1,000 times bigger. This is a place where you can get to do that.\" }","title":"\u521b\u5efa\u8f6c\u5f55 Beta"},{"location":"api-reference/audio/#beta_1","text":"POST : https://api.openai.com/v1/audio/translations \u5c06\u97f3\u9891\u7ffb\u8bd1\u6210\u82f1\u8bed\u3002 \u8bf7\u6c42\u4f53 : file string Required The audio file to translate, in one of these formats: mp3, mp4, mpeg, mpga, m4a, wav, or webm. model string Required ID of the model to use. Only whisper-1 is currently available. prompt : string Optional An optional text to guide the model's style or continue a previous audio segment. The prompt should be in English. response_format string Optional Defaults to json The format of the transcript output, in one of these options: json, text, srt, verbose_json, or vtt. temperature number Optional Defaults to 0 The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit. \u793a\u4f8b : 1 2 3 4 5 6 curl https://api.openai.com/v1/audio/translations \\ -X POST \\ -H 'Authorization: Bearer TOKEN' \\ -H 'Content-Type: multipart/form-data' \\ -F file = @/path/to/file/german.m4a \\ -F model = whisper-1 \u53c2\u6570 : 1 2 3 4 { \"file\" : \"german.m4a\" , \"model\" : \"whisper-1\" } \u8fd4\u56de : 1 2 3 { \"text\" : \"Hello, my name is Wolfgang and I come from Germany. Where are you heading today?\" }","title":"\u521b\u5efa\u7ffb\u8bd1 Beta"},{"location":"api-reference/authentication/","text":"\u8ba4\u8bc1 \u00b6 OpenAI API \u4f7f\u7528 API \u5bc6\u94a5\u8fdb\u884c\u8ba4\u8bc1\u3002\u8bbf\u95ee\u60a8\u7684 API \u5bc6\u94a5\u9875\u9762\u68c0\u7d22\u60a8\u5c06\u5728\u8bf7\u6c42\u4e2d\u4f7f\u7528\u7684 API \u5bc6\u94a5\u3002 \u8bb0\u4f4f\uff0c\u4f60\u7684 API \u5bc6\u94a5\u662f\u4e00\u4e2a\u79d8\u5bc6!\u4e0d\u8981\u4e0e\u4ed6\u4eba\u5171\u4eab\u6216\u5728\u4efb\u4f55\u5ba2\u6237\u7aef\u4ee3\u7801(\u6d4f\u89c8\u5668\uff0c\u5e94\u7528\u7a0b\u5e8f)\u4e2d\u516c\u5f00\u5b83\u3002 \u751f\u4ea7\u8bf7\u6c42\u5fc5\u987b\u901a\u8fc7\u60a8\u81ea\u5df1\u7684\u540e\u7aef\u670d\u52a1\u5668\u8def\u7531\uff0c\u60a8\u7684 API \u5bc6\u94a5\u53ef\u4ee5\u4ece\u73af\u5883\u53d8\u91cf\u6216\u5bc6\u94a5\u7ba1\u7406\u670d\u52a1\u5b89\u5168\u5730\u52a0\u8f7d\u3002 All API requests should include your API key in an Authorization HTTP header as follows: Authorization: Bearer YOUR_API_KEY Requesting organization For users who belong to multiple organizations, you can pass a header to specify which organization is used for an API request. Usage from these API requests will count against the specified organization's subscription quota. Example curl command: 1 2 3 curl https://api.openai.com/v1/models \\ -H 'Authorization: Bearer YOUR_API_KEY' \\ -H 'OpenAI-Organization: org-HW93VY7Oko7ptiKrGabG7oIM' Example with the openai Python package: 1 2 3 4 5 import os import openai openai . organization = \"org-HW93VY7Oko7ptiKrGabG7oIM\" openai . api_key = os . getenv ( \"OPENAI_API_KEY\" ) openai . Model . list () Example with the openai Node.js package: 1 2 3 4 5 6 7 8 import { Configuration , OpenAIApi } from \"openai\" ; const configuration = new Configuration ({ organization : \"org-HW93VY7Oko7ptiKrGabG7oIM\" , apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . listEngines (); Organization IDs can be found on your Organization settings page.","title":"\u8ba4\u8bc1"},{"location":"api-reference/authentication/#_1","text":"OpenAI API \u4f7f\u7528 API \u5bc6\u94a5\u8fdb\u884c\u8ba4\u8bc1\u3002\u8bbf\u95ee\u60a8\u7684 API \u5bc6\u94a5\u9875\u9762\u68c0\u7d22\u60a8\u5c06\u5728\u8bf7\u6c42\u4e2d\u4f7f\u7528\u7684 API \u5bc6\u94a5\u3002 \u8bb0\u4f4f\uff0c\u4f60\u7684 API \u5bc6\u94a5\u662f\u4e00\u4e2a\u79d8\u5bc6!\u4e0d\u8981\u4e0e\u4ed6\u4eba\u5171\u4eab\u6216\u5728\u4efb\u4f55\u5ba2\u6237\u7aef\u4ee3\u7801(\u6d4f\u89c8\u5668\uff0c\u5e94\u7528\u7a0b\u5e8f)\u4e2d\u516c\u5f00\u5b83\u3002 \u751f\u4ea7\u8bf7\u6c42\u5fc5\u987b\u901a\u8fc7\u60a8\u81ea\u5df1\u7684\u540e\u7aef\u670d\u52a1\u5668\u8def\u7531\uff0c\u60a8\u7684 API \u5bc6\u94a5\u53ef\u4ee5\u4ece\u73af\u5883\u53d8\u91cf\u6216\u5bc6\u94a5\u7ba1\u7406\u670d\u52a1\u5b89\u5168\u5730\u52a0\u8f7d\u3002 All API requests should include your API key in an Authorization HTTP header as follows: Authorization: Bearer YOUR_API_KEY Requesting organization For users who belong to multiple organizations, you can pass a header to specify which organization is used for an API request. Usage from these API requests will count against the specified organization's subscription quota. Example curl command: 1 2 3 curl https://api.openai.com/v1/models \\ -H 'Authorization: Bearer YOUR_API_KEY' \\ -H 'OpenAI-Organization: org-HW93VY7Oko7ptiKrGabG7oIM' Example with the openai Python package: 1 2 3 4 5 import os import openai openai . organization = \"org-HW93VY7Oko7ptiKrGabG7oIM\" openai . api_key = os . getenv ( \"OPENAI_API_KEY\" ) openai . Model . list () Example with the openai Node.js package: 1 2 3 4 5 6 7 8 import { Configuration , OpenAIApi } from \"openai\" ; const configuration = new Configuration ({ organization : \"org-HW93VY7Oko7ptiKrGabG7oIM\" , apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . listEngines (); Organization IDs can be found on your Organization settings page.","title":"\u8ba4\u8bc1"},{"location":"api-reference/chat/","text":"\u804a\u5929 \u00b6 \u7ed9\u5b9a\u4e00\u4e2a\u804a\u5929\u5bf9\u8bdd\uff0c\u8be5\u6a21\u578b\u5c06\u8fd4\u56de\u4e00\u4e2a\u804a\u5929\u5b8c\u6210\u54cd\u5e94\u3002 \u521b\u5efa\u804a\u5929\u8865\u5168 Beta \u00b6 POST : https://api.openai.com/v1/chat/completions Creates a completion for the chat message \u8bf7\u6c42\u4f53 : \u53c2\u6570 \u7c7b\u578b \u5fc5\u987b \u9ed8\u8ba4 \u63cf\u8ff0 model string Required ID of the model to use. Currently, only gpt-3.5-turbo and gpt-3.5-turbo-0301 are supported. messages array Required The messages to generate chat completions for, in the chat format. temperature number Optional 1 What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.We generally recommend altering this or top_p but not both. top_p number Optional 1 An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.We generally recommend altering this or temperature but not both. n integer Optional 1 How many chat completion choices to generate for each input message. stream boolean Optional false If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. stop string or array Optional null Up to 4 sequences where the API will stop generating further tokens. max_tokens integer Optional inf The maximum number of tokens allowed for the generated answer. By default, the number of tokens the model can return will be (4096 - prompt tokens). presence_penalty number Optional 0 \u4ecb\u4e8e-2.0 \u548c 2.0 \u4e4b\u95f4\u7684\u6570\u5b57\u3002\u6b63\u503c\u4f1a\u6839\u636e\u65b0\u6807\u8bb0\u5230\u76ee\u524d\u4e3a\u6b62\u662f\u5426\u51fa\u73b0\u5728\u6587\u672c\u4e2d\u6765\u60e9\u7f5a\u5b83\u4eec\uff0c\u4ece\u800c\u589e\u52a0\u6a21\u578b\u8c08\u8bba\u65b0\u4e3b\u9898\u7684\u53ef\u80fd\u6027\u3002\u8bf7\u53c2\u9605\u6709\u5173\u9891\u7387\u548c\u5b58\u5728\u60e9\u7f5a\u7684\u66f4\u591a\u4fe1\u606f\u3002 frequency_penalty number Optional 0 \u4ecb\u4e8e-2.0 \u548c 2.0 \u4e4b\u95f4\u7684\u6570\u5b57\u3002\u6b63\u503c\u4f1a\u6839\u636e\u65b0\u7b26\u53f7\u5728\u6587\u672c\u4e2d\u7684\u73b0\u6709\u9891\u7387\u6765\u60e9\u7f5a\u5b83\u4eec\uff0c\u4ece\u800c\u964d\u4f4e\u6a21\u578b\u9010\u5b57\u91cd\u590d\u540c\u4e00\u884c\u7684\u53ef\u80fd\u6027\u3002\u8bf7\u53c2\u9605\u6709\u5173\u9891\u7387\u548c\u5b58\u5728\u60e9\u7f5a\u7684\u66f4\u591a\u4fe1\u606f\u3002 logit_bias map Optional null \u4fee\u6539\u6307\u5b9a\u4ee4\u724c\u5728\u8865\u5168\u4e2d\u51fa\u73b0\u7684\u53ef\u80fd\u6027\u3002\u63a5\u53d7\u4e00\u4e2a json \u5bf9\u8c61\uff0c\u8be5\u5bf9\u8c61\u5c06\u6807\u8bb0(\u7531\u6807\u8bb0\u5668\u4e2d\u7684\u6807\u8bb0 ID \u6307\u5b9a)\u6620\u5c04\u5230\u4ece-100 \u5230 100 \u7684\u5173\u8054\u504f\u5dee\u503c\u3002\u5728\u6570\u5b66\u4e0a\uff0c\u504f\u5dee\u88ab\u6dfb\u52a0\u5230\u62bd\u6837\u524d\u7531\u6a21\u578b\u751f\u6210\u7684\u5bf9\u6570\u4e2d\u3002\u6bcf\u4e2a\u6a21\u578b\u7684\u786e\u5207\u6548\u679c\u4f1a\u6709\u6240\u4e0d\u540c\uff0c\u4f46\u4ecb\u4e8e-1 \u548c 1 \u4e4b\u95f4\u7684\u503c\u5e94\u8be5\u4f1a\u51cf\u5c11\u6216\u589e\u52a0\u9009\u62e9\u7684\u53ef\u80fd\u6027;\u50cf-100 \u6216 100 \u8fd9\u6837\u7684\u503c\u5e94\u8be5\u5bfc\u81f4\u76f8\u5173\u4ee4\u724c\u7684\u7981\u6b62\u6216\u6392\u4ed6\u9009\u62e9\u3002 user string Optional \u4ee3\u8868\u7ec8\u7aef\u7528\u6237\u7684\u552f\u4e00\u6807\u8bc6\u7b26\uff0c\u53ef\u4ee5\u5e2e\u52a9 OpenAI \u76d1\u89c6\u548c\u68c0\u6d4b\u6ee5\u7528\u3002\u5b66\u4e60\u66f4\u591a\u7684\u77e5\u8bc6\u3002 \u793a\u4f8b : 1 2 3 4 5 6 7 8 9 10 11 12 const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const completion = await openai . createChatCompletion ({ model : \"gpt-3.5-turbo\" , messages : [{ role : \"user\" , content : \"Hello world\" }], }); console . log ( completion . data . choices [ 0 ]. message ); \u53c2\u6570 : 1 2 3 4 { \"model\" : \"gpt-3.5-turbo\" , \"messages\" : [{ \"role\" : \"user\" , \"content\" : \"Hello!\" }] } \u8fd4\u56de : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 { \"id\" : \"chatcmpl-123\" , \"object\" : \"chat.completion\" , \"created\" : 1677652288 , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"\\n\\nHello there, how may I assist you today?\" }, \"finish_reason\" : \"stop\" } ], \"usage\" : { \"prompt_tokens\" : 9 , \"completion_tokens\" : 12 , \"total_tokens\" : 21 } }","title":"\u804a\u5929"},{"location":"api-reference/chat/#_1","text":"\u7ed9\u5b9a\u4e00\u4e2a\u804a\u5929\u5bf9\u8bdd\uff0c\u8be5\u6a21\u578b\u5c06\u8fd4\u56de\u4e00\u4e2a\u804a\u5929\u5b8c\u6210\u54cd\u5e94\u3002","title":"\u804a\u5929"},{"location":"api-reference/chat/#beta","text":"POST : https://api.openai.com/v1/chat/completions Creates a completion for the chat message \u8bf7\u6c42\u4f53 : \u53c2\u6570 \u7c7b\u578b \u5fc5\u987b \u9ed8\u8ba4 \u63cf\u8ff0 model string Required ID of the model to use. Currently, only gpt-3.5-turbo and gpt-3.5-turbo-0301 are supported. messages array Required The messages to generate chat completions for, in the chat format. temperature number Optional 1 What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.We generally recommend altering this or top_p but not both. top_p number Optional 1 An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.We generally recommend altering this or temperature but not both. n integer Optional 1 How many chat completion choices to generate for each input message. stream boolean Optional false If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. stop string or array Optional null Up to 4 sequences where the API will stop generating further tokens. max_tokens integer Optional inf The maximum number of tokens allowed for the generated answer. By default, the number of tokens the model can return will be (4096 - prompt tokens). presence_penalty number Optional 0 \u4ecb\u4e8e-2.0 \u548c 2.0 \u4e4b\u95f4\u7684\u6570\u5b57\u3002\u6b63\u503c\u4f1a\u6839\u636e\u65b0\u6807\u8bb0\u5230\u76ee\u524d\u4e3a\u6b62\u662f\u5426\u51fa\u73b0\u5728\u6587\u672c\u4e2d\u6765\u60e9\u7f5a\u5b83\u4eec\uff0c\u4ece\u800c\u589e\u52a0\u6a21\u578b\u8c08\u8bba\u65b0\u4e3b\u9898\u7684\u53ef\u80fd\u6027\u3002\u8bf7\u53c2\u9605\u6709\u5173\u9891\u7387\u548c\u5b58\u5728\u60e9\u7f5a\u7684\u66f4\u591a\u4fe1\u606f\u3002 frequency_penalty number Optional 0 \u4ecb\u4e8e-2.0 \u548c 2.0 \u4e4b\u95f4\u7684\u6570\u5b57\u3002\u6b63\u503c\u4f1a\u6839\u636e\u65b0\u7b26\u53f7\u5728\u6587\u672c\u4e2d\u7684\u73b0\u6709\u9891\u7387\u6765\u60e9\u7f5a\u5b83\u4eec\uff0c\u4ece\u800c\u964d\u4f4e\u6a21\u578b\u9010\u5b57\u91cd\u590d\u540c\u4e00\u884c\u7684\u53ef\u80fd\u6027\u3002\u8bf7\u53c2\u9605\u6709\u5173\u9891\u7387\u548c\u5b58\u5728\u60e9\u7f5a\u7684\u66f4\u591a\u4fe1\u606f\u3002 logit_bias map Optional null \u4fee\u6539\u6307\u5b9a\u4ee4\u724c\u5728\u8865\u5168\u4e2d\u51fa\u73b0\u7684\u53ef\u80fd\u6027\u3002\u63a5\u53d7\u4e00\u4e2a json \u5bf9\u8c61\uff0c\u8be5\u5bf9\u8c61\u5c06\u6807\u8bb0(\u7531\u6807\u8bb0\u5668\u4e2d\u7684\u6807\u8bb0 ID \u6307\u5b9a)\u6620\u5c04\u5230\u4ece-100 \u5230 100 \u7684\u5173\u8054\u504f\u5dee\u503c\u3002\u5728\u6570\u5b66\u4e0a\uff0c\u504f\u5dee\u88ab\u6dfb\u52a0\u5230\u62bd\u6837\u524d\u7531\u6a21\u578b\u751f\u6210\u7684\u5bf9\u6570\u4e2d\u3002\u6bcf\u4e2a\u6a21\u578b\u7684\u786e\u5207\u6548\u679c\u4f1a\u6709\u6240\u4e0d\u540c\uff0c\u4f46\u4ecb\u4e8e-1 \u548c 1 \u4e4b\u95f4\u7684\u503c\u5e94\u8be5\u4f1a\u51cf\u5c11\u6216\u589e\u52a0\u9009\u62e9\u7684\u53ef\u80fd\u6027;\u50cf-100 \u6216 100 \u8fd9\u6837\u7684\u503c\u5e94\u8be5\u5bfc\u81f4\u76f8\u5173\u4ee4\u724c\u7684\u7981\u6b62\u6216\u6392\u4ed6\u9009\u62e9\u3002 user string Optional \u4ee3\u8868\u7ec8\u7aef\u7528\u6237\u7684\u552f\u4e00\u6807\u8bc6\u7b26\uff0c\u53ef\u4ee5\u5e2e\u52a9 OpenAI \u76d1\u89c6\u548c\u68c0\u6d4b\u6ee5\u7528\u3002\u5b66\u4e60\u66f4\u591a\u7684\u77e5\u8bc6\u3002 \u793a\u4f8b : 1 2 3 4 5 6 7 8 9 10 11 12 const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const completion = await openai . createChatCompletion ({ model : \"gpt-3.5-turbo\" , messages : [{ role : \"user\" , content : \"Hello world\" }], }); console . log ( completion . data . choices [ 0 ]. message ); \u53c2\u6570 : 1 2 3 4 { \"model\" : \"gpt-3.5-turbo\" , \"messages\" : [{ \"role\" : \"user\" , \"content\" : \"Hello!\" }] } \u8fd4\u56de : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 { \"id\" : \"chatcmpl-123\" , \"object\" : \"chat.completion\" , \"created\" : 1677652288 , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"\\n\\nHello there, how may I assist you today?\" }, \"finish_reason\" : \"stop\" } ], \"usage\" : { \"prompt_tokens\" : 9 , \"completion_tokens\" : 12 , \"total_tokens\" : 21 } }","title":"\u521b\u5efa\u804a\u5929\u8865\u5168 Beta"},{"location":"api-reference/completions/","text":"\u8865\u5168 \u00b6 \u7ed9\u5b9a\u4e00\u4e2a\u63d0\u793a\uff0c\u6a21\u578b\u5c06\u8fd4\u56de\u4e00\u4e2a\u6216\u591a\u4e2a\u9884\u6d4b\u7684\u8865\u5168\uff0c\u5e76\u4e14\u8fd8\u53ef\u4ee5\u8fd4\u56de\u6bcf\u4e2a\u4f4d\u7f6e\u4e0a\u66ff\u4ee3\u6807\u8bb0\u7684\u6982\u7387\u3002 \u521b\u5efa\u8865\u5168 \u00b6 POST : https://api.openai.com/v1/completions \u4e3a\u63d0\u4f9b\u7684\u63d0\u793a\u7b26\u548c\u53c2\u6570\u521b\u5efa\u8865\u5168 \u8bf7\u6c42\u4f53 : \u53c2\u6570 \u7c7b\u578b \u5fc5\u987b \u9ed8\u8ba4 \u63cf\u8ff0 model string Required \u8981\u4f7f\u7528\u7684\u6a21\u578b\u7684 ID\u3002\u60a8\u53ef\u4ee5\u4f7f\u7528\u5217\u8868\u6a21\u578b API \u6765\u67e5\u770b\u6240\u6709\u53ef\u7528\u7684\u6a21\u578b\uff0c\u6216\u8005\u67e5\u770b\u6211\u4eec\u7684\u6a21\u578b\u6982\u8ff0\u6765\u4e86\u89e3\u5b83\u4eec\u7684\u63cf\u8ff0\u3002 prompt string or array Optional <\\|endoftext\\|> \u7528\u4e8e\u751f\u6210\u8865\u5168\u7684\u63d0\u793a\u7b26\uff0c\u7f16\u7801\u4e3a\u5b57\u7b26\u4e32\u3001\u5b57\u7b26\u4e32\u6570\u7ec4\u3001\u4ee4\u724c\u6570\u7ec4\u6216\u4ee4\u724c\u6570\u7ec4\u3002\u6ce8\u610f\uff0c<|endoftext|>\u662f\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u770b\u5230\u7684\u6587\u6863\u5206\u9694\u7b26\uff0c\u56e0\u6b64\u5982\u679c\u6ca1\u6709\u6307\u5b9a\u63d0\u793a\u7b26\uff0c\u6a21\u578b\u5c06\u50cf\u4ece\u65b0\u6587\u6863\u7684\u5f00\u5934\u751f\u6210\u4e00\u6837\u3002 suffix string Optional null \u5728\u63d2\u5165\u6587\u672c\u5b8c\u6210\u540e\u51fa\u73b0\u7684\u540e\u7f00\u3002 max_tokens integer Optional 16 \u5728\u8865\u5168\u8fc7\u7a0b\u4e2d\u751f\u6210\u7684\u6700\u5927\u4ee4\u724c\u6570\u3002\u63d0\u793a\u7b26\u7684\u6807\u8bb0\u8ba1\u6570\u52a0\u4e0a max_tokens \u4e0d\u80fd\u8d85\u8fc7\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\u3002\u5927\u591a\u6570\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e3a 2048 \u4e2a\u4ee4\u724c(\u9664\u4e86\u6700\u65b0\u7684\u6a21\u578b\uff0c\u5b83\u652f\u6301 4096 \u4e2a)\u3002 temperature number Optional 1 \u4f7f\u7528\u4ec0\u4e48\u53d6\u6837\u6e29\u5ea6\uff0c0 \u5230 2 \u4e4b\u95f4\u3002\u8f83\u9ad8\u7684\u503c(\u5982 0.8)\u5c06\u4f7f\u8f93\u51fa\u66f4\u52a0\u968f\u673a\uff0c\u800c\u8f83\u4f4e\u7684\u503c(\u5982 0.2)\u5c06\u4f7f\u8f93\u51fa\u66f4\u52a0\u96c6\u4e2d\u548c\u786e\u5b9a\u3002\u6211\u4eec\u901a\u5e38\u5efa\u8bae\u4fee\u6539\u8fd9\u4e2a\u6216 top_p \uff0c\u4f46\u4e0d\u5efa\u8bae\u540c\u65f6\u4fee\u6539\u3002 top_p number Optional 1 \u6e29\u5ea6\u91c7\u6837\u7684\u53e6\u4e00\u79cd\u66ff\u4ee3\u65b9\u6cd5\u79f0\u4e3a\u6838\u91c7\u6837\uff0c\u5176\u4e2d\u6a21\u578b\u8003\u8651\u5177\u6709 top_p \u6982\u7387\u8d28\u91cf\u7684\u6807\u8bb0\u7684\u7ed3\u679c\u3002\u6240\u4ee5 0.1 \u610f\u5473\u7740\u53ea\u8003\u8651\u5305\u542b\u524d 10%\u6982\u7387\u8d28\u91cf\u7684\u4ee4\u724c\u3002\u6211\u4eec\u901a\u5e38\u5efa\u8bae\u6539\u53d8\u8fd9\u4e2a\u6216\u6e29\u5ea6\uff0c\u4f46\u4e0d\u5efa\u8bae\u4e24\u8005\u90fd\u6539\u53d8\u3002 n integer Optional 1 \u4e3a\u6bcf\u4e2a\u63d0\u793a\u7b26\u751f\u6210\u591a\u5c11\u4e2a\u8865\u5168\u3002\u6ce8\u610f:\u56e0\u4e3a\u8fd9\u4e2a\u53c2\u6570\u4f1a\u751f\u6210\u5f88\u591a\u8865\u5168\uff0c\u6240\u4ee5\u5b83\u4f1a\u5f88\u5feb\u6d88\u8017\u6389\u4f60\u7684\u4ee4\u724c\u914d\u989d\u3002\u8bf7\u8c28\u614e\u4f7f\u7528\uff0c\u5e76\u786e\u4fdd\u60a8\u5bf9 max_tokens \u6709\u5408\u7406\u7684\u8bbe\u7f6e\uff0c\u7136\u540e\u505c\u6b62\u3002 stream boolean Optional false \u662f\u5426\u56de\u6d41\u90e8\u5206\u8fdb\u5ea6\u3002\u5982\u679c\u8bbe\u7f6e\u4e86\uff0c\u4ee4\u724c\u5c06\u5728\u5b83\u4eec\u53ef\u7528\u65f6\u4f5c\u4e3a\u4ec5\u6570\u636e\u7684 \u670d\u52a1\u5668\u53d1\u9001\u4e8b\u4ef6 \u53d1\u9001\uff0c\u6d41\u7531 data:[DONE] \u6d88\u606f\u7ec8\u6b62\u3002 logprobs integer Optional null \u5305\u62ec logprobs \u6700\u53ef\u80fd\u4ee4\u724c\u7684\u5bf9\u6570\u6982\u7387\uff0c\u4ee5\u53ca\u6240\u9009\u4ee4\u724c\u3002\u4f8b\u5982\uff0c\u5982\u679c logprobs \u4e3a 5,API \u5c06\u8fd4\u56de 5 \u4e2a\u6700\u53ef\u80fd\u7684\u4ee4\u724c\u7684\u5217\u8868\u3002API \u5c06\u59cb\u7ec8\u8fd4\u56de\u91c7\u6837\u4ee4\u724c\u7684 logprobb\uff0c\u56e0\u6b64\u54cd\u5e94\u4e2d\u6700\u591a\u53ef\u80fd\u6709 logprobs+1 \u4e2a\u5143\u7d20\u3002logprobs \u7684\u6700\u5927\u503c\u4e3a 5\u3002\u5982\u679c\u60a8\u9700\u8981\u66f4\u591a\uff0c\u8bf7\u901a\u8fc7\u6211\u4eec\u7684\u5e2e\u52a9\u4e2d\u5fc3\u8054\u7cfb\u6211\u4eec\uff0c\u5e76\u63cf\u8ff0\u60a8\u7684\u7528\u4f8b\u3002 echo boolean Optional false \u9664\u4e86\u5b8c\u6210\u4e4b\u5916\uff0c\u8fd8\u56de\u663e\u63d0\u793a\u7b26 stop string or array Optional null \u6700\u591a 4 \u4e2a\u5e8f\u5217\uff0cAPI \u5c06\u505c\u6b62\u751f\u6210\u8fdb\u4e00\u6b65\u7684\u4ee4\u724c\u3002\u8fd4\u56de\u7684\u6587\u672c\u5c06\u4e0d\u5305\u542b\u505c\u6b62\u5e8f\u5217\u3002 presence_penalty number Optional 0 \u4ecb\u4e8e-2.0 \u548c 2.0 \u4e4b\u95f4\u7684\u6570\u5b57\u3002\u6b63\u503c\u4f1a\u6839\u636e\u65b0\u6807\u8bb0\u5230\u76ee\u524d\u4e3a\u6b62\u662f\u5426\u51fa\u73b0\u5728\u6587\u672c\u4e2d\u6765\u60e9\u7f5a\u5b83\u4eec\uff0c\u4ece\u800c\u589e\u52a0\u6a21\u578b\u8c08\u8bba\u65b0\u4e3b\u9898\u7684\u53ef\u80fd\u6027\u3002\u8bf7\u53c2\u9605\u6709\u5173\u9891\u7387\u548c\u5b58\u5728\u60e9\u7f5a\u7684\u66f4\u591a\u4fe1\u606f\u3002 frequency_penalty number Optional 0 \u4ecb\u4e8e-2.0 \u548c 2.0 \u4e4b\u95f4\u7684\u6570\u5b57\u3002\u6b63\u503c\u4f1a\u6839\u636e\u65b0\u7b26\u53f7\u5728\u6587\u672c\u4e2d\u7684\u73b0\u6709\u9891\u7387\u6765\u60e9\u7f5a\u5b83\u4eec\uff0c\u4ece\u800c\u964d\u4f4e\u6a21\u578b\u9010\u5b57\u91cd\u590d\u540c\u4e00\u884c\u7684\u53ef\u80fd\u6027\u3002\u8bf7\u53c2\u9605\u6709\u5173\u9891\u7387\u548c\u5b58\u5728\u60e9\u7f5a\u7684\u66f4\u591a\u4fe1\u606f\u3002 best_of integer Optional 1 \u5728\u670d\u52a1\u5668\u7aef\u751f\u6210 best_of \u8865\u5168\uff0c\u5e76\u8fd4\u56de\u201cbest\u201d(\u6bcf\u4e2a\u4ee4\u724c\u5177\u6709\u6700\u9ad8\u65e5\u5fd7\u6982\u7387\u7684\u90a3\u4e2a)\u3002\u7ed3\u679c\u4e0d\u80fd\u6d41\u3002\u5f53\u4e0e n \u4e00\u8d77\u4f7f\u7528\u65f6\uff0cbest_of \u63a7\u5236\u5019\u9009\u8865\u5168\u7684\u6570\u91cf\uff0cn \u6307\u5b9a\u8fd4\u56de\u591a\u5c11- best_of \u5fc5\u987b\u5927\u4e8e n\u3002\u6ce8\u610f:\u56e0\u4e3a\u8fd9\u4e2a\u53c2\u6570\u4f1a\u751f\u6210\u5f88\u591a\u8865\u5168\uff0c\u5b83\u4f1a\u5f88\u5feb\u6d88\u8017\u6389\u4f60\u7684\u4ee4\u724c\u914d\u989d\u3002\u8bf7\u8c28\u614e\u4f7f\u7528\uff0c\u5e76\u786e\u4fdd\u60a8\u5bf9 max_tokens \u6709\u5408\u7406\u7684\u8bbe\u7f6e\uff0c\u7136\u540e\u505c\u6b62\u3002 logit_bias map Optional null \u4fee\u6539\u6307\u5b9a\u4ee4\u724c\u5728\u8865\u5168\u4e2d\u51fa\u73b0\u7684\u53ef\u80fd\u6027\u3002\u63a5\u53d7\u4e00\u4e2a json \u5bf9\u8c61\uff0c\u8be5\u5bf9\u8c61\u5c06\u6807\u8bb0(\u7531 GPT \u6807\u8bb0\u5668\u4e2d\u7684\u6807\u8bb0 ID \u6307\u5b9a)\u6620\u5c04\u5230\u4ece-100 \u5230 100 \u7684\u5173\u8054\u504f\u5dee\u503c\u3002\u60a8\u53ef\u4ee5\u4f7f\u7528\u8fd9\u4e2a\u6807\u8bb0\u5668\u5de5\u5177(\u9002\u7528\u4e8e GPT-2 \u548c GPT-3)\u5c06\u6587\u672c\u8f6c\u6362\u4e3a\u6807\u8bb0 id\u3002\u5728\u6570\u5b66\u4e0a\uff0c\u504f\u5dee\u88ab\u6dfb\u52a0\u5230\u62bd\u6837\u524d\u7531\u6a21\u578b\u751f\u6210\u7684\u5bf9\u6570\u4e2d\u3002\u6bcf\u4e2a\u6a21\u578b\u7684\u786e\u5207\u6548\u679c\u4f1a\u6709\u6240\u4e0d\u540c\uff0c\u4f46\u4ecb\u4e8e-1 \u548c 1 \u4e4b\u95f4\u7684\u503c\u5e94\u8be5\u4f1a\u51cf\u5c11\u6216\u589e\u52a0\u9009\u62e9\u7684\u53ef\u80fd\u6027;\u50cf-100 \u6216 100 \u8fd9\u6837\u7684\u503c\u5e94\u8be5\u5bfc\u81f4\u76f8\u5173\u4ee4\u724c\u7684\u7981\u6b62\u6216\u6392\u4ed6\u9009\u62e9\u3002\u4f8b\u5982\uff0c\u60a8\u53ef\u4ee5\u4f20\u9012{\"50256\":-100}\u6765\u9632\u6b62\u751f\u6210<|endoftext|>\u4ee4\u724c\u3002 user string Optional \u4ee3\u8868\u7ec8\u7aef\u7528\u6237\u7684\u552f\u4e00\u6807\u8bc6\u7b26\uff0c\u53ef\u4ee5\u5e2e\u52a9 OpenAI \u76d1\u89c6\u548c\u68c0\u6d4b\u6ee5\u7528\u3002\u5b66\u4e60\u66f4\u591a\u7684\u77e5\u8bc6\u3002 \u793a\u4f8b:text-davinci-003 1 2 3 4 5 6 7 8 9 10 11 const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . createCompletion ({ model : \"text-davinci-003\" , prompt : \"Say this is a test\" , max_tokens : 7 , temperature : 0 , }); \u53c2\u6570:text-davinci-003 1 2 3 4 5 6 7 8 9 10 11 { \"model\" : \"text-davinci-003\" , \"prompt\" : \"Say this is a test\" , \"max_tokens\" : 7 , \"temperature\" : 0 , \"top_p\" : 1 , \"n\" : 1 , \"stream\" : false , \"logprobs\" : null , \"stop\" : \"\\n\" } \u8fd4\u56de:text-davinci-003 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"id\" : \"cmpl-uqkvlQyYK7bGYrRHQ0eXlWi7\" , \"object\" : \"text_completion\" , \"created\" : 1589478378 , \"model\" : \"text-davinci-003\" , \"choices\" : [ { \"text\" : \"\\n\\nThis is indeed a test\" , \"index\" : 0 , \"logprobs\" : null , \"finish_reason\" : \"length\" } ], \"usage\" : { \"prompt_tokens\" : 5 , \"completion_tokens\" : 7 , \"total_tokens\" : 12 } }","title":"\u8865\u5168"},{"location":"api-reference/completions/#_1","text":"\u7ed9\u5b9a\u4e00\u4e2a\u63d0\u793a\uff0c\u6a21\u578b\u5c06\u8fd4\u56de\u4e00\u4e2a\u6216\u591a\u4e2a\u9884\u6d4b\u7684\u8865\u5168\uff0c\u5e76\u4e14\u8fd8\u53ef\u4ee5\u8fd4\u56de\u6bcf\u4e2a\u4f4d\u7f6e\u4e0a\u66ff\u4ee3\u6807\u8bb0\u7684\u6982\u7387\u3002","title":"\u8865\u5168"},{"location":"api-reference/completions/#_2","text":"POST : https://api.openai.com/v1/completions \u4e3a\u63d0\u4f9b\u7684\u63d0\u793a\u7b26\u548c\u53c2\u6570\u521b\u5efa\u8865\u5168 \u8bf7\u6c42\u4f53 : \u53c2\u6570 \u7c7b\u578b \u5fc5\u987b \u9ed8\u8ba4 \u63cf\u8ff0 model string Required \u8981\u4f7f\u7528\u7684\u6a21\u578b\u7684 ID\u3002\u60a8\u53ef\u4ee5\u4f7f\u7528\u5217\u8868\u6a21\u578b API \u6765\u67e5\u770b\u6240\u6709\u53ef\u7528\u7684\u6a21\u578b\uff0c\u6216\u8005\u67e5\u770b\u6211\u4eec\u7684\u6a21\u578b\u6982\u8ff0\u6765\u4e86\u89e3\u5b83\u4eec\u7684\u63cf\u8ff0\u3002 prompt string or array Optional <\\|endoftext\\|> \u7528\u4e8e\u751f\u6210\u8865\u5168\u7684\u63d0\u793a\u7b26\uff0c\u7f16\u7801\u4e3a\u5b57\u7b26\u4e32\u3001\u5b57\u7b26\u4e32\u6570\u7ec4\u3001\u4ee4\u724c\u6570\u7ec4\u6216\u4ee4\u724c\u6570\u7ec4\u3002\u6ce8\u610f\uff0c<|endoftext|>\u662f\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u770b\u5230\u7684\u6587\u6863\u5206\u9694\u7b26\uff0c\u56e0\u6b64\u5982\u679c\u6ca1\u6709\u6307\u5b9a\u63d0\u793a\u7b26\uff0c\u6a21\u578b\u5c06\u50cf\u4ece\u65b0\u6587\u6863\u7684\u5f00\u5934\u751f\u6210\u4e00\u6837\u3002 suffix string Optional null \u5728\u63d2\u5165\u6587\u672c\u5b8c\u6210\u540e\u51fa\u73b0\u7684\u540e\u7f00\u3002 max_tokens integer Optional 16 \u5728\u8865\u5168\u8fc7\u7a0b\u4e2d\u751f\u6210\u7684\u6700\u5927\u4ee4\u724c\u6570\u3002\u63d0\u793a\u7b26\u7684\u6807\u8bb0\u8ba1\u6570\u52a0\u4e0a max_tokens \u4e0d\u80fd\u8d85\u8fc7\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\u3002\u5927\u591a\u6570\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e3a 2048 \u4e2a\u4ee4\u724c(\u9664\u4e86\u6700\u65b0\u7684\u6a21\u578b\uff0c\u5b83\u652f\u6301 4096 \u4e2a)\u3002 temperature number Optional 1 \u4f7f\u7528\u4ec0\u4e48\u53d6\u6837\u6e29\u5ea6\uff0c0 \u5230 2 \u4e4b\u95f4\u3002\u8f83\u9ad8\u7684\u503c(\u5982 0.8)\u5c06\u4f7f\u8f93\u51fa\u66f4\u52a0\u968f\u673a\uff0c\u800c\u8f83\u4f4e\u7684\u503c(\u5982 0.2)\u5c06\u4f7f\u8f93\u51fa\u66f4\u52a0\u96c6\u4e2d\u548c\u786e\u5b9a\u3002\u6211\u4eec\u901a\u5e38\u5efa\u8bae\u4fee\u6539\u8fd9\u4e2a\u6216 top_p \uff0c\u4f46\u4e0d\u5efa\u8bae\u540c\u65f6\u4fee\u6539\u3002 top_p number Optional 1 \u6e29\u5ea6\u91c7\u6837\u7684\u53e6\u4e00\u79cd\u66ff\u4ee3\u65b9\u6cd5\u79f0\u4e3a\u6838\u91c7\u6837\uff0c\u5176\u4e2d\u6a21\u578b\u8003\u8651\u5177\u6709 top_p \u6982\u7387\u8d28\u91cf\u7684\u6807\u8bb0\u7684\u7ed3\u679c\u3002\u6240\u4ee5 0.1 \u610f\u5473\u7740\u53ea\u8003\u8651\u5305\u542b\u524d 10%\u6982\u7387\u8d28\u91cf\u7684\u4ee4\u724c\u3002\u6211\u4eec\u901a\u5e38\u5efa\u8bae\u6539\u53d8\u8fd9\u4e2a\u6216\u6e29\u5ea6\uff0c\u4f46\u4e0d\u5efa\u8bae\u4e24\u8005\u90fd\u6539\u53d8\u3002 n integer Optional 1 \u4e3a\u6bcf\u4e2a\u63d0\u793a\u7b26\u751f\u6210\u591a\u5c11\u4e2a\u8865\u5168\u3002\u6ce8\u610f:\u56e0\u4e3a\u8fd9\u4e2a\u53c2\u6570\u4f1a\u751f\u6210\u5f88\u591a\u8865\u5168\uff0c\u6240\u4ee5\u5b83\u4f1a\u5f88\u5feb\u6d88\u8017\u6389\u4f60\u7684\u4ee4\u724c\u914d\u989d\u3002\u8bf7\u8c28\u614e\u4f7f\u7528\uff0c\u5e76\u786e\u4fdd\u60a8\u5bf9 max_tokens \u6709\u5408\u7406\u7684\u8bbe\u7f6e\uff0c\u7136\u540e\u505c\u6b62\u3002 stream boolean Optional false \u662f\u5426\u56de\u6d41\u90e8\u5206\u8fdb\u5ea6\u3002\u5982\u679c\u8bbe\u7f6e\u4e86\uff0c\u4ee4\u724c\u5c06\u5728\u5b83\u4eec\u53ef\u7528\u65f6\u4f5c\u4e3a\u4ec5\u6570\u636e\u7684 \u670d\u52a1\u5668\u53d1\u9001\u4e8b\u4ef6 \u53d1\u9001\uff0c\u6d41\u7531 data:[DONE] \u6d88\u606f\u7ec8\u6b62\u3002 logprobs integer Optional null \u5305\u62ec logprobs \u6700\u53ef\u80fd\u4ee4\u724c\u7684\u5bf9\u6570\u6982\u7387\uff0c\u4ee5\u53ca\u6240\u9009\u4ee4\u724c\u3002\u4f8b\u5982\uff0c\u5982\u679c logprobs \u4e3a 5,API \u5c06\u8fd4\u56de 5 \u4e2a\u6700\u53ef\u80fd\u7684\u4ee4\u724c\u7684\u5217\u8868\u3002API \u5c06\u59cb\u7ec8\u8fd4\u56de\u91c7\u6837\u4ee4\u724c\u7684 logprobb\uff0c\u56e0\u6b64\u54cd\u5e94\u4e2d\u6700\u591a\u53ef\u80fd\u6709 logprobs+1 \u4e2a\u5143\u7d20\u3002logprobs \u7684\u6700\u5927\u503c\u4e3a 5\u3002\u5982\u679c\u60a8\u9700\u8981\u66f4\u591a\uff0c\u8bf7\u901a\u8fc7\u6211\u4eec\u7684\u5e2e\u52a9\u4e2d\u5fc3\u8054\u7cfb\u6211\u4eec\uff0c\u5e76\u63cf\u8ff0\u60a8\u7684\u7528\u4f8b\u3002 echo boolean Optional false \u9664\u4e86\u5b8c\u6210\u4e4b\u5916\uff0c\u8fd8\u56de\u663e\u63d0\u793a\u7b26 stop string or array Optional null \u6700\u591a 4 \u4e2a\u5e8f\u5217\uff0cAPI \u5c06\u505c\u6b62\u751f\u6210\u8fdb\u4e00\u6b65\u7684\u4ee4\u724c\u3002\u8fd4\u56de\u7684\u6587\u672c\u5c06\u4e0d\u5305\u542b\u505c\u6b62\u5e8f\u5217\u3002 presence_penalty number Optional 0 \u4ecb\u4e8e-2.0 \u548c 2.0 \u4e4b\u95f4\u7684\u6570\u5b57\u3002\u6b63\u503c\u4f1a\u6839\u636e\u65b0\u6807\u8bb0\u5230\u76ee\u524d\u4e3a\u6b62\u662f\u5426\u51fa\u73b0\u5728\u6587\u672c\u4e2d\u6765\u60e9\u7f5a\u5b83\u4eec\uff0c\u4ece\u800c\u589e\u52a0\u6a21\u578b\u8c08\u8bba\u65b0\u4e3b\u9898\u7684\u53ef\u80fd\u6027\u3002\u8bf7\u53c2\u9605\u6709\u5173\u9891\u7387\u548c\u5b58\u5728\u60e9\u7f5a\u7684\u66f4\u591a\u4fe1\u606f\u3002 frequency_penalty number Optional 0 \u4ecb\u4e8e-2.0 \u548c 2.0 \u4e4b\u95f4\u7684\u6570\u5b57\u3002\u6b63\u503c\u4f1a\u6839\u636e\u65b0\u7b26\u53f7\u5728\u6587\u672c\u4e2d\u7684\u73b0\u6709\u9891\u7387\u6765\u60e9\u7f5a\u5b83\u4eec\uff0c\u4ece\u800c\u964d\u4f4e\u6a21\u578b\u9010\u5b57\u91cd\u590d\u540c\u4e00\u884c\u7684\u53ef\u80fd\u6027\u3002\u8bf7\u53c2\u9605\u6709\u5173\u9891\u7387\u548c\u5b58\u5728\u60e9\u7f5a\u7684\u66f4\u591a\u4fe1\u606f\u3002 best_of integer Optional 1 \u5728\u670d\u52a1\u5668\u7aef\u751f\u6210 best_of \u8865\u5168\uff0c\u5e76\u8fd4\u56de\u201cbest\u201d(\u6bcf\u4e2a\u4ee4\u724c\u5177\u6709\u6700\u9ad8\u65e5\u5fd7\u6982\u7387\u7684\u90a3\u4e2a)\u3002\u7ed3\u679c\u4e0d\u80fd\u6d41\u3002\u5f53\u4e0e n \u4e00\u8d77\u4f7f\u7528\u65f6\uff0cbest_of \u63a7\u5236\u5019\u9009\u8865\u5168\u7684\u6570\u91cf\uff0cn \u6307\u5b9a\u8fd4\u56de\u591a\u5c11- best_of \u5fc5\u987b\u5927\u4e8e n\u3002\u6ce8\u610f:\u56e0\u4e3a\u8fd9\u4e2a\u53c2\u6570\u4f1a\u751f\u6210\u5f88\u591a\u8865\u5168\uff0c\u5b83\u4f1a\u5f88\u5feb\u6d88\u8017\u6389\u4f60\u7684\u4ee4\u724c\u914d\u989d\u3002\u8bf7\u8c28\u614e\u4f7f\u7528\uff0c\u5e76\u786e\u4fdd\u60a8\u5bf9 max_tokens \u6709\u5408\u7406\u7684\u8bbe\u7f6e\uff0c\u7136\u540e\u505c\u6b62\u3002 logit_bias map Optional null \u4fee\u6539\u6307\u5b9a\u4ee4\u724c\u5728\u8865\u5168\u4e2d\u51fa\u73b0\u7684\u53ef\u80fd\u6027\u3002\u63a5\u53d7\u4e00\u4e2a json \u5bf9\u8c61\uff0c\u8be5\u5bf9\u8c61\u5c06\u6807\u8bb0(\u7531 GPT \u6807\u8bb0\u5668\u4e2d\u7684\u6807\u8bb0 ID \u6307\u5b9a)\u6620\u5c04\u5230\u4ece-100 \u5230 100 \u7684\u5173\u8054\u504f\u5dee\u503c\u3002\u60a8\u53ef\u4ee5\u4f7f\u7528\u8fd9\u4e2a\u6807\u8bb0\u5668\u5de5\u5177(\u9002\u7528\u4e8e GPT-2 \u548c GPT-3)\u5c06\u6587\u672c\u8f6c\u6362\u4e3a\u6807\u8bb0 id\u3002\u5728\u6570\u5b66\u4e0a\uff0c\u504f\u5dee\u88ab\u6dfb\u52a0\u5230\u62bd\u6837\u524d\u7531\u6a21\u578b\u751f\u6210\u7684\u5bf9\u6570\u4e2d\u3002\u6bcf\u4e2a\u6a21\u578b\u7684\u786e\u5207\u6548\u679c\u4f1a\u6709\u6240\u4e0d\u540c\uff0c\u4f46\u4ecb\u4e8e-1 \u548c 1 \u4e4b\u95f4\u7684\u503c\u5e94\u8be5\u4f1a\u51cf\u5c11\u6216\u589e\u52a0\u9009\u62e9\u7684\u53ef\u80fd\u6027;\u50cf-100 \u6216 100 \u8fd9\u6837\u7684\u503c\u5e94\u8be5\u5bfc\u81f4\u76f8\u5173\u4ee4\u724c\u7684\u7981\u6b62\u6216\u6392\u4ed6\u9009\u62e9\u3002\u4f8b\u5982\uff0c\u60a8\u53ef\u4ee5\u4f20\u9012{\"50256\":-100}\u6765\u9632\u6b62\u751f\u6210<|endoftext|>\u4ee4\u724c\u3002 user string Optional \u4ee3\u8868\u7ec8\u7aef\u7528\u6237\u7684\u552f\u4e00\u6807\u8bc6\u7b26\uff0c\u53ef\u4ee5\u5e2e\u52a9 OpenAI \u76d1\u89c6\u548c\u68c0\u6d4b\u6ee5\u7528\u3002\u5b66\u4e60\u66f4\u591a\u7684\u77e5\u8bc6\u3002 \u793a\u4f8b:text-davinci-003 1 2 3 4 5 6 7 8 9 10 11 const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . createCompletion ({ model : \"text-davinci-003\" , prompt : \"Say this is a test\" , max_tokens : 7 , temperature : 0 , }); \u53c2\u6570:text-davinci-003 1 2 3 4 5 6 7 8 9 10 11 { \"model\" : \"text-davinci-003\" , \"prompt\" : \"Say this is a test\" , \"max_tokens\" : 7 , \"temperature\" : 0 , \"top_p\" : 1 , \"n\" : 1 , \"stream\" : false , \"logprobs\" : null , \"stop\" : \"\\n\" } \u8fd4\u56de:text-davinci-003 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"id\" : \"cmpl-uqkvlQyYK7bGYrRHQ0eXlWi7\" , \"object\" : \"text_completion\" , \"created\" : 1589478378 , \"model\" : \"text-davinci-003\" , \"choices\" : [ { \"text\" : \"\\n\\nThis is indeed a test\" , \"index\" : 0 , \"logprobs\" : null , \"finish_reason\" : \"length\" } ], \"usage\" : { \"prompt_tokens\" : 5 , \"completion_tokens\" : 7 , \"total_tokens\" : 12 } }","title":"\u521b\u5efa\u8865\u5168"},{"location":"api-reference/edits/","text":"Edits \u00b6 \u7ed9\u5b9a\u63d0\u793a\u7b26\u548c\u6307\u4ee4\uff0c\u6a21\u578b\u5c06\u8fd4\u56de\u63d0\u793a\u7b26\u7684\u7f16\u8f91\u7248\u672c\u3002 \u521b\u5efa edit \u00b6 POST : https://api.openai.com/v1/edits \u4e3a\u6240\u63d0\u4f9b\u7684\u8f93\u5165\u3001\u6307\u4ee4\u548c\u53c2\u6570\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u7f16\u8f91\u3002 \u8bf7\u6c42\u4f53 : model string Required ID of the model to use. You can use the text-davinci-edit-001 or code-davinci-edit-001 model with this endpoint. input string Optional Defaults to '' The input text to use as a starting point for the edit. instruction string Required The instruction that tells the model how to edit the prompt. n integer Optional Defaults to 1 How many edits to generate for the input and instruction. temperature number Optional Defaults to 1 What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both. top_p number Optional Defaults to 1 An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or temperature but not both. \u793a\u4f8b : text-davinci-edit-001 text-davinci-edit-001 1 2 3 4 5 6 7 8 9 10 const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . createEdit ({ model : \"text-davinci-edit-001\" , input : \"What day of the wek is it?\" , instruction : \"Fix the spelling mistakes\" , }); \u53c2\u6570 : text-davinci-edit-001 text-davinci-edit-001 1 2 3 4 5 { \"model\" : \"text-davinci-edit-001\" , \"input\" : \"What day of the wek is it?\" , \"instruction\" : \"Fix the spelling mistakes\" } \u8fd4\u56de : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \"object\" : \"edit\" , \"created\" : 1589478378 , \"choices\" : [ { \"text\" : \"What day of the week is it?\" , \"index\" : 0 } ], \"usage\" : { \"prompt_tokens\" : 25 , \"completion_tokens\" : 32 , \"total_tokens\" : 57 } }","title":"Edits"},{"location":"api-reference/edits/#edits","text":"\u7ed9\u5b9a\u63d0\u793a\u7b26\u548c\u6307\u4ee4\uff0c\u6a21\u578b\u5c06\u8fd4\u56de\u63d0\u793a\u7b26\u7684\u7f16\u8f91\u7248\u672c\u3002","title":"Edits"},{"location":"api-reference/edits/#edit","text":"POST : https://api.openai.com/v1/edits \u4e3a\u6240\u63d0\u4f9b\u7684\u8f93\u5165\u3001\u6307\u4ee4\u548c\u53c2\u6570\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u7f16\u8f91\u3002 \u8bf7\u6c42\u4f53 : model string Required ID of the model to use. You can use the text-davinci-edit-001 or code-davinci-edit-001 model with this endpoint. input string Optional Defaults to '' The input text to use as a starting point for the edit. instruction string Required The instruction that tells the model how to edit the prompt. n integer Optional Defaults to 1 How many edits to generate for the input and instruction. temperature number Optional Defaults to 1 What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both. top_p number Optional Defaults to 1 An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or temperature but not both. \u793a\u4f8b : text-davinci-edit-001 text-davinci-edit-001 1 2 3 4 5 6 7 8 9 10 const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . createEdit ({ model : \"text-davinci-edit-001\" , input : \"What day of the wek is it?\" , instruction : \"Fix the spelling mistakes\" , }); \u53c2\u6570 : text-davinci-edit-001 text-davinci-edit-001 1 2 3 4 5 { \"model\" : \"text-davinci-edit-001\" , \"input\" : \"What day of the wek is it?\" , \"instruction\" : \"Fix the spelling mistakes\" } \u8fd4\u56de : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \"object\" : \"edit\" , \"created\" : 1589478378 , \"choices\" : [ { \"text\" : \"What day of the week is it?\" , \"index\" : 0 } ], \"usage\" : { \"prompt_tokens\" : 25 , \"completion_tokens\" : 32 , \"total_tokens\" : 57 } }","title":"\u521b\u5efa edit"},{"location":"api-reference/embeddings/","text":"\u5d4c\u5165 \u00b6 \u83b7\u5f97\u4e00\u4e2a\u7ed9\u5b9a\u8f93\u5165\u7684\u5411\u91cf\u8868\u793a\uff0c\u53ef\u4ee5\u88ab\u673a\u5668\u5b66\u4e60\u6a21\u578b\u548c\u7b97\u6cd5\u8f7b\u677e\u4f7f\u7528\u3002 \u76f8\u5173\u6307\u5357: Embeddings \u521b\u5efa\u5d4c\u5165 \u00b6 POST : https://api.openai.com/v1/embeddings \u521b\u5efa\u8868\u793a\u8f93\u5165\u6587\u672c\u7684\u5d4c\u5165\u5411\u91cf\u3002 \u8bf7\u6c42\u4f53 : model string Required ID of the model to use. You can use the List models API to see all of your available models, or see our Model overview for descriptions of them. input string or array Required Input text to get embeddings for, encoded as a string or array of tokens. To get embeddings for multiple inputs in a single request, pass an array of strings or array of token arrays. Each input must not exceed 8192 tokens in length. user string Optional A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. Learn more. \u793a\u4f8b : node.js 1 2 3 4 5 6 7 8 9 const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . createEmbedding ({ model : \"text-embedding-ada-002\" , input : \"The food was delicious and the waiter...\" , }); \u53c2\u6570 : 1 2 3 4 { \"model\" : \"text-embedding-ada-002\" , \"input\" : \"The food was delicious and the waiter...\" } \u8fd4\u56de : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 { \"object\" : \"list\" , \"data\" : [ { \"object\" : \"embedding\" , \"embedding\" : [ 0.0023064255 , -0.009327292 , .... ( 1536 fl oa ts t o tal f or ada -002 ) -0.0028842222 ], \"index\" : 0 } ], \"model\" : \"text-embedding-ada-002\" , \"usage\" : { \"prompt_tokens\" : 8 , \"total_tokens\" : 8 } }","title":"\u5d4c\u5165"},{"location":"api-reference/embeddings/#_1","text":"\u83b7\u5f97\u4e00\u4e2a\u7ed9\u5b9a\u8f93\u5165\u7684\u5411\u91cf\u8868\u793a\uff0c\u53ef\u4ee5\u88ab\u673a\u5668\u5b66\u4e60\u6a21\u578b\u548c\u7b97\u6cd5\u8f7b\u677e\u4f7f\u7528\u3002 \u76f8\u5173\u6307\u5357: Embeddings","title":"\u5d4c\u5165"},{"location":"api-reference/embeddings/#_2","text":"POST : https://api.openai.com/v1/embeddings \u521b\u5efa\u8868\u793a\u8f93\u5165\u6587\u672c\u7684\u5d4c\u5165\u5411\u91cf\u3002 \u8bf7\u6c42\u4f53 : model string Required ID of the model to use. You can use the List models API to see all of your available models, or see our Model overview for descriptions of them. input string or array Required Input text to get embeddings for, encoded as a string or array of tokens. To get embeddings for multiple inputs in a single request, pass an array of strings or array of token arrays. Each input must not exceed 8192 tokens in length. user string Optional A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. Learn more. \u793a\u4f8b : node.js 1 2 3 4 5 6 7 8 9 const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . createEmbedding ({ model : \"text-embedding-ada-002\" , input : \"The food was delicious and the waiter...\" , }); \u53c2\u6570 : 1 2 3 4 { \"model\" : \"text-embedding-ada-002\" , \"input\" : \"The food was delicious and the waiter...\" } \u8fd4\u56de : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 { \"object\" : \"list\" , \"data\" : [ { \"object\" : \"embedding\" , \"embedding\" : [ 0.0023064255 , -0.009327292 , .... ( 1536 fl oa ts t o tal f or ada -002 ) -0.0028842222 ], \"index\" : 0 } ], \"model\" : \"text-embedding-ada-002\" , \"usage\" : { \"prompt_tokens\" : 8 , \"total_tokens\" : 8 } }","title":"\u521b\u5efa\u5d4c\u5165"},{"location":"api-reference/engines/","text":"\u5f15\u64ce(\u5f03) \u00b6 \u5f15\u64ce\u7aef\u70b9\u5df2\u5f03\u7528\u3002 \u8bf7\u4f7f\u7528\u4ed6\u4eec\u7684\u66ff\u4ee3\u54c1 \u6a21\u578b \u3002\u5b66\u4e60\u66f4\u591a\u7684\u77e5\u8bc6\u3002 \u8fd9\u4e9b\u7aef\u70b9\u63cf\u8ff0\u5e76\u63d0\u4f9b\u4e86\u5bf9 API \u4e2d\u5404\u79cd\u53ef\u7528\u5f15\u64ce\u7684\u8bbf\u95ee\u3002 \u5217\u51fa\u5f15\u64ce Deprecated \u00b6 GET : https://api.openai.com/v1/engines \u5217\u51fa\u5f53\u524d\u53ef\u7528\u7684(\u672a\u7ecf\u4f18\u5316\u7684)\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u5173\u4e8e\u6bcf\u4e2a\u6a21\u578b\u7684\u57fa\u672c\u4fe1\u606f\uff0c\u5982\u6240\u6709\u8005\u548c\u53ef\u7528\u6027\u3002 \u793a\u4f8b : 1 2 3 4 5 6 const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . listEngines (); \u8fd4\u56de : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 { \"data\" : [ { \"id\" : \"engine-id-0\" , \"object\" : \"engine\" , \"owner\" : \"organization-owner\" , \"ready\" : true }, { \"id\" : \"engine-id-2\" , \"object\" : \"engine\" , \"owner\" : \"organization-owner\" , \"ready\" : true }, { \"id\" : \"engine-id-3\" , \"object\" : \"engine\" , \"owner\" : \"openai\" , \"ready\" : false } ], \"object\" : \"list\" } \u68c0\u7d22\u5f15\u64ce Deprecated \u00b6 GET : https://api.openai.com/v1/engines/{engine_id } Retrieves a model instance, providing basic information about it such as the owner and availability. \u8def\u5f84\u53c2\u6570 : engine_id string Required The ID of the engine to use for this request \u793a\u4f8b : text-davinci-003 text-davinci-003 1 2 3 4 5 6 const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . retrieveEngine ( \"text-davinci-003\" ); \u8fd4\u56de : text-davinci-003 text-davinci-003 1 2 3 4 5 6 { \"id\" : \"text-davinci-003\" , \"object\" : \"engine\" , \"owner\" : \"openai\" , \"ready\" : true }","title":"\u5f15\u64ce(\u5f03)"},{"location":"api-reference/engines/#_1","text":"\u5f15\u64ce\u7aef\u70b9\u5df2\u5f03\u7528\u3002 \u8bf7\u4f7f\u7528\u4ed6\u4eec\u7684\u66ff\u4ee3\u54c1 \u6a21\u578b \u3002\u5b66\u4e60\u66f4\u591a\u7684\u77e5\u8bc6\u3002 \u8fd9\u4e9b\u7aef\u70b9\u63cf\u8ff0\u5e76\u63d0\u4f9b\u4e86\u5bf9 API \u4e2d\u5404\u79cd\u53ef\u7528\u5f15\u64ce\u7684\u8bbf\u95ee\u3002","title":"\u5f15\u64ce(\u5f03)"},{"location":"api-reference/engines/#deprecated","text":"GET : https://api.openai.com/v1/engines \u5217\u51fa\u5f53\u524d\u53ef\u7528\u7684(\u672a\u7ecf\u4f18\u5316\u7684)\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u5173\u4e8e\u6bcf\u4e2a\u6a21\u578b\u7684\u57fa\u672c\u4fe1\u606f\uff0c\u5982\u6240\u6709\u8005\u548c\u53ef\u7528\u6027\u3002 \u793a\u4f8b : 1 2 3 4 5 6 const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . listEngines (); \u8fd4\u56de : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 { \"data\" : [ { \"id\" : \"engine-id-0\" , \"object\" : \"engine\" , \"owner\" : \"organization-owner\" , \"ready\" : true }, { \"id\" : \"engine-id-2\" , \"object\" : \"engine\" , \"owner\" : \"organization-owner\" , \"ready\" : true }, { \"id\" : \"engine-id-3\" , \"object\" : \"engine\" , \"owner\" : \"openai\" , \"ready\" : false } ], \"object\" : \"list\" }","title":"\u5217\u51fa\u5f15\u64ce Deprecated"},{"location":"api-reference/engines/#deprecated_1","text":"GET : https://api.openai.com/v1/engines/{engine_id } Retrieves a model instance, providing basic information about it such as the owner and availability. \u8def\u5f84\u53c2\u6570 : engine_id string Required The ID of the engine to use for this request \u793a\u4f8b : text-davinci-003 text-davinci-003 1 2 3 4 5 6 const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . retrieveEngine ( \"text-davinci-003\" ); \u8fd4\u56de : text-davinci-003 text-davinci-003 1 2 3 4 5 6 { \"id\" : \"text-davinci-003\" , \"object\" : \"engine\" , \"owner\" : \"openai\" , \"ready\" : true }","title":"\u68c0\u7d22\u5f15\u64ce Deprecated"},{"location":"api-reference/files/","text":"\u6587\u4ef6 \u00b6 \u6587\u4ef6\u7528\u4e8e\u4e0a\u4f20\u53ef\u4e0e\u5fae\u8c03\u7b49\u529f\u80fd\u4e00\u8d77\u4f7f\u7528\u7684\u6587\u6863\u3002 \u5217\u8868\u6587\u4ef6 \u00b6 GET : https://api.openai.com/v1/files \u8fd4\u56de\u5c5e\u4e8e\u7528\u6237\u7ec4\u7ec7\u7684\u6587\u4ef6\u5217\u8868\u3002 Example : request node.js 1 2 3 4 5 6 const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . listFiles (); * *\u8fd4\u56de :** 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 { \"data\" : [ { \"id\" : \"file-ccdDZrC3iZVNiQVeEA6Z66wf\" , \"object\" : \"file\" , \"bytes\" : 175 , \"created_at\" : 1613677385 , \"filename\" : \"train.jsonl\" , \"purpose\" : \"search\" }, { \"id\" : \"file-XjGxS3KTG0uNmNOK362iJua3\" , \"object\" : \"file\" , \"bytes\" : 140 , \"created_at\" : 1613779121 , \"filename\" : \"puppy.jsonl\" , \"purpose\" : \"search\" } ], \"object\" : \"list\" } \u4e0a\u4f20\u6587\u4ef6 \u00b6 POST https://api.openai.com/v1/files Upload a file that contains document(s) to be used across various endpoints/features. Currently, the size of all the files uploaded by one organization can be up to 1 GB. Please contact us if you need to increase the storage limit. \u8bf7\u6c42\u4f53 : file string Required Name of the JSON Lines file to be uploaded. If the purpose is set to \"fine-tune\", each line is a JSON record with \"prompt\" and \"completion\" fields representing your training examples. purpose string Required The intended purpose of the uploaded documents. Use \"fine-tune\" for Fine-tuning. This allows us to validate the format of the uploaded file. \u793a\u4f8b : node.js node.js 1 2 3 4 5 6 7 const fs = require ( \"fs\" ); const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . createFile ( fs . createReadStream ( \"mydata.jsonl\" ), \"fine-tune\" ); \u8fd4\u56de : 1 2 3 4 5 6 7 8 { \"id\" : \"file-XjGxS3KTG0uNmNOK362iJua3\" , \"object\" : \"file\" , \"bytes\" : 140 , \"created_at\" : 1613779121 , \"filename\" : \"mydata.jsonl\" , \"purpose\" : \"fine-tune\" } Delete file \u00b6 DELETE https://api.openai.com/v1/files/{file_id } Delete a file. \u8def\u5f84\u53c2\u6570 : file_id string Required The ID of the file to use for this request \u793a\u4f8b : node.js node.js 1 2 3 4 5 6 const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . deleteFile ( \"file-XjGxS3KTG0uNmNOK362iJua3\" ); \u8fd4\u56de : 1 2 3 4 5 { \"id\" : \"file-XjGxS3KTG0uNmNOK362iJua3\" , \"object\" : \"file\" , \"deleted\" : true } \u68c0\u7d22\u6587\u4ef6 \u00b6 GET : https://api.openai.com/v1/files/{file_id } Returns information about a specific file. \u8def\u5f84\u53c2\u6570 : file_id string Required The ID of the file to use for this request \u793a\u4f8b : node.js node.js 1 2 3 4 5 6 const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . retrieveFile ( \"file-XjGxS3KTG0uNmNOK362iJua3\" ); \u8fd4\u56de : 1 2 3 4 5 6 7 8 { \"id\" : \"file-XjGxS3KTG0uNmNOK362iJua3\" , \"object\" : \"file\" , \"bytes\" : 140 , \"created_at\" : 1613779657 , \"filename\" : \"mydata.jsonl\" , \"purpose\" : \"fine-tune\" } \u68c0\u7d22\u6587\u4ef6\u5185\u5bb9 \u00b6 GET : https://api.openai.com/v1/files/{file_id}/content Returns the contents of the specified file \u8def\u5f84\u53c2\u6570 : file_id string Required The ID of the file to use for this request \u793a\u4f8b : node.js 1 2 3 4 5 6 7 node . js ; const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . downloadFile ( \"file-XjGxS3KTG0uNmNOK362iJua3\" );","title":"\u6587\u4ef6"},{"location":"api-reference/files/#_1","text":"\u6587\u4ef6\u7528\u4e8e\u4e0a\u4f20\u53ef\u4e0e\u5fae\u8c03\u7b49\u529f\u80fd\u4e00\u8d77\u4f7f\u7528\u7684\u6587\u6863\u3002","title":"\u6587\u4ef6"},{"location":"api-reference/files/#_2","text":"GET : https://api.openai.com/v1/files \u8fd4\u56de\u5c5e\u4e8e\u7528\u6237\u7ec4\u7ec7\u7684\u6587\u4ef6\u5217\u8868\u3002 Example : request node.js 1 2 3 4 5 6 const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . listFiles (); * *\u8fd4\u56de :** 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 { \"data\" : [ { \"id\" : \"file-ccdDZrC3iZVNiQVeEA6Z66wf\" , \"object\" : \"file\" , \"bytes\" : 175 , \"created_at\" : 1613677385 , \"filename\" : \"train.jsonl\" , \"purpose\" : \"search\" }, { \"id\" : \"file-XjGxS3KTG0uNmNOK362iJua3\" , \"object\" : \"file\" , \"bytes\" : 140 , \"created_at\" : 1613779121 , \"filename\" : \"puppy.jsonl\" , \"purpose\" : \"search\" } ], \"object\" : \"list\" }","title":"\u5217\u8868\u6587\u4ef6"},{"location":"api-reference/files/#_3","text":"POST https://api.openai.com/v1/files Upload a file that contains document(s) to be used across various endpoints/features. Currently, the size of all the files uploaded by one organization can be up to 1 GB. Please contact us if you need to increase the storage limit. \u8bf7\u6c42\u4f53 : file string Required Name of the JSON Lines file to be uploaded. If the purpose is set to \"fine-tune\", each line is a JSON record with \"prompt\" and \"completion\" fields representing your training examples. purpose string Required The intended purpose of the uploaded documents. Use \"fine-tune\" for Fine-tuning. This allows us to validate the format of the uploaded file. \u793a\u4f8b : node.js node.js 1 2 3 4 5 6 7 const fs = require ( \"fs\" ); const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . createFile ( fs . createReadStream ( \"mydata.jsonl\" ), \"fine-tune\" ); \u8fd4\u56de : 1 2 3 4 5 6 7 8 { \"id\" : \"file-XjGxS3KTG0uNmNOK362iJua3\" , \"object\" : \"file\" , \"bytes\" : 140 , \"created_at\" : 1613779121 , \"filename\" : \"mydata.jsonl\" , \"purpose\" : \"fine-tune\" }","title":"\u4e0a\u4f20\u6587\u4ef6"},{"location":"api-reference/files/#delete-file","text":"DELETE https://api.openai.com/v1/files/{file_id } Delete a file. \u8def\u5f84\u53c2\u6570 : file_id string Required The ID of the file to use for this request \u793a\u4f8b : node.js node.js 1 2 3 4 5 6 const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . deleteFile ( \"file-XjGxS3KTG0uNmNOK362iJua3\" ); \u8fd4\u56de : 1 2 3 4 5 { \"id\" : \"file-XjGxS3KTG0uNmNOK362iJua3\" , \"object\" : \"file\" , \"deleted\" : true }","title":"Delete file"},{"location":"api-reference/files/#_4","text":"GET : https://api.openai.com/v1/files/{file_id } Returns information about a specific file. \u8def\u5f84\u53c2\u6570 : file_id string Required The ID of the file to use for this request \u793a\u4f8b : node.js node.js 1 2 3 4 5 6 const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . retrieveFile ( \"file-XjGxS3KTG0uNmNOK362iJua3\" ); \u8fd4\u56de : 1 2 3 4 5 6 7 8 { \"id\" : \"file-XjGxS3KTG0uNmNOK362iJua3\" , \"object\" : \"file\" , \"bytes\" : 140 , \"created_at\" : 1613779657 , \"filename\" : \"mydata.jsonl\" , \"purpose\" : \"fine-tune\" }","title":"\u68c0\u7d22\u6587\u4ef6"},{"location":"api-reference/files/#_5","text":"GET : https://api.openai.com/v1/files/{file_id}/content Returns the contents of the specified file \u8def\u5f84\u53c2\u6570 : file_id string Required The ID of the file to use for this request \u793a\u4f8b : node.js 1 2 3 4 5 6 7 node . js ; const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . downloadFile ( \"file-XjGxS3KTG0uNmNOK362iJua3\" );","title":"\u68c0\u7d22\u6587\u4ef6\u5185\u5bb9"},{"location":"api-reference/fine-tunes/","text":"\u5fae\u8c03 \u00b6 \u7ba1\u7406\u5fae\u8c03\u5de5\u4f5c\uff0c\u4f7f\u6a21\u578b\u9002\u5408\u7279\u5b9a\u7684\u8bad\u7ec3\u6570\u636e\u3002 \u76f8\u5173\u6307\u5357: Fine-tune models \u521b\u5efa\u8c03\u6574 \u00b6 POST : https://api.openai.com/v1/fine-tunes \u521b\u5efa\u4ece\u7ed9\u5b9a\u6570\u636e\u96c6\u5bf9\u6307\u5b9a\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u7684\u4f5c\u4e1a\u3002 \u54cd\u5e94\u5305\u542b\u6392\u961f\u4f5c\u4e1a\u7684\u8be6\u7ec6\u4fe1\u606f\uff0c\u5305\u62ec\u5b8c\u6210\u540e\u7684\u4f5c\u4e1a\u72b6\u6001\u548c\u8c03\u4f18\u6a21\u578b\u7684\u540d\u79f0\u3002 \u4e86\u89e3\u66f4\u591a\u5173\u4e8e\u5fae\u8c03\u7684\u4fe1\u606f \u8bf7\u6c42\u4f53 : \u53c2\u6570 \u7c7b\u578b \u5fc5\u987b \u9ed8\u8ba4 \u63cf\u8ff0 training_file string Required \u4e0a\u4f20\u7684\u57f9\u8bad\u6570\u636e\u6587\u4ef6 ID\u3002\u6709\u5173\u5982\u4f55\u4e0a\u4f20\u6587\u4ef6\uff0c\u8bf7\u53c2\u89c1\u4e0a\u4f20\u6587\u4ef6\u3002\u60a8\u7684\u6570\u636e\u96c6\u5fc5\u987b\u683c\u5f0f\u5316\u4e3a JSONL \u6587\u4ef6\uff0c\u5176\u4e2d\u6bcf\u4e2a\u8bad\u7ec3\u793a\u4f8b\u90fd\u662f\u5e26\u6709\u201c\u63d0\u793a\u201d\u548c\u201c\u5b8c\u6210\u201d\u952e\u7684 JSON \u5bf9\u8c61\u3002\u6b64\u5916\uff0c\u60a8\u5fc5\u987b\u4e0a\u4f20\u5e26\u6709\u5fae\u8c03\u76ee\u7684\u7684\u6587\u4ef6\u3002\u6709\u5173\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u5fae\u8c03\u6307\u5357\u3002 validation_file string Optional \u5305\u542b\u9a8c\u8bc1\u6570\u636e\u7684\u4e0a\u4f20\u6587\u4ef6\u7684 ID\u3002\u5982\u679c\u60a8\u63d0\u4f9b\u6b64\u6587\u4ef6\uff0c\u5219\u5728\u8c03\u4f18\u671f\u95f4\uff0c\u6570\u636e\u5c06\u7528\u4e8e\u5b9a\u671f\u751f\u6210\u9a8c\u8bc1\u6307\u6807\u3002\u8fd9\u4e9b\u6307\u6807\u53ef\u4ee5\u5728\u5fae\u8c03\u7ed3\u679c\u6587\u4ef6\u4e2d\u67e5\u770b\u3002\u8bad\u7ec3\u6570\u636e\u548c\u9a8c\u8bc1\u6570\u636e\u5e94\u8be5\u662f\u4e92\u65a5\u7684\u3002\u60a8\u7684\u6570\u636e\u96c6\u5fc5\u987b\u683c\u5f0f\u5316\u4e3a JSONL \u6587\u4ef6\uff0c\u5176\u4e2d\u6bcf\u4e2a\u9a8c\u8bc1\u793a\u4f8b\u90fd\u662f\u5e26\u6709\u201c\u63d0\u793a\u201d\u548c\u201c\u5b8c\u6210\u201d\u952e\u7684 JSON \u5bf9\u8c61\u3002\u6b64\u5916\uff0c\u60a8\u5fc5\u987b\u4e0a\u4f20\u5e26\u6709\u5fae\u8c03\u76ee\u7684\u7684\u6587\u4ef6\u3002\u6709\u5173\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u5fae\u8c03\u6307\u5357\u3002 model string Optional curie \u8981\u5fae\u8c03\u7684\u57fa\u672c\u6a21\u578b\u7684\u540d\u79f0\u3002\u60a8\u53ef\u4ee5\u4ece\u201cada\u201d\u3001\u201cbabbage\u201d\u3001\u201ccurie\u201d\u3001\u201cdavinci\u201d\u6216\u5728 2022-04-21 \u4e4b\u540e\u521b\u5efa\u7684\u5fae\u8c03\u6a21\u578b\u4e2d\u9009\u62e9\u4e00\u4e2a\u3002\u8981\u4e86\u89e3\u5173\u4e8e\u8fd9\u4e9b\u6a21\u578b\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 models \u6587\u6863\u3002 n_epochs integer Optional 4 \u8bad\u7ec3\u6a21\u578b\u7684 epoch \u6570\u3002epoch \u6307\u7684\u662f\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u4e00\u4e2a\u5b8c\u6574\u5468\u671f\u3002 batch_size integer Optional null \u7528\u4e8e\u57f9\u8bad\u7684\u6279\u5927\u5c0f\u3002\u6279\u5927\u5c0f\u662f\u7528\u4e8e\u8bad\u7ec3\u5355\u4e2a\u5411\u524d\u548c\u5411\u540e\u4f20\u9012\u7684\u8bad\u7ec3\u793a\u4f8b\u7684\u6570\u91cf\u3002\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u6279\u5904\u7406\u5927\u5c0f\u5c06\u88ab\u52a8\u6001\u914d\u7f6e\u4e3a\u8bad\u7ec3\u96c6\u4e2d\u793a\u4f8b\u6570\u91cf\u7684~0.2%\uff0c\u4e0a\u9650\u4e3a 256 -\u901a\u5e38\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u53d1\u73b0\u66f4\u5927\u7684\u6279\u5904\u7406\u5927\u5c0f\u5f80\u5f80\u66f4\u9002\u5408\u4e8e\u66f4\u5927\u7684\u6570\u636e\u96c6\u3002 learning_rate_multiplier number Optional null \u7528\u4e8e\u8bad\u7ec3\u7684\u5b66\u4e60\u7387\u4e58\u6570\u3002\u5fae\u8c03\u5b66\u4e60\u7387\u662f\u7528\u4e8e\u9884\u8bad\u7ec3\u7684\u539f\u59cb\u5b66\u4e60\u7387\u4e58\u4ee5\u6b64\u503c\u3002\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u5b66\u4e60\u7387\u4e58\u6570\u662f 0.05\u30010.1 \u6216 0.2\uff0c\u8fd9\u53d6\u51b3\u4e8e\u6700\u7ec8\u7684 batch_size(\u8f83\u5927\u7684\u5b66\u4e60\u7387\u5f80\u5f80\u5728\u8f83\u5927\u7684\u6279\u5904\u7406\u5927\u5c0f\u4e0b\u8868\u73b0\u66f4\u597d)\u3002\u6211\u4eec\u5efa\u8bae\u5728 0.02 \u5230 0.2 \u7684\u8303\u56f4\u5185\u8fdb\u884c\u8bd5\u9a8c\uff0c\u770b\u770b\u4ec0\u4e48\u4f1a\u4ea7\u751f\u6700\u597d\u7684\u7ed3\u679c\u3002 prompt_loss_weight number Optional 0.01 \u7528\u4e8e\u63d0\u793a\u4ee4\u724c\u635f\u5931\u7684\u6743\u91cd\u3002\u8fd9\u63a7\u5236\u4e86\u6a21\u578b\u5c1d\u8bd5\u5b66\u4e60\u751f\u6210\u63d0\u793a\u7684\u7a0b\u5ea6(\u4e0e\u5b8c\u6210\u5ea6\u7684\u6743\u91cd\u59cb\u7ec8\u4e3a 1.0 \u76f8\u6bd4)\uff0c\u5e76\u4e14\u53ef\u4ee5\u5728\u5b8c\u6210\u5ea6\u8f83\u77ed\u65f6\u4e3a\u8bad\u7ec3\u6dfb\u52a0\u7a33\u5b9a\u6548\u679c\u3002\u5982\u679c\u63d0\u793a\u975e\u5e38\u957f(\u76f8\u5bf9\u4e8e\u5b8c\u6210)\uff0c\u51cf\u5c11\u8fd9\u4e2a\u6743\u91cd\u53ef\u80fd\u662f\u6709\u610f\u4e49\u7684\uff0c\u4ee5\u907f\u514d\u8fc7\u5ea6\u4f18\u5148\u5b66\u4e60\u63d0\u793a\u3002 compute_classification_metrics boolean Optional false \u5982\u679c\u8bbe\u7f6e\u4e86\uff0c\u6211\u4eec\u5c06\u5728\u6bcf\u4e2a\u7eaa\u5143\u7ed3\u675f\u65f6\u4f7f\u7528\u9a8c\u8bc1\u96c6\u8ba1\u7b97\u7279\u5b9a\u4e8e\u5206\u7c7b\u7684\u6307\u6807\uff0c\u5982\u51c6\u786e\u6027\u548c F-1 \u5206\u6570\u3002\u8fd9\u4e9b\u6307\u6807\u53ef\u4ee5\u5728\u7ed3\u679c\u6587\u4ef6\u4e2d\u67e5\u770b\u3002\u4e3a\u4e86\u8ba1\u7b97\u5206\u7c7b\u6307\u6807\uff0c\u5fc5\u987b\u63d0\u4f9b validation_file\u3002\u6b64\u5916\uff0c\u5fc5\u987b\u4e3a\u591a\u7c7b\u5206\u7c7b\u6307\u5b9a classification_n_classes\uff0c\u4e3a\u4e8c\u8fdb\u5236\u5206\u7c7b\u6307\u5b9a classification_positive_class\u3002 classification_n_classes integer Optional null \u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u7c7b\u6570\u3002\u591a\u7c7b\u5206\u7c7b\u65f6\uff0c\u6b64\u53c2\u6570\u5fc5\u9009\u3002 classification_positive_class string Optional null \u4e8c\u5143\u5206\u7c7b\u4e2d\u7684\u6b63\u7c7b\u5728\u8fdb\u884c\u4e8c\u8fdb\u5236\u5206\u7c7b\u65f6\uff0c\u9700\u8981\u8fd9\u4e2a\u53c2\u6570\u6765\u751f\u6210\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u548c F1 \u6307\u6807\u3002 classification_betas array Optional null \u5982\u679c\u63d0\u4f9b\u4e86\u8fd9\u4e2a\uff0c\u6211\u4eec\u5c06\u5728\u6307\u5b9a\u7684 beta \u503c\u5904\u8ba1\u7b97 F-beta \u5206\u6570\u3002F-beta \u5206\u6570\u662f F-1 \u5206\u6570\u7684\u6cdb\u5316\u3002\u8fd9\u53ea\u7528\u4e8e\u4e8c\u8fdb\u5236\u5206\u7c7b\u3002\u5982\u679c beta \u503c\u4e3a 1(\u5373 F-1 \u5206)\uff0c\u7cbe\u786e\u5ea6\u548c\u56de\u5fc6\u7387\u5177\u6709\u76f8\u540c\u7684\u6743\u91cd\u3002\u66f4\u5927\u7684\u6d4b\u8bd5\u5206\u6570\u66f4\u6ce8\u91cd\u56de\u5fc6\uff0c\u800c\u4e0d\u662f\u51c6\u786e\u6027\u3002\u6d4b\u8bd5\u7248\u5206\u6570\u8d8a\u5c0f\uff0c\u7cbe\u786e\u5ea6\u5c31\u8d8a\u91cd\u8981\uff0c\u56de\u5fc6\u5c31\u8d8a\u4e0d\u91cd\u8981\u3002 suffix string Optional null \u4e00\u4e2a\u6700\u591a 40 \u4e2a\u5b57\u7b26\u7684\u5b57\u7b26\u4e32\uff0c\u5c06\u88ab\u6dfb\u52a0\u5230\u7ecf\u8fc7\u5fae\u8c03\u7684\u6a21\u578b\u540d\u79f0\u4e2d\u3002\u4f8b\u5982\uff0c\u540e\u7f00\u201ccustom-model-name\u201d\u5c06\u751f\u6210\u50cf ada:ft-your-org:custom-model-name-2022-02-15-04-21-04 \u8fd9\u6837\u7684\u6a21\u578b\u540d\u3002 \u793a\u4f8b : 1 2 3 4 5 6 7 8 const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . createFineTune ({ training_file : \"file-XGinujblHPwGLSztz8cPS8XY\" , }); \u8fd4\u56de 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 { \"id\" : \"ft-AF1WoRqd3aJAHsqc9NY7iL8F\" , \"object\" : \"fine-tune\" , \"model\" : \"curie\" , \"created_at\" : 1614807352 , \"events\" : [ { \"object\" : \"fine-tune-event\" , \"created_at\" : 1614807352 , \"level\" : \"info\" , \"message\" : \"Job enqueued. Waiting for jobs ahead to complete. Queue number: 0.\" } ], \"fine_tuned_model\" : null , \"hyperparams\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 0.1 , \"n_epochs\" : 4 , \"prompt_loss_weight\" : 0.1 }, \"organization_id\" : \"org-...\" , \"result_files\" : [], \"status\" : \"pending\" , \"validation_files\" : [], \"training_files\" : [ { \"id\" : \"file-XGinujblHPwGLSztz8cPS8XY\" , \"object\" : \"file\" , \"bytes\" : 1547276 , \"created_at\" : 1610062281 , \"filename\" : \"my-data-train.jsonl\" , \"purpose\" : \"fine-tune-train\" } ], \"updated_at\" : 1614807352 } \u5217\u51fa\u5fae\u8c03 \u00b6 GET : https://api.openai.com/v1/fine-tunes \u5217\u51fa\u7ec4\u7ec7\u7684\u5fae\u8c03\u5de5\u4f5c \u793a\u4f8b : 1 2 3 4 5 6 const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . listFineTunes (); \u8fd4\u56de 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 { \"object\" : \"list\" , \"data\" : [ { \"id\" : \"ft-AF1WoRqd3aJAHsqc9NY7iL8F\" , \"object\" : \"fine-tune\" , \"model\" : \"curie\" , \"created_at\" : 1614807352 , \"fine_tuned_model\" : null , \"hyperparams\" : { ... }, \"organization_id\" : \"org-...\" , \"result_files\" : [], \"status\" : \"pending\" , \"validation_files\" : [], \"training_files\" : [{}], \"updated_at\" : 1614807352 }, { ... }, { ... } ] } \u68c0\u7d22\u5fae\u8c03 \u00b6 GET : https://api.openai.com/v1/fine-tunes/{fine_tune_id } \u83b7\u53d6\u6709\u5173\u5fae\u8c03\u4f5c\u4e1a\u7684\u4fe1\u606f\u3002 \u4e86\u89e3\u66f4\u591a\u5173\u4e8e\u5fae\u8c03\u7684\u4fe1\u606f \u8def\u5f84\u53c2\u6570 : \u53c2\u6570 \u7c7b\u578b \u5fc5\u987b \u63cf\u8ff0 fine_tune_id string Required The ID of the fine-tune job \u793a\u4f8b : 1 2 3 4 5 6 const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . retrieveFineTune ( \"ft-AF1WoRqd3aJAHsqc9NY7iL8F\" ); \u8fd4\u56de 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 { \"id\" : \"ft-AF1WoRqd3aJAHsqc9NY7iL8F\" , \"object\" : \"fine-tune\" , \"model\" : \"curie\" , \"created_at\" : 1614807352 , \"events\" : [ { \"object\" : \"fine-tune-event\" , \"created_at\" : 1614807352 , \"level\" : \"info\" , \"message\" : \"Job enqueued. Waiting for jobs ahead to complete. Queue number: 0.\" }, { \"object\" : \"fine-tune-event\" , \"created_at\" : 1614807356 , \"level\" : \"info\" , \"message\" : \"Job started.\" }, { \"object\" : \"fine-tune-event\" , \"created_at\" : 1614807861 , \"level\" : \"info\" , \"message\" : \"Uploaded snapshot: curie:ft-acmeco-2021-03-03-21-44-20.\" }, { \"object\" : \"fine-tune-event\" , \"created_at\" : 1614807864 , \"level\" : \"info\" , \"message\" : \"Uploaded result files: file-QQm6ZpqdNwAaVC3aSz5sWwLT.\" }, { \"object\" : \"fine-tune-event\" , \"created_at\" : 1614807864 , \"level\" : \"info\" , \"message\" : \"Job succeeded.\" } ], \"fine_tuned_model\" : \"curie:ft-acmeco-2021-03-03-21-44-20\" , \"hyperparams\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 0.1 , \"n_epochs\" : 4 , \"prompt_loss_weight\" : 0.1 }, \"organization_id\" : \"org-...\" , \"result_files\" : [ { \"id\" : \"file-QQm6ZpqdNwAaVC3aSz5sWwLT\" , \"object\" : \"file\" , \"bytes\" : 81509 , \"created_at\" : 1614807863 , \"filename\" : \"compiled_results.csv\" , \"purpose\" : \"fine-tune-results\" } ], \"status\" : \"succeeded\" , \"validation_files\" : [], \"training_files\" : [ { \"id\" : \"file-XGinujblHPwGLSztz8cPS8XY\" , \"object\" : \"file\" , \"bytes\" : 1547276 , \"created_at\" : 1610062281 , \"filename\" : \"my-data-train.jsonl\" , \"purpose\" : \"fine-tune-train\" } ], \"updated_at\" : 1614807865 } \u53d6\u6d88\u5fae\u8c03 \u00b6 POST : https://api.openai.com/v1/fine-tunes/{fine_tune_id}/cancel \u7acb\u5373\u53d6\u6d88\u5fae\u8c03\u4f5c\u4e1a\u3002 \u8def\u5f84\u53c2\u6570 : \u53c2\u6570 \u5b57\u7b26 \u9700\u8981 \u63cf\u8ff0 fine_tune_id string Required The ID of the fine-tune job to cancel \u793a\u4f8b : 1 2 3 4 5 6 const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . cancelFineTune ( \"ft-AF1WoRqd3aJAHsqc9NY7iL8F\" ); \u8fd4\u56de : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 { \"id\" : \"ft-xhrpBbvVUzYGo8oUO1FY4nI7\" , \"object\" : \"fine-tune\" , \"model\" : \"curie\" , \"created_at\" : 1614807770 , \"events\" : [{ ... }], \"fine_tuned_model\" : null , \"hyperparams\" : { ... }, \"organization_id\" : \"org-...\" , \"result_files\" : [], \"status\" : \"cancelled\" , \"validation_files\" : [], \"training_files\" : [ { \"id\" : \"file-XGinujblHPwGLSztz8cPS8XY\" , \"object\" : \"file\" , \"bytes\" : 1547276 , \"created_at\" : 1610062281 , \"filename\" : \"my-data-train.jsonl\" , \"purpose\" : \"fine-tune-train\" } ], \"updated_at\" : 1614807789 } \u5217\u51fa\u5fae\u8c03\u4e8b\u4ef6 \u00b6 GET : https://api.openai.com/v1/fine-tunes/{fine_tune_id}/events \u83b7\u53d6\u5fae\u8c03\u4f5c\u4e1a\u7684\u7ec6\u7c92\u5ea6\u72b6\u6001\u66f4\u65b0\u3002 \u8def\u5f84\u53c2\u6570 : | | | | | ---------------- | ----------------- | -------- | ----------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | | fine_tune_id | string | Required | | The ID of the fine-tune job to get events for. | | Query parameters | stream | boolean | Optional | Defaults to false | Whether to stream events for the fine-tune job. If set to true, events will be sent as data-only server-sent events as they become available. The stream will terminate with a data: [DONE] message when the job is finished (succeeded, cancelled, or failed). | If set to false, only events generated so far will be returned. \u793a\u4f8b : 1 2 3 4 5 6 const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . listFineTuneEvents ( \"ft-AF1WoRqd3aJAHsqc9NY7iL8F\" ); \u8fd4\u56de 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 { \"object\" : \"list\" , \"data\" : [ { \"object\" : \"fine-tune-event\" , \"created_at\" : 1614807352 , \"level\" : \"info\" , \"message\" : \"Job enqueued. Waiting for jobs ahead to complete. Queue number: 0.\" }, { \"object\" : \"fine-tune-event\" , \"created_at\" : 1614807356 , \"level\" : \"info\" , \"message\" : \"Job started.\" }, { \"object\" : \"fine-tune-event\" , \"created_at\" : 1614807861 , \"level\" : \"info\" , \"message\" : \"Uploaded snapshot: curie:ft-acmeco-2021-03-03-21-44-20.\" }, { \"object\" : \"fine-tune-event\" , \"created_at\" : 1614807864 , \"level\" : \"info\" , \"message\" : \"Uploaded result files: file-QQm6ZpqdNwAaVC3aSz5sWwLT.\" }, { \"object\" : \"fine-tune-event\" , \"created_at\" : 1614807864 , \"level\" : \"info\" , \"message\" : \"Job succeeded.\" } ] } \u5220\u9664\u5fae\u8c03\u6a21\u578b \u00b6 DELETE : https://api.openai.com/v1/models/{model } \u5220\u9664\u4e00\u4e2a\u7ecf\u8fc7\u5fae\u8c03\u7684\u6a21\u578b\u3002\u60a8\u7684\u7ec4\u7ec7\u4e2d\u5fc5\u987b\u5177\u6709\u201cOwner\u201d\u89d2\u8272\u3002 \u8def\u5f84\u53c2\u6570 : \u6a21\u578b \u5b57\u7b26 \u5fc5\u987b \u63cf\u8ff0 model string Required The model to delete \u793a\u4f8b : 1 2 3 4 5 6 const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . deleteModel ( \"curie:ft-acmeco-2021-03-03-21-44-20\" ); \u8fd4\u56de 1 2 3 4 5 { \"id\" : \"curie:ft-acmeco-2021-03-03-21-44-20\" , \"object\" : \"model\" , \"deleted\" : true }","title":"\u5fae\u8c03"},{"location":"api-reference/fine-tunes/#_1","text":"\u7ba1\u7406\u5fae\u8c03\u5de5\u4f5c\uff0c\u4f7f\u6a21\u578b\u9002\u5408\u7279\u5b9a\u7684\u8bad\u7ec3\u6570\u636e\u3002 \u76f8\u5173\u6307\u5357: Fine-tune models","title":"\u5fae\u8c03"},{"location":"api-reference/fine-tunes/#_2","text":"POST : https://api.openai.com/v1/fine-tunes \u521b\u5efa\u4ece\u7ed9\u5b9a\u6570\u636e\u96c6\u5bf9\u6307\u5b9a\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u7684\u4f5c\u4e1a\u3002 \u54cd\u5e94\u5305\u542b\u6392\u961f\u4f5c\u4e1a\u7684\u8be6\u7ec6\u4fe1\u606f\uff0c\u5305\u62ec\u5b8c\u6210\u540e\u7684\u4f5c\u4e1a\u72b6\u6001\u548c\u8c03\u4f18\u6a21\u578b\u7684\u540d\u79f0\u3002 \u4e86\u89e3\u66f4\u591a\u5173\u4e8e\u5fae\u8c03\u7684\u4fe1\u606f \u8bf7\u6c42\u4f53 : \u53c2\u6570 \u7c7b\u578b \u5fc5\u987b \u9ed8\u8ba4 \u63cf\u8ff0 training_file string Required \u4e0a\u4f20\u7684\u57f9\u8bad\u6570\u636e\u6587\u4ef6 ID\u3002\u6709\u5173\u5982\u4f55\u4e0a\u4f20\u6587\u4ef6\uff0c\u8bf7\u53c2\u89c1\u4e0a\u4f20\u6587\u4ef6\u3002\u60a8\u7684\u6570\u636e\u96c6\u5fc5\u987b\u683c\u5f0f\u5316\u4e3a JSONL \u6587\u4ef6\uff0c\u5176\u4e2d\u6bcf\u4e2a\u8bad\u7ec3\u793a\u4f8b\u90fd\u662f\u5e26\u6709\u201c\u63d0\u793a\u201d\u548c\u201c\u5b8c\u6210\u201d\u952e\u7684 JSON \u5bf9\u8c61\u3002\u6b64\u5916\uff0c\u60a8\u5fc5\u987b\u4e0a\u4f20\u5e26\u6709\u5fae\u8c03\u76ee\u7684\u7684\u6587\u4ef6\u3002\u6709\u5173\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u5fae\u8c03\u6307\u5357\u3002 validation_file string Optional \u5305\u542b\u9a8c\u8bc1\u6570\u636e\u7684\u4e0a\u4f20\u6587\u4ef6\u7684 ID\u3002\u5982\u679c\u60a8\u63d0\u4f9b\u6b64\u6587\u4ef6\uff0c\u5219\u5728\u8c03\u4f18\u671f\u95f4\uff0c\u6570\u636e\u5c06\u7528\u4e8e\u5b9a\u671f\u751f\u6210\u9a8c\u8bc1\u6307\u6807\u3002\u8fd9\u4e9b\u6307\u6807\u53ef\u4ee5\u5728\u5fae\u8c03\u7ed3\u679c\u6587\u4ef6\u4e2d\u67e5\u770b\u3002\u8bad\u7ec3\u6570\u636e\u548c\u9a8c\u8bc1\u6570\u636e\u5e94\u8be5\u662f\u4e92\u65a5\u7684\u3002\u60a8\u7684\u6570\u636e\u96c6\u5fc5\u987b\u683c\u5f0f\u5316\u4e3a JSONL \u6587\u4ef6\uff0c\u5176\u4e2d\u6bcf\u4e2a\u9a8c\u8bc1\u793a\u4f8b\u90fd\u662f\u5e26\u6709\u201c\u63d0\u793a\u201d\u548c\u201c\u5b8c\u6210\u201d\u952e\u7684 JSON \u5bf9\u8c61\u3002\u6b64\u5916\uff0c\u60a8\u5fc5\u987b\u4e0a\u4f20\u5e26\u6709\u5fae\u8c03\u76ee\u7684\u7684\u6587\u4ef6\u3002\u6709\u5173\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u5fae\u8c03\u6307\u5357\u3002 model string Optional curie \u8981\u5fae\u8c03\u7684\u57fa\u672c\u6a21\u578b\u7684\u540d\u79f0\u3002\u60a8\u53ef\u4ee5\u4ece\u201cada\u201d\u3001\u201cbabbage\u201d\u3001\u201ccurie\u201d\u3001\u201cdavinci\u201d\u6216\u5728 2022-04-21 \u4e4b\u540e\u521b\u5efa\u7684\u5fae\u8c03\u6a21\u578b\u4e2d\u9009\u62e9\u4e00\u4e2a\u3002\u8981\u4e86\u89e3\u5173\u4e8e\u8fd9\u4e9b\u6a21\u578b\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 models \u6587\u6863\u3002 n_epochs integer Optional 4 \u8bad\u7ec3\u6a21\u578b\u7684 epoch \u6570\u3002epoch \u6307\u7684\u662f\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u4e00\u4e2a\u5b8c\u6574\u5468\u671f\u3002 batch_size integer Optional null \u7528\u4e8e\u57f9\u8bad\u7684\u6279\u5927\u5c0f\u3002\u6279\u5927\u5c0f\u662f\u7528\u4e8e\u8bad\u7ec3\u5355\u4e2a\u5411\u524d\u548c\u5411\u540e\u4f20\u9012\u7684\u8bad\u7ec3\u793a\u4f8b\u7684\u6570\u91cf\u3002\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u6279\u5904\u7406\u5927\u5c0f\u5c06\u88ab\u52a8\u6001\u914d\u7f6e\u4e3a\u8bad\u7ec3\u96c6\u4e2d\u793a\u4f8b\u6570\u91cf\u7684~0.2%\uff0c\u4e0a\u9650\u4e3a 256 -\u901a\u5e38\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u53d1\u73b0\u66f4\u5927\u7684\u6279\u5904\u7406\u5927\u5c0f\u5f80\u5f80\u66f4\u9002\u5408\u4e8e\u66f4\u5927\u7684\u6570\u636e\u96c6\u3002 learning_rate_multiplier number Optional null \u7528\u4e8e\u8bad\u7ec3\u7684\u5b66\u4e60\u7387\u4e58\u6570\u3002\u5fae\u8c03\u5b66\u4e60\u7387\u662f\u7528\u4e8e\u9884\u8bad\u7ec3\u7684\u539f\u59cb\u5b66\u4e60\u7387\u4e58\u4ee5\u6b64\u503c\u3002\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u5b66\u4e60\u7387\u4e58\u6570\u662f 0.05\u30010.1 \u6216 0.2\uff0c\u8fd9\u53d6\u51b3\u4e8e\u6700\u7ec8\u7684 batch_size(\u8f83\u5927\u7684\u5b66\u4e60\u7387\u5f80\u5f80\u5728\u8f83\u5927\u7684\u6279\u5904\u7406\u5927\u5c0f\u4e0b\u8868\u73b0\u66f4\u597d)\u3002\u6211\u4eec\u5efa\u8bae\u5728 0.02 \u5230 0.2 \u7684\u8303\u56f4\u5185\u8fdb\u884c\u8bd5\u9a8c\uff0c\u770b\u770b\u4ec0\u4e48\u4f1a\u4ea7\u751f\u6700\u597d\u7684\u7ed3\u679c\u3002 prompt_loss_weight number Optional 0.01 \u7528\u4e8e\u63d0\u793a\u4ee4\u724c\u635f\u5931\u7684\u6743\u91cd\u3002\u8fd9\u63a7\u5236\u4e86\u6a21\u578b\u5c1d\u8bd5\u5b66\u4e60\u751f\u6210\u63d0\u793a\u7684\u7a0b\u5ea6(\u4e0e\u5b8c\u6210\u5ea6\u7684\u6743\u91cd\u59cb\u7ec8\u4e3a 1.0 \u76f8\u6bd4)\uff0c\u5e76\u4e14\u53ef\u4ee5\u5728\u5b8c\u6210\u5ea6\u8f83\u77ed\u65f6\u4e3a\u8bad\u7ec3\u6dfb\u52a0\u7a33\u5b9a\u6548\u679c\u3002\u5982\u679c\u63d0\u793a\u975e\u5e38\u957f(\u76f8\u5bf9\u4e8e\u5b8c\u6210)\uff0c\u51cf\u5c11\u8fd9\u4e2a\u6743\u91cd\u53ef\u80fd\u662f\u6709\u610f\u4e49\u7684\uff0c\u4ee5\u907f\u514d\u8fc7\u5ea6\u4f18\u5148\u5b66\u4e60\u63d0\u793a\u3002 compute_classification_metrics boolean Optional false \u5982\u679c\u8bbe\u7f6e\u4e86\uff0c\u6211\u4eec\u5c06\u5728\u6bcf\u4e2a\u7eaa\u5143\u7ed3\u675f\u65f6\u4f7f\u7528\u9a8c\u8bc1\u96c6\u8ba1\u7b97\u7279\u5b9a\u4e8e\u5206\u7c7b\u7684\u6307\u6807\uff0c\u5982\u51c6\u786e\u6027\u548c F-1 \u5206\u6570\u3002\u8fd9\u4e9b\u6307\u6807\u53ef\u4ee5\u5728\u7ed3\u679c\u6587\u4ef6\u4e2d\u67e5\u770b\u3002\u4e3a\u4e86\u8ba1\u7b97\u5206\u7c7b\u6307\u6807\uff0c\u5fc5\u987b\u63d0\u4f9b validation_file\u3002\u6b64\u5916\uff0c\u5fc5\u987b\u4e3a\u591a\u7c7b\u5206\u7c7b\u6307\u5b9a classification_n_classes\uff0c\u4e3a\u4e8c\u8fdb\u5236\u5206\u7c7b\u6307\u5b9a classification_positive_class\u3002 classification_n_classes integer Optional null \u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u7c7b\u6570\u3002\u591a\u7c7b\u5206\u7c7b\u65f6\uff0c\u6b64\u53c2\u6570\u5fc5\u9009\u3002 classification_positive_class string Optional null \u4e8c\u5143\u5206\u7c7b\u4e2d\u7684\u6b63\u7c7b\u5728\u8fdb\u884c\u4e8c\u8fdb\u5236\u5206\u7c7b\u65f6\uff0c\u9700\u8981\u8fd9\u4e2a\u53c2\u6570\u6765\u751f\u6210\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u548c F1 \u6307\u6807\u3002 classification_betas array Optional null \u5982\u679c\u63d0\u4f9b\u4e86\u8fd9\u4e2a\uff0c\u6211\u4eec\u5c06\u5728\u6307\u5b9a\u7684 beta \u503c\u5904\u8ba1\u7b97 F-beta \u5206\u6570\u3002F-beta \u5206\u6570\u662f F-1 \u5206\u6570\u7684\u6cdb\u5316\u3002\u8fd9\u53ea\u7528\u4e8e\u4e8c\u8fdb\u5236\u5206\u7c7b\u3002\u5982\u679c beta \u503c\u4e3a 1(\u5373 F-1 \u5206)\uff0c\u7cbe\u786e\u5ea6\u548c\u56de\u5fc6\u7387\u5177\u6709\u76f8\u540c\u7684\u6743\u91cd\u3002\u66f4\u5927\u7684\u6d4b\u8bd5\u5206\u6570\u66f4\u6ce8\u91cd\u56de\u5fc6\uff0c\u800c\u4e0d\u662f\u51c6\u786e\u6027\u3002\u6d4b\u8bd5\u7248\u5206\u6570\u8d8a\u5c0f\uff0c\u7cbe\u786e\u5ea6\u5c31\u8d8a\u91cd\u8981\uff0c\u56de\u5fc6\u5c31\u8d8a\u4e0d\u91cd\u8981\u3002 suffix string Optional null \u4e00\u4e2a\u6700\u591a 40 \u4e2a\u5b57\u7b26\u7684\u5b57\u7b26\u4e32\uff0c\u5c06\u88ab\u6dfb\u52a0\u5230\u7ecf\u8fc7\u5fae\u8c03\u7684\u6a21\u578b\u540d\u79f0\u4e2d\u3002\u4f8b\u5982\uff0c\u540e\u7f00\u201ccustom-model-name\u201d\u5c06\u751f\u6210\u50cf ada:ft-your-org:custom-model-name-2022-02-15-04-21-04 \u8fd9\u6837\u7684\u6a21\u578b\u540d\u3002 \u793a\u4f8b : 1 2 3 4 5 6 7 8 const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . createFineTune ({ training_file : \"file-XGinujblHPwGLSztz8cPS8XY\" , }); \u8fd4\u56de 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 { \"id\" : \"ft-AF1WoRqd3aJAHsqc9NY7iL8F\" , \"object\" : \"fine-tune\" , \"model\" : \"curie\" , \"created_at\" : 1614807352 , \"events\" : [ { \"object\" : \"fine-tune-event\" , \"created_at\" : 1614807352 , \"level\" : \"info\" , \"message\" : \"Job enqueued. Waiting for jobs ahead to complete. Queue number: 0.\" } ], \"fine_tuned_model\" : null , \"hyperparams\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 0.1 , \"n_epochs\" : 4 , \"prompt_loss_weight\" : 0.1 }, \"organization_id\" : \"org-...\" , \"result_files\" : [], \"status\" : \"pending\" , \"validation_files\" : [], \"training_files\" : [ { \"id\" : \"file-XGinujblHPwGLSztz8cPS8XY\" , \"object\" : \"file\" , \"bytes\" : 1547276 , \"created_at\" : 1610062281 , \"filename\" : \"my-data-train.jsonl\" , \"purpose\" : \"fine-tune-train\" } ], \"updated_at\" : 1614807352 }","title":"\u521b\u5efa\u8c03\u6574"},{"location":"api-reference/fine-tunes/#_3","text":"GET : https://api.openai.com/v1/fine-tunes \u5217\u51fa\u7ec4\u7ec7\u7684\u5fae\u8c03\u5de5\u4f5c \u793a\u4f8b : 1 2 3 4 5 6 const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . listFineTunes (); \u8fd4\u56de 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 { \"object\" : \"list\" , \"data\" : [ { \"id\" : \"ft-AF1WoRqd3aJAHsqc9NY7iL8F\" , \"object\" : \"fine-tune\" , \"model\" : \"curie\" , \"created_at\" : 1614807352 , \"fine_tuned_model\" : null , \"hyperparams\" : { ... }, \"organization_id\" : \"org-...\" , \"result_files\" : [], \"status\" : \"pending\" , \"validation_files\" : [], \"training_files\" : [{}], \"updated_at\" : 1614807352 }, { ... }, { ... } ] }","title":"\u5217\u51fa\u5fae\u8c03"},{"location":"api-reference/fine-tunes/#_4","text":"GET : https://api.openai.com/v1/fine-tunes/{fine_tune_id } \u83b7\u53d6\u6709\u5173\u5fae\u8c03\u4f5c\u4e1a\u7684\u4fe1\u606f\u3002 \u4e86\u89e3\u66f4\u591a\u5173\u4e8e\u5fae\u8c03\u7684\u4fe1\u606f \u8def\u5f84\u53c2\u6570 : \u53c2\u6570 \u7c7b\u578b \u5fc5\u987b \u63cf\u8ff0 fine_tune_id string Required The ID of the fine-tune job \u793a\u4f8b : 1 2 3 4 5 6 const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . retrieveFineTune ( \"ft-AF1WoRqd3aJAHsqc9NY7iL8F\" ); \u8fd4\u56de 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 { \"id\" : \"ft-AF1WoRqd3aJAHsqc9NY7iL8F\" , \"object\" : \"fine-tune\" , \"model\" : \"curie\" , \"created_at\" : 1614807352 , \"events\" : [ { \"object\" : \"fine-tune-event\" , \"created_at\" : 1614807352 , \"level\" : \"info\" , \"message\" : \"Job enqueued. Waiting for jobs ahead to complete. Queue number: 0.\" }, { \"object\" : \"fine-tune-event\" , \"created_at\" : 1614807356 , \"level\" : \"info\" , \"message\" : \"Job started.\" }, { \"object\" : \"fine-tune-event\" , \"created_at\" : 1614807861 , \"level\" : \"info\" , \"message\" : \"Uploaded snapshot: curie:ft-acmeco-2021-03-03-21-44-20.\" }, { \"object\" : \"fine-tune-event\" , \"created_at\" : 1614807864 , \"level\" : \"info\" , \"message\" : \"Uploaded result files: file-QQm6ZpqdNwAaVC3aSz5sWwLT.\" }, { \"object\" : \"fine-tune-event\" , \"created_at\" : 1614807864 , \"level\" : \"info\" , \"message\" : \"Job succeeded.\" } ], \"fine_tuned_model\" : \"curie:ft-acmeco-2021-03-03-21-44-20\" , \"hyperparams\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 0.1 , \"n_epochs\" : 4 , \"prompt_loss_weight\" : 0.1 }, \"organization_id\" : \"org-...\" , \"result_files\" : [ { \"id\" : \"file-QQm6ZpqdNwAaVC3aSz5sWwLT\" , \"object\" : \"file\" , \"bytes\" : 81509 , \"created_at\" : 1614807863 , \"filename\" : \"compiled_results.csv\" , \"purpose\" : \"fine-tune-results\" } ], \"status\" : \"succeeded\" , \"validation_files\" : [], \"training_files\" : [ { \"id\" : \"file-XGinujblHPwGLSztz8cPS8XY\" , \"object\" : \"file\" , \"bytes\" : 1547276 , \"created_at\" : 1610062281 , \"filename\" : \"my-data-train.jsonl\" , \"purpose\" : \"fine-tune-train\" } ], \"updated_at\" : 1614807865 }","title":"\u68c0\u7d22\u5fae\u8c03"},{"location":"api-reference/fine-tunes/#_5","text":"POST : https://api.openai.com/v1/fine-tunes/{fine_tune_id}/cancel \u7acb\u5373\u53d6\u6d88\u5fae\u8c03\u4f5c\u4e1a\u3002 \u8def\u5f84\u53c2\u6570 : \u53c2\u6570 \u5b57\u7b26 \u9700\u8981 \u63cf\u8ff0 fine_tune_id string Required The ID of the fine-tune job to cancel \u793a\u4f8b : 1 2 3 4 5 6 const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . cancelFineTune ( \"ft-AF1WoRqd3aJAHsqc9NY7iL8F\" ); \u8fd4\u56de : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 { \"id\" : \"ft-xhrpBbvVUzYGo8oUO1FY4nI7\" , \"object\" : \"fine-tune\" , \"model\" : \"curie\" , \"created_at\" : 1614807770 , \"events\" : [{ ... }], \"fine_tuned_model\" : null , \"hyperparams\" : { ... }, \"organization_id\" : \"org-...\" , \"result_files\" : [], \"status\" : \"cancelled\" , \"validation_files\" : [], \"training_files\" : [ { \"id\" : \"file-XGinujblHPwGLSztz8cPS8XY\" , \"object\" : \"file\" , \"bytes\" : 1547276 , \"created_at\" : 1610062281 , \"filename\" : \"my-data-train.jsonl\" , \"purpose\" : \"fine-tune-train\" } ], \"updated_at\" : 1614807789 }","title":"\u53d6\u6d88\u5fae\u8c03"},{"location":"api-reference/fine-tunes/#_6","text":"GET : https://api.openai.com/v1/fine-tunes/{fine_tune_id}/events \u83b7\u53d6\u5fae\u8c03\u4f5c\u4e1a\u7684\u7ec6\u7c92\u5ea6\u72b6\u6001\u66f4\u65b0\u3002 \u8def\u5f84\u53c2\u6570 : | | | | | ---------------- | ----------------- | -------- | ----------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | | fine_tune_id | string | Required | | The ID of the fine-tune job to get events for. | | Query parameters | stream | boolean | Optional | Defaults to false | Whether to stream events for the fine-tune job. If set to true, events will be sent as data-only server-sent events as they become available. The stream will terminate with a data: [DONE] message when the job is finished (succeeded, cancelled, or failed). | If set to false, only events generated so far will be returned. \u793a\u4f8b : 1 2 3 4 5 6 const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . listFineTuneEvents ( \"ft-AF1WoRqd3aJAHsqc9NY7iL8F\" ); \u8fd4\u56de 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 { \"object\" : \"list\" , \"data\" : [ { \"object\" : \"fine-tune-event\" , \"created_at\" : 1614807352 , \"level\" : \"info\" , \"message\" : \"Job enqueued. Waiting for jobs ahead to complete. Queue number: 0.\" }, { \"object\" : \"fine-tune-event\" , \"created_at\" : 1614807356 , \"level\" : \"info\" , \"message\" : \"Job started.\" }, { \"object\" : \"fine-tune-event\" , \"created_at\" : 1614807861 , \"level\" : \"info\" , \"message\" : \"Uploaded snapshot: curie:ft-acmeco-2021-03-03-21-44-20.\" }, { \"object\" : \"fine-tune-event\" , \"created_at\" : 1614807864 , \"level\" : \"info\" , \"message\" : \"Uploaded result files: file-QQm6ZpqdNwAaVC3aSz5sWwLT.\" }, { \"object\" : \"fine-tune-event\" , \"created_at\" : 1614807864 , \"level\" : \"info\" , \"message\" : \"Job succeeded.\" } ] }","title":"\u5217\u51fa\u5fae\u8c03\u4e8b\u4ef6"},{"location":"api-reference/fine-tunes/#_7","text":"DELETE : https://api.openai.com/v1/models/{model } \u5220\u9664\u4e00\u4e2a\u7ecf\u8fc7\u5fae\u8c03\u7684\u6a21\u578b\u3002\u60a8\u7684\u7ec4\u7ec7\u4e2d\u5fc5\u987b\u5177\u6709\u201cOwner\u201d\u89d2\u8272\u3002 \u8def\u5f84\u53c2\u6570 : \u6a21\u578b \u5b57\u7b26 \u5fc5\u987b \u63cf\u8ff0 model string Required The model to delete \u793a\u4f8b : 1 2 3 4 5 6 const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . deleteModel ( \"curie:ft-acmeco-2021-03-03-21-44-20\" ); \u8fd4\u56de 1 2 3 4 5 { \"id\" : \"curie:ft-acmeco-2021-03-03-21-44-20\" , \"object\" : \"model\" , \"deleted\" : true }","title":"\u5220\u9664\u5fae\u8c03\u6a21\u578b"},{"location":"api-reference/images/","text":"\u56fe\u50cf \u00b6 \u7ed9\u5b9a\u4e00\u4e2a\u63d0\u793a\u548c/\u6216\u4e00\u4e2a\u8f93\u5165\u56fe\u50cf\uff0c\u6a21\u578b\u5c06\u751f\u6210\u4e00\u4e2a\u65b0\u7684\u56fe\u50cf\u3002 \u76f8\u5173\u6307\u5357: Image generation \u521b\u5efa\u56fe\u50cf Beta \u00b6 POST : https://api.openai.com/v1/images/generations \u521b\u5efa\u7ed9\u5b9a\u63d0\u793a\u7684\u56fe\u50cf\u3002 \u8bf7\u6c42\u4f53 : \u53c2\u6570 \u7c7b\u578b \u5fc5\u987b \u9ed8\u8ba4 \u63cf\u8ff0 prompt string Required \u6240\u9700\u56fe\u50cf\u7684\u6587\u672c\u63cf\u8ff0\u3002\u6700\u5927\u957f\u5ea6\u4e3a 1000 \u4e2a\u5b57\u7b26\u3002 n integer Optional 1 \u8981\u751f\u6210\u7684\u56fe\u50cf\u6570\u91cf\u3002\u5fc5\u987b\u5728 1 \u5230 10 \u4e4b\u95f4\u3002 size string Optional 1024x1024 \u751f\u6210\u56fe\u50cf\u7684\u5927\u5c0f\u3002\u5fc5\u987b\u662f 256x256\u3001512x512 \u6216 1024x1024 \u4e2d\u7684\u4e00\u4e2a\u3002 response_format string Optional url \u6240\u751f\u6210\u56fe\u50cf\u7684\u8fd4\u56de\u683c\u5f0f\u3002\u5fc5\u987b\u662f url \u6216 b64_json \u4e4b\u4e00\u3002 user string Optional \u4ee3\u8868\u7ec8\u7aef\u7528\u6237\u7684\u552f\u4e00\u6807\u8bc6\u7b26\uff0c\u53ef\u4ee5\u5e2e\u52a9 OpenAI \u76d1\u89c6\u548c\u68c0\u6d4b\u6ee5\u7528\u3002\u5b66\u4e60\u66f4\u591a\u7684\u77e5\u8bc6\u3002 \u793a\u4f8b : node.js node.js 1 2 3 4 5 6 7 8 9 10 const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . createImage ({ prompt : \"A cute baby sea otter\" , n : 2 , size : \"1024x1024\" , }); \u53c2\u6570 : 1 2 3 4 5 { \"prompt\" : \"A cute baby sea otter\" , \"n\" : 2 , \"size\" : \"1024x1024\" } \u8fd4\u56de : 1 2 3 4 5 6 7 8 9 10 11 { \"created\" : 1589478378 , \"data\" : [ { \"url\" : \"https://...\" }, { \"url\" : \"https://...\" } ] } \u521b\u5efa\u56fe\u50cf\u7f16\u8f91 Beta \u00b6 POST : https://api.openai.com/v1/images/edits \u7ed9\u5b9a\u539f\u59cb\u56fe\u50cf\u548c\u63d0\u793a\uff0c\u521b\u5efa\u7f16\u8f91\u6216\u6269\u5c55\u7684\u56fe\u50cf\u3002 \u8bf7\u6c42\u4f53 : \u53c2\u6570 \u7c7b\u578b \u5fc5\u987b \u9ed8\u8ba4\u503c \u63cf\u8ff0 image string Required \u8981\u7f16\u8f91\u7684\u56fe\u50cf\u3002\u5fc5\u987b\u662f\u6709\u6548\u7684 PNG \u6587\u4ef6\uff0c\u5c0f\u4e8e 4MB \u5e76\u4e14\u662f\u6b63\u65b9\u5f62\u3002\u5982\u679c\u6ca1\u6709\u63d0\u4f9b\u63a9\u6a21\uff0c\u5219\u56fe\u50cf\u5fc5\u987b\u5177\u6709\u900f\u660e\u5ea6\uff0c\u900f\u660e\u90e8\u5206\u5c06\u7528\u4f5c\u63a9\u6a21\u3002 mask string Optional \u4e00\u4e2a\u989d\u5916\u7684\u56fe\u50cf\uff0c\u5176\u5b8c\u5168\u900f\u660e\u7684\u533a\u57df\uff08\u4f8b\u5982 alpha \u4e3a\u96f6\u7684\u533a\u57df\uff09\u6307\u793a\u5e94\u8be5\u5728\u54ea\u91cc\u7f16\u8f91\u56fe\u50cf\u3002\u5fc5\u987b\u662f\u6709\u6548\u7684 PNG \u6587\u4ef6\uff0c\u5c0f\u4e8e 4MB\uff0c\u5e76\u4e14\u5177\u6709\u4e0e\u56fe\u50cf\u76f8\u540c\u7684\u5c3a\u5bf8\u3002 prompt string Required \u6240\u9700\u56fe\u50cf\u7684\u6587\u672c\u63cf\u8ff0\u3002\u6700\u5927\u957f\u5ea6\u4e3a 1000 \u4e2a\u5b57\u7b26\u3002 n integer Optional 1 \u8981\u751f\u6210\u7684\u56fe\u50cf\u6570\u3002\u5fc5\u987b\u4ecb\u4e8e 1 \u548c 10 \u4e4b\u95f4\u3002 size string Optional 1024x1024 \u751f\u6210\u56fe\u50cf\u7684\u5927\u5c0f\u3002\u5fc5\u987b\u662f 256x256\u3001512x512 \u6216 1024x1024 \u4e2d\u7684\u4e00\u4e2a\u3002 response_format string Optional url \u751f\u6210\u7684\u56fe\u50cf\u8fd4\u56de\u7684\u683c\u5f0f\u3002\u5fc5\u987b\u662f url \u6216 b64_json \u4e2d\u7684\u4e00\u4e2a\u3002 user string Optional \u4ee3\u8868\u60a8\u7684\u6700\u7ec8\u7528\u6237\u7684\u552f\u4e00\u6807\u8bc6\u7b26\uff0c\u53ef\u5e2e\u52a9 OpenAI \u76d1\u89c6\u548c\u68c0\u6d4b\u6ee5\u7528\u3002\u4e86\u89e3\u66f4\u591a\u4fe1\u606f\u3002 \u793a\u4f8b : node.js 1 2 3 4 5 6 7 8 9 10 11 12 const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . createImageEdit ( fs . createReadStream ( \"otter.png\" ), fs . createReadStream ( \"mask.png\" ), \"A cute baby sea otter wearing a beret\" , 2 , \"1024x1024\" ); \u8fd4\u56de : 1 2 3 4 5 6 7 8 9 10 11 { \"created\" : 1589478378 , \"data\" : [ { \"url\" : \"https://...\" }, { \"url\" : \"https://...\" } ] } \u521b\u5efa\u56fe\u50cf\u53d8\u4f53 Beta \u00b6 POST : https://api.openai.com/v1/images/variations \u521b\u5efa\u7ed9\u5b9a\u56fe\u50cf\u7684\u53d8\u4f53\u3002 \u8bf7\u6c42\u4f53 : \u53c2\u6570 \u7c7b\u578b \u5fc5\u9009\u9879 \u9ed8\u8ba4\u503c \u63cf\u8ff0 image string Required \u7528\u4f5c\u53d8\u4f53\u57fa\u7840\u7684\u56fe\u50cf\u3002\u5fc5\u987b\u662f\u6709\u6548\u7684 PNG \u6587\u4ef6\uff0c\u5c0f\u4e8e 4MB \u5e76\u4e14\u662f\u6b63\u65b9\u5f62\u3002 n integer Optional 1 \u8981\u751f\u6210\u7684\u56fe\u50cf\u6570\u91cf\u3002\u5fc5\u987b\u4ecb\u4e8e 1 \u548c 10 \u4e4b\u95f4\u3002 size string Optional 1024x1024 \u751f\u6210\u7684\u56fe\u50cf\u5927\u5c0f\u3002\u5fc5\u987b\u662f 256x256\u3001512x512 \u6216 1024x1024 \u4e4b\u4e00\u3002 response_format string Optional url \u8fd4\u56de\u751f\u6210\u7684\u56fe\u50cf\u7684\u683c\u5f0f\u3002\u5fc5\u987b\u662f url \u6216 b64_json \u4e4b\u4e00\u3002 user string Optional \u8868\u793a\u7ec8\u7aef\u7528\u6237\u7684\u552f\u4e00\u6807\u8bc6\u7b26\uff0c\u53ef\u5e2e\u52a9 OpenAI \u76d1\u6d4b\u548c\u68c0\u6d4b\u6ee5\u7528\u3002\u4e86\u89e3\u66f4\u591a\u4fe1\u606f\u3002 \u793a\u4f8b : node.js 1 2 3 4 5 6 const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . createImageVariation ( fs . createReadStream ( \"otter.png\" ), 2 , \"1024x1024\" ); \u8fd4\u56de : 1 2 3 4 5 6 7 8 9 10 11 { \"created\" : 1589478378 , \"data\" : [ { \"url\" : \"https://...\" }, { \"url\" : \"https://...\" } ] }","title":"\u56fe\u50cf"},{"location":"api-reference/images/#_1","text":"\u7ed9\u5b9a\u4e00\u4e2a\u63d0\u793a\u548c/\u6216\u4e00\u4e2a\u8f93\u5165\u56fe\u50cf\uff0c\u6a21\u578b\u5c06\u751f\u6210\u4e00\u4e2a\u65b0\u7684\u56fe\u50cf\u3002 \u76f8\u5173\u6307\u5357: Image generation","title":"\u56fe\u50cf"},{"location":"api-reference/images/#beta","text":"POST : https://api.openai.com/v1/images/generations \u521b\u5efa\u7ed9\u5b9a\u63d0\u793a\u7684\u56fe\u50cf\u3002 \u8bf7\u6c42\u4f53 : \u53c2\u6570 \u7c7b\u578b \u5fc5\u987b \u9ed8\u8ba4 \u63cf\u8ff0 prompt string Required \u6240\u9700\u56fe\u50cf\u7684\u6587\u672c\u63cf\u8ff0\u3002\u6700\u5927\u957f\u5ea6\u4e3a 1000 \u4e2a\u5b57\u7b26\u3002 n integer Optional 1 \u8981\u751f\u6210\u7684\u56fe\u50cf\u6570\u91cf\u3002\u5fc5\u987b\u5728 1 \u5230 10 \u4e4b\u95f4\u3002 size string Optional 1024x1024 \u751f\u6210\u56fe\u50cf\u7684\u5927\u5c0f\u3002\u5fc5\u987b\u662f 256x256\u3001512x512 \u6216 1024x1024 \u4e2d\u7684\u4e00\u4e2a\u3002 response_format string Optional url \u6240\u751f\u6210\u56fe\u50cf\u7684\u8fd4\u56de\u683c\u5f0f\u3002\u5fc5\u987b\u662f url \u6216 b64_json \u4e4b\u4e00\u3002 user string Optional \u4ee3\u8868\u7ec8\u7aef\u7528\u6237\u7684\u552f\u4e00\u6807\u8bc6\u7b26\uff0c\u53ef\u4ee5\u5e2e\u52a9 OpenAI \u76d1\u89c6\u548c\u68c0\u6d4b\u6ee5\u7528\u3002\u5b66\u4e60\u66f4\u591a\u7684\u77e5\u8bc6\u3002 \u793a\u4f8b : node.js node.js 1 2 3 4 5 6 7 8 9 10 const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . createImage ({ prompt : \"A cute baby sea otter\" , n : 2 , size : \"1024x1024\" , }); \u53c2\u6570 : 1 2 3 4 5 { \"prompt\" : \"A cute baby sea otter\" , \"n\" : 2 , \"size\" : \"1024x1024\" } \u8fd4\u56de : 1 2 3 4 5 6 7 8 9 10 11 { \"created\" : 1589478378 , \"data\" : [ { \"url\" : \"https://...\" }, { \"url\" : \"https://...\" } ] }","title":"\u521b\u5efa\u56fe\u50cf Beta"},{"location":"api-reference/images/#beta_1","text":"POST : https://api.openai.com/v1/images/edits \u7ed9\u5b9a\u539f\u59cb\u56fe\u50cf\u548c\u63d0\u793a\uff0c\u521b\u5efa\u7f16\u8f91\u6216\u6269\u5c55\u7684\u56fe\u50cf\u3002 \u8bf7\u6c42\u4f53 : \u53c2\u6570 \u7c7b\u578b \u5fc5\u987b \u9ed8\u8ba4\u503c \u63cf\u8ff0 image string Required \u8981\u7f16\u8f91\u7684\u56fe\u50cf\u3002\u5fc5\u987b\u662f\u6709\u6548\u7684 PNG \u6587\u4ef6\uff0c\u5c0f\u4e8e 4MB \u5e76\u4e14\u662f\u6b63\u65b9\u5f62\u3002\u5982\u679c\u6ca1\u6709\u63d0\u4f9b\u63a9\u6a21\uff0c\u5219\u56fe\u50cf\u5fc5\u987b\u5177\u6709\u900f\u660e\u5ea6\uff0c\u900f\u660e\u90e8\u5206\u5c06\u7528\u4f5c\u63a9\u6a21\u3002 mask string Optional \u4e00\u4e2a\u989d\u5916\u7684\u56fe\u50cf\uff0c\u5176\u5b8c\u5168\u900f\u660e\u7684\u533a\u57df\uff08\u4f8b\u5982 alpha \u4e3a\u96f6\u7684\u533a\u57df\uff09\u6307\u793a\u5e94\u8be5\u5728\u54ea\u91cc\u7f16\u8f91\u56fe\u50cf\u3002\u5fc5\u987b\u662f\u6709\u6548\u7684 PNG \u6587\u4ef6\uff0c\u5c0f\u4e8e 4MB\uff0c\u5e76\u4e14\u5177\u6709\u4e0e\u56fe\u50cf\u76f8\u540c\u7684\u5c3a\u5bf8\u3002 prompt string Required \u6240\u9700\u56fe\u50cf\u7684\u6587\u672c\u63cf\u8ff0\u3002\u6700\u5927\u957f\u5ea6\u4e3a 1000 \u4e2a\u5b57\u7b26\u3002 n integer Optional 1 \u8981\u751f\u6210\u7684\u56fe\u50cf\u6570\u3002\u5fc5\u987b\u4ecb\u4e8e 1 \u548c 10 \u4e4b\u95f4\u3002 size string Optional 1024x1024 \u751f\u6210\u56fe\u50cf\u7684\u5927\u5c0f\u3002\u5fc5\u987b\u662f 256x256\u3001512x512 \u6216 1024x1024 \u4e2d\u7684\u4e00\u4e2a\u3002 response_format string Optional url \u751f\u6210\u7684\u56fe\u50cf\u8fd4\u56de\u7684\u683c\u5f0f\u3002\u5fc5\u987b\u662f url \u6216 b64_json \u4e2d\u7684\u4e00\u4e2a\u3002 user string Optional \u4ee3\u8868\u60a8\u7684\u6700\u7ec8\u7528\u6237\u7684\u552f\u4e00\u6807\u8bc6\u7b26\uff0c\u53ef\u5e2e\u52a9 OpenAI \u76d1\u89c6\u548c\u68c0\u6d4b\u6ee5\u7528\u3002\u4e86\u89e3\u66f4\u591a\u4fe1\u606f\u3002 \u793a\u4f8b : node.js 1 2 3 4 5 6 7 8 9 10 11 12 const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . createImageEdit ( fs . createReadStream ( \"otter.png\" ), fs . createReadStream ( \"mask.png\" ), \"A cute baby sea otter wearing a beret\" , 2 , \"1024x1024\" ); \u8fd4\u56de : 1 2 3 4 5 6 7 8 9 10 11 { \"created\" : 1589478378 , \"data\" : [ { \"url\" : \"https://...\" }, { \"url\" : \"https://...\" } ] }","title":"\u521b\u5efa\u56fe\u50cf\u7f16\u8f91 Beta"},{"location":"api-reference/images/#beta_2","text":"POST : https://api.openai.com/v1/images/variations \u521b\u5efa\u7ed9\u5b9a\u56fe\u50cf\u7684\u53d8\u4f53\u3002 \u8bf7\u6c42\u4f53 : \u53c2\u6570 \u7c7b\u578b \u5fc5\u9009\u9879 \u9ed8\u8ba4\u503c \u63cf\u8ff0 image string Required \u7528\u4f5c\u53d8\u4f53\u57fa\u7840\u7684\u56fe\u50cf\u3002\u5fc5\u987b\u662f\u6709\u6548\u7684 PNG \u6587\u4ef6\uff0c\u5c0f\u4e8e 4MB \u5e76\u4e14\u662f\u6b63\u65b9\u5f62\u3002 n integer Optional 1 \u8981\u751f\u6210\u7684\u56fe\u50cf\u6570\u91cf\u3002\u5fc5\u987b\u4ecb\u4e8e 1 \u548c 10 \u4e4b\u95f4\u3002 size string Optional 1024x1024 \u751f\u6210\u7684\u56fe\u50cf\u5927\u5c0f\u3002\u5fc5\u987b\u662f 256x256\u3001512x512 \u6216 1024x1024 \u4e4b\u4e00\u3002 response_format string Optional url \u8fd4\u56de\u751f\u6210\u7684\u56fe\u50cf\u7684\u683c\u5f0f\u3002\u5fc5\u987b\u662f url \u6216 b64_json \u4e4b\u4e00\u3002 user string Optional \u8868\u793a\u7ec8\u7aef\u7528\u6237\u7684\u552f\u4e00\u6807\u8bc6\u7b26\uff0c\u53ef\u5e2e\u52a9 OpenAI \u76d1\u6d4b\u548c\u68c0\u6d4b\u6ee5\u7528\u3002\u4e86\u89e3\u66f4\u591a\u4fe1\u606f\u3002 \u793a\u4f8b : node.js 1 2 3 4 5 6 const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . createImageVariation ( fs . createReadStream ( \"otter.png\" ), 2 , \"1024x1024\" ); \u8fd4\u56de : 1 2 3 4 5 6 7 8 9 10 11 { \"created\" : 1589478378 , \"data\" : [ { \"url\" : \"https://...\" }, { \"url\" : \"https://...\" } ] }","title":"\u521b\u5efa\u56fe\u50cf\u53d8\u4f53 Beta"},{"location":"api-reference/introduction/","text":"\u4ecb\u7ecd \u00b6 You can interact with the API through HTTP requests from any language, via our official Python bindings, our official Node.js library, or a community-maintained library. To install the official Python bindings, run the following command: 1 pip install openai To install the official Node.js library, run the following command in your Node.js project directory: 1 npm install openai","title":"\u4ecb\u7ecd"},{"location":"api-reference/introduction/#_1","text":"You can interact with the API through HTTP requests from any language, via our official Python bindings, our official Node.js library, or a community-maintained library. To install the official Python bindings, run the following command: 1 pip install openai To install the official Node.js library, run the following command in your Node.js project directory: 1 npm install openai","title":"\u4ecb\u7ecd"},{"location":"api-reference/making-requests/","text":"\u53d1\u51fa\u8bf7\u6c42 \u00b6 You can paste the command below into your terminal to run your first API request. Make sure to replace YOUR_API_KEY with your secret API key. 1 2 3 4 curl https://api.openai.com/v1/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer YOUR_API_KEY\" \\ -d '{\"model\": \"text-davinci-003\", \"prompt\": \"Say this is a test\", \"temperature\": 0, \"max_tokens\": 7}' This request queries the Davinci model to complete the text starting with a prompt of \"Say this is a test\". The max_tokens parameter sets an upper bound on how many tokens the API will return. You should get a response back that resembles the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"id\" : \"cmpl-GERzeJQ4lvqPk8SkZu4XMIuR\" , \"object\" : \"text_completion\" , \"created\" : 1586839808 , \"model\" : \"text-davinci:003\" , \"choices\" : [ { \"text\" : \"\\n\\nThis is indeed a test\" , \"index\" : 0 , \"logprobs\" : null , \"finish_reason\" : \"length\" } ], \"usage\" : { \"prompt_tokens\" : 5 , \"completion_tokens\" : 7 , \"total_tokens\" : 12 } } Now you've generated your first completion. If you concatenate the prompt and the completion text (which the API will do for you if you set the echo parameter to true), the resulting text is \"Say this is a test. This is indeed a test.\" You can also set the stream parameter to true for the API to stream back text (as data-only server-sent events).","title":"\u53d1\u51fa\u8bf7\u6c42"},{"location":"api-reference/making-requests/#_1","text":"You can paste the command below into your terminal to run your first API request. Make sure to replace YOUR_API_KEY with your secret API key. 1 2 3 4 curl https://api.openai.com/v1/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer YOUR_API_KEY\" \\ -d '{\"model\": \"text-davinci-003\", \"prompt\": \"Say this is a test\", \"temperature\": 0, \"max_tokens\": 7}' This request queries the Davinci model to complete the text starting with a prompt of \"Say this is a test\". The max_tokens parameter sets an upper bound on how many tokens the API will return. You should get a response back that resembles the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"id\" : \"cmpl-GERzeJQ4lvqPk8SkZu4XMIuR\" , \"object\" : \"text_completion\" , \"created\" : 1586839808 , \"model\" : \"text-davinci:003\" , \"choices\" : [ { \"text\" : \"\\n\\nThis is indeed a test\" , \"index\" : 0 , \"logprobs\" : null , \"finish_reason\" : \"length\" } ], \"usage\" : { \"prompt_tokens\" : 5 , \"completion_tokens\" : 7 , \"total_tokens\" : 12 } } Now you've generated your first completion. If you concatenate the prompt and the completion text (which the API will do for you if you set the echo parameter to true), the resulting text is \"Say this is a test. This is indeed a test.\" You can also set the stream parameter to true for the API to stream back text (as data-only server-sent events).","title":"\u53d1\u51fa\u8bf7\u6c42"},{"location":"api-reference/models/","text":"\u6a21\u578b \u00b6 \u5217\u51fa\u5e76\u63cf\u8ff0 API \u4e2d\u53ef\u7528\u7684\u5404\u79cd\u6a21\u578b\u3002 \u60a8\u53ef\u4ee5\u53c2\u8003 Models \u6587\u6863\u6765\u4e86\u89e3\u53ef\u7528\u7684\u6a21\u578b\u4ee5\u53ca\u5b83\u4eec\u4e4b\u95f4\u7684\u5dee\u5f02\u3002 \u5217\u51fa\u6a21\u5f0f \u00b6 GET : https://api.openai.com/v1/models \u5217\u51fa\u5f53\u524d\u53ef\u7528\u7684\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u5173\u4e8e\u6bcf\u4e2a\u6a21\u578b\u7684\u57fa\u672c\u4fe1\u606f\uff0c\u5982\u6240\u6709\u8005\u548c\u53ef\u7528\u6027\u3002 \u793a\u4f8b : node.js 1 2 3 4 5 6 const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . listModels (); \u8fd4\u56de 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 { \"data\" : [ { \"id\" : \"model-id-0\" , \"object\" : \"model\" , \"owned_by\" : \"organization-owner\" , \"permission\" : [ ... ] }, { \"id\" : \"model-id-1\" , \"object\" : \"model\" , \"owned_by\" : \"organization-owner\" , \"permission\" : [ ... ] }, { \"id\" : \"model-id-2\" , \"object\" : \"model\" , \"owned_by\" : \"openai\" , \"permission\" : [ ... ] } ], \"object\" : \"list\" } \u68c0\u7d22\u6a21\u578b \u00b6 GET : https://api.openai.com/v1/models/{model } Retrieves a model instance, providing basic information about the model such as the owner and permissioning. \u8def\u5f84\u53c2\u6570 : model string Required : The ID of the model to use for this request \u793a\u4f8b : text-davinci-003,node.js 1 2 3 4 5 6 const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . retrieveModel ( \"text-davinci-003\" ); \u8fd4\u56de : text-davinci-003 1 2 3 4 5 6 { \"id\" : \"text-davinci-003\" , \"object\" : \"model\" , \"owned_by\" : \"openai\" , \"permission\" : [ ... ] }","title":"\u6a21\u578b"},{"location":"api-reference/models/#_1","text":"\u5217\u51fa\u5e76\u63cf\u8ff0 API \u4e2d\u53ef\u7528\u7684\u5404\u79cd\u6a21\u578b\u3002 \u60a8\u53ef\u4ee5\u53c2\u8003 Models \u6587\u6863\u6765\u4e86\u89e3\u53ef\u7528\u7684\u6a21\u578b\u4ee5\u53ca\u5b83\u4eec\u4e4b\u95f4\u7684\u5dee\u5f02\u3002","title":"\u6a21\u578b"},{"location":"api-reference/models/#_2","text":"GET : https://api.openai.com/v1/models \u5217\u51fa\u5f53\u524d\u53ef\u7528\u7684\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u5173\u4e8e\u6bcf\u4e2a\u6a21\u578b\u7684\u57fa\u672c\u4fe1\u606f\uff0c\u5982\u6240\u6709\u8005\u548c\u53ef\u7528\u6027\u3002 \u793a\u4f8b : node.js 1 2 3 4 5 6 const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . listModels (); \u8fd4\u56de 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 { \"data\" : [ { \"id\" : \"model-id-0\" , \"object\" : \"model\" , \"owned_by\" : \"organization-owner\" , \"permission\" : [ ... ] }, { \"id\" : \"model-id-1\" , \"object\" : \"model\" , \"owned_by\" : \"organization-owner\" , \"permission\" : [ ... ] }, { \"id\" : \"model-id-2\" , \"object\" : \"model\" , \"owned_by\" : \"openai\" , \"permission\" : [ ... ] } ], \"object\" : \"list\" }","title":"\u5217\u51fa\u6a21\u5f0f"},{"location":"api-reference/models/#_3","text":"GET : https://api.openai.com/v1/models/{model } Retrieves a model instance, providing basic information about the model such as the owner and permissioning. \u8def\u5f84\u53c2\u6570 : model string Required : The ID of the model to use for this request \u793a\u4f8b : text-davinci-003,node.js 1 2 3 4 5 6 const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . retrieveModel ( \"text-davinci-003\" ); \u8fd4\u56de : text-davinci-003 1 2 3 4 5 6 { \"id\" : \"text-davinci-003\" , \"object\" : \"model\" , \"owned_by\" : \"openai\" , \"permission\" : [ ... ] }","title":"\u68c0\u7d22\u6a21\u578b"},{"location":"api-reference/moderations/","text":"\u9002\u5ea6 \u00b6 \u7ed9\u5b9a\u4e00\u4e2a\u8f93\u5165\u6587\u672c\uff0c\u5982\u679c\u6a21\u578b\u5c06\u5176\u5f52\u7c7b\u4e3a\u8fdd\u53cd OpenAI \u7684\u5185\u5bb9\u7b56\u7565\uff0c\u5219\u8f93\u51fa\u8be5\u6587\u672c\u3002 \u76f8\u5173\u6307\u5357: Moderations \u521b\u5efa\u9002\u5ea6 \u00b6 POST : https://api.openai.com/v1/moderations Classifies if text violates OpenAI's Content Policy \u8bf7\u6c42\u4f53 : input string or array Required The input text to classify model string Optional Defaults to text-moderation-latest Two content moderations models are available: text-moderation-stable and text-moderation-latest. The default is text-moderation-latest which will be automatically upgraded over time. This ensures you are always using our most accurate model. If you use text-moderation-stable, we will provide advanced notice before updating the model. Accuracy of text-moderation-stable may be slightly lower than for text-moderation-latest. \u793a\u4f8b : node.js node.js 1 2 3 4 5 6 7 8 const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . createModeration ({ input : \"I want to kill them.\" , }); \u53c2\u6570 : 1 2 3 { \"input\" : \"I want to kill them.\" } \u8fd4\u56de : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 { \"id\" : \"modr-5MWoLO\" , \"model\" : \"text-moderation-001\" , \"results\" : [ { \"categories\" : { \"hate\" : false , \"hate/threatening\" : true , \"self-harm\" : false , \"sexual\" : false , \"sexual/minors\" : false , \"violence\" : true , \"violence/graphic\" : false }, \"category_scores\" : { \"hate\" : 0.22714105248451233 , \"hate/threatening\" : 0.4132447838783264 , \"self-harm\" : 0.005232391878962517 , \"sexual\" : 0.01407341007143259 , \"sexual/minors\" : 0.0038522258400917053 , \"violence\" : 0.9223177433013916 , \"violence/graphic\" : 0.036865197122097015 }, \"flagged\" : true } ] }","title":"\u9002\u5ea6"},{"location":"api-reference/moderations/#_1","text":"\u7ed9\u5b9a\u4e00\u4e2a\u8f93\u5165\u6587\u672c\uff0c\u5982\u679c\u6a21\u578b\u5c06\u5176\u5f52\u7c7b\u4e3a\u8fdd\u53cd OpenAI \u7684\u5185\u5bb9\u7b56\u7565\uff0c\u5219\u8f93\u51fa\u8be5\u6587\u672c\u3002 \u76f8\u5173\u6307\u5357: Moderations","title":"\u9002\u5ea6"},{"location":"api-reference/moderations/#_2","text":"POST : https://api.openai.com/v1/moderations Classifies if text violates OpenAI's Content Policy \u8bf7\u6c42\u4f53 : input string or array Required The input text to classify model string Optional Defaults to text-moderation-latest Two content moderations models are available: text-moderation-stable and text-moderation-latest. The default is text-moderation-latest which will be automatically upgraded over time. This ensures you are always using our most accurate model. If you use text-moderation-stable, we will provide advanced notice before updating the model. Accuracy of text-moderation-stable may be slightly lower than for text-moderation-latest. \u793a\u4f8b : node.js node.js 1 2 3 4 5 6 7 8 const { Configuration , OpenAIApi } = require ( \"openai\" ); const configuration = new Configuration ({ apiKey : process . env . OPENAI_API_KEY , }); const openai = new OpenAIApi ( configuration ); const response = await openai . createModeration ({ input : \"I want to kill them.\" , }); \u53c2\u6570 : 1 2 3 { \"input\" : \"I want to kill them.\" } \u8fd4\u56de : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 { \"id\" : \"modr-5MWoLO\" , \"model\" : \"text-moderation-001\" , \"results\" : [ { \"categories\" : { \"hate\" : false , \"hate/threatening\" : true , \"self-harm\" : false , \"sexual\" : false , \"sexual/minors\" : false , \"violence\" : true , \"violence/graphic\" : false }, \"category_scores\" : { \"hate\" : 0.22714105248451233 , \"hate/threatening\" : 0.4132447838783264 , \"self-harm\" : 0.005232391878962517 , \"sexual\" : 0.01407341007143259 , \"sexual/minors\" : 0.0038522258400917053 , \"violence\" : 0.9223177433013916 , \"violence/graphic\" : 0.036865197122097015 }, \"flagged\" : true } ] }","title":"\u521b\u5efa\u9002\u5ea6"},{"location":"api-reference/parameter-details/","text":"\u53c2\u6570\u7ec6\u8282 \u00b6 \u9891\u7387\u548c\u5b58\u5728\u60e9\u7f5a \u00b6 \u5728 Completions API \u4e2d\u53d1\u73b0\u7684\u9891\u7387\u548c\u5b58\u5728\u60e9\u7f5a\u53ef\u7528\u4e8e\u51cf\u5c11\u91c7\u6837\u91cd\u590d\u4ee4\u724c\u5e8f\u5217\u7684\u53ef\u80fd\u6027\u3002 \u5b83\u4eec\u901a\u8fc7\u76f4\u63a5\u4fee\u6539 logits(\u975e\u6807\u51c6\u5316\u7684\u5bf9\u6570\u6982\u7387)\u548c\u6dfb\u52a0\u8d21\u732e\u6765\u5de5\u4f5c\u3002 1 mu[j] -> mu[j] - c[j] _ alpha_frequency - float(c[j] > 0) _ alpha_presence \u5b83\u4eec\u662f: mu[j] \u662f\u7b2c j \u4e2a\u7b26\u53f7\u7684\u5bf9\u6570 c[j] \u8868\u793a\u8be5\u4ee4\u724c\u5728\u5f53\u524d\u4f4d\u7f6e\u4e4b\u524d\u91c7\u6837\u7684\u9891\u7387 float(c[j] > 0) is 1 if c[j] > 0 and 0 otherwise alpha_frequency \u662f\u9891\u7387\u60e9\u7f5a\u7cfb\u6570 alpha_presence \u662f\u5b58\u5728\u60e9\u7f5a\u7cfb\u6570 \u6b63\u5982\u6211\u4eec\u6240\u770b\u5230\u7684\uff0c\u5b58\u5728\u60e9\u7f5a\u662f\u4e00\u6b21\u6027\u7684\u9644\u52a0\u8d21\u732e\uff0c\u9002\u7528\u4e8e\u81f3\u5c11\u91c7\u6837\u4e00\u6b21\u7684\u6240\u6709\u4ee4\u724c\uff0c\u9891\u7387\u60e9\u7f5a\u662f\u4e0e\u7279\u5b9a\u4ee4\u724c\u5df2\u7ecf\u91c7\u6837\u7684\u9891\u7387\u6210\u6b63\u6bd4\u7684\u8d21\u732e\u3002 \u5982\u679c\u76ee\u6807\u53ea\u662f\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u51cf\u5c11\u91cd\u590d\u91c7\u6837\uff0c\u60e9\u7f5a\u7cfb\u6570\u7684\u5408\u7406\u503c\u5927\u7ea6\u5728 0.1 \u5230 1 \u4e4b\u95f4\u3002 \u5982\u679c\u76ee\u6807\u662f\u5f3a\u70c8\u6291\u5236\u91cd\u590d\uff0c\u90a3\u4e48\u53ef\u4ee5\u5c06\u7cfb\u6570\u589e\u52a0\u5230 2\uff0c\u4f46\u8fd9\u53ef\u80fd\u4f1a\u663e\u8457\u964d\u4f4e\u6837\u672c\u7684\u8d28\u91cf\u3002\u8d1f\u503c\u53ef\u4ee5\u7528\u6765\u589e\u52a0\u91cd\u590d\u7684\u53ef\u80fd\u6027\u3002","title":"\u53c2\u6570\u7ec6\u8282"},{"location":"api-reference/parameter-details/#_1","text":"","title":"\u53c2\u6570\u7ec6\u8282"},{"location":"api-reference/parameter-details/#_2","text":"\u5728 Completions API \u4e2d\u53d1\u73b0\u7684\u9891\u7387\u548c\u5b58\u5728\u60e9\u7f5a\u53ef\u7528\u4e8e\u51cf\u5c11\u91c7\u6837\u91cd\u590d\u4ee4\u724c\u5e8f\u5217\u7684\u53ef\u80fd\u6027\u3002 \u5b83\u4eec\u901a\u8fc7\u76f4\u63a5\u4fee\u6539 logits(\u975e\u6807\u51c6\u5316\u7684\u5bf9\u6570\u6982\u7387)\u548c\u6dfb\u52a0\u8d21\u732e\u6765\u5de5\u4f5c\u3002 1 mu[j] -> mu[j] - c[j] _ alpha_frequency - float(c[j] > 0) _ alpha_presence \u5b83\u4eec\u662f: mu[j] \u662f\u7b2c j \u4e2a\u7b26\u53f7\u7684\u5bf9\u6570 c[j] \u8868\u793a\u8be5\u4ee4\u724c\u5728\u5f53\u524d\u4f4d\u7f6e\u4e4b\u524d\u91c7\u6837\u7684\u9891\u7387 float(c[j] > 0) is 1 if c[j] > 0 and 0 otherwise alpha_frequency \u662f\u9891\u7387\u60e9\u7f5a\u7cfb\u6570 alpha_presence \u662f\u5b58\u5728\u60e9\u7f5a\u7cfb\u6570 \u6b63\u5982\u6211\u4eec\u6240\u770b\u5230\u7684\uff0c\u5b58\u5728\u60e9\u7f5a\u662f\u4e00\u6b21\u6027\u7684\u9644\u52a0\u8d21\u732e\uff0c\u9002\u7528\u4e8e\u81f3\u5c11\u91c7\u6837\u4e00\u6b21\u7684\u6240\u6709\u4ee4\u724c\uff0c\u9891\u7387\u60e9\u7f5a\u662f\u4e0e\u7279\u5b9a\u4ee4\u724c\u5df2\u7ecf\u91c7\u6837\u7684\u9891\u7387\u6210\u6b63\u6bd4\u7684\u8d21\u732e\u3002 \u5982\u679c\u76ee\u6807\u53ea\u662f\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u51cf\u5c11\u91cd\u590d\u91c7\u6837\uff0c\u60e9\u7f5a\u7cfb\u6570\u7684\u5408\u7406\u503c\u5927\u7ea6\u5728 0.1 \u5230 1 \u4e4b\u95f4\u3002 \u5982\u679c\u76ee\u6807\u662f\u5f3a\u70c8\u6291\u5236\u91cd\u590d\uff0c\u90a3\u4e48\u53ef\u4ee5\u5c06\u7cfb\u6570\u589e\u52a0\u5230 2\uff0c\u4f46\u8fd9\u53ef\u80fd\u4f1a\u663e\u8457\u964d\u4f4e\u6837\u672c\u7684\u8d28\u91cf\u3002\u8d1f\u503c\u53ef\u4ee5\u7528\u6765\u589e\u52a0\u91cd\u590d\u7684\u53ef\u80fd\u6027\u3002","title":"\u9891\u7387\u548c\u5b58\u5728\u60e9\u7f5a"},{"location":"guides/","text":"\u5f15\u5bfc \u00b6 text-completion code-completion chat-completion image-generation fine-tuning embeddings speech-to-text moderation rate-limits error-codes safety-best-practices production-best-practices","title":"\u5f15\u5bfc"},{"location":"guides/#_1","text":"text-completion code-completion chat-completion image-generation fine-tuning embeddings speech-to-text moderation rate-limits error-codes safety-best-practices production-best-practices","title":"\u5f15\u5bfc"},{"location":"guides/chat/","text":"\u804a\u5929\u8865\u5168 Beta \u00b6 ChatGPT \u7531 OpenAI \u6700\u5148\u8fdb\u7684\u8bed\u8a00\u6a21\u578b gpt-3.5-turbo \u652f\u6301\u3002 \u4f7f\u7528 OpenAI API\uff0c\u4f60\u53ef\u4ee5\u7528 gpt-3.5-turbo \u6784\u5efa\u4f60\u81ea\u5df1\u7684\u5e94\u7528\u7a0b\u5e8f\u6765\u505a\u4ee5\u4e0b\u4e8b\u60c5: \u8d77\u8349\u4e00\u5c01\u7535\u5b50\u90ae\u4ef6\u6216\u5176\u4ed6\u6587\u5b57 \u7f16\u5199 Python \u4ee3\u7801 \u56de\u7b54\u5173\u4e8e\u4e00\u7ec4\u6587\u6863\u7684\u95ee\u9898 \u521b\u5efa\u4f1a\u8bdd\u4ee3\u7406 \u7ed9\u4f60\u7684\u8f6f\u4ef6\u4e00\u4e2a\u81ea\u7136\u8bed\u8a00\u754c\u9762 \u8f85\u5bfc\u5404\u79cd\u5b66\u79d1 \u7ffb\u8bd1\u8bed\u8a00 \u6a21\u62df\u89d2\u8272\u7684\u89c6\u9891\u6e38\u620f\u548c\u66f4\u591a \u672c\u6307\u5357\u89e3\u91ca\u4e86\u5982\u4f55\u5bf9\u57fa\u4e8e\u804a\u5929\u7684\u8bed\u8a00\u6a21\u578b\u8fdb\u884c API \u8c03\u7528\uff0c\u5e76\u5206\u4eab\u4e86\u83b7\u5f97\u826f\u597d\u7ed3\u679c\u7684\u6280\u5de7\u3002\u4f60\u4e5f\u53ef\u4ee5\u5728 OpenAI Playground \u4e2d\u5c1d\u8bd5\u65b0\u7684\u804a\u5929\u683c\u5f0f\u3002 \u4ecb\u7ecd \u00b6 \u804a\u5929\u6a21\u578b\u4ee5\u4e00\u7cfb\u5217\u6d88\u606f\u4f5c\u4e3a\u8f93\u5165\uff0c\u5e76\u8fd4\u56de\u6a21\u578b\u751f\u6210\u7684\u6d88\u606f\u4f5c\u4e3a\u8f93\u51fa\u3002 \u5c3d\u7ba1\u804a\u5929\u683c\u5f0f\u7684\u8bbe\u8ba1\u662f\u4e3a\u4e86\u7b80\u5316\u591a\u56de\u5408\u5bf9\u8bdd\uff0c\u4f46\u5b83\u5bf9\u4e8e\u6ca1\u6709\u4efb\u4f55\u5bf9\u8bdd\u7684\u5355\u56de\u5408\u4efb\u52a1\u540c\u6837\u6709\u7528(\u4f8b\u5982\u4ee5\u524d\u7531 text-davinci-003 \u7b49\u6307\u4ee4\u9075\u5faa\u6a21\u578b\u63d0\u4f9b\u7684\u4efb\u52a1)\u3002 \u4e00\u4e2a API \u8c03\u7528\u793a\u4f8b\u5982\u4e0b\u6240\u793a: 1 2 3 4 5 6 7 8 9 10 11 12 13 # \u6ce8\u610f:\u4e0b\u9762\u7684\u4ee3\u7801\u9700\u8981\u4f7f\u7528OpenAI Python v0.27.0\u624d\u80fd\u5de5\u4f5c import openai openai . ChatCompletion . create ( model = \"gpt-3.5-turbo\" , messages = [ { \"role\" : \"system\" , \"content\" : \"\u4f60\u662f\u4e2a\u5f88\u6709\u5e2e\u52a9\u7684\u52a9\u624b\u3002\" }, { \"role\" : \"user\" , \"content\" : \"\u8c01\u8d62\u5f97\u4e862020\u5e74\u7684\u4e16\u754c\u804c\u4e1a\u68d2\u7403\u5927\u8d5b?\" }, { \"role\" : \"assistant\" , \"content\" : \"\u6d1b\u6749\u77f6\u9053\u5947\u961f\u57282020\u5e74\u8d62\u5f97\u4e86\u4e16\u754c\u804c\u4e1a\u68d2\u7403\u5927\u8d5b\u51a0\u519b\u3002\" }, { \"role\" : \"user\" , \"content\" : \"\u5728\u54ea\u91cc\u6f14\u51fa\u7684?\" } ] ) \u4e3b\u8981\u8f93\u5165\u662f messages \u53c2\u6570\u3002\u6d88\u606f\u5fc5\u987b\u662f\u6d88\u606f\u5bf9\u8c61\u7684\u6570\u7ec4\uff0c\u5176\u4e2d\u6bcf\u4e2a\u5bf9\u8c61\u90fd\u6709\u4e00\u4e2a\u89d2\u8272(\u201csystem\u201d, \u201cuser\u201d, or \u201cassistant\u201d)\u548c\u5185\u5bb9(\u6d88\u606f\u7684\u5185\u5bb9)\u3002\u5bf9\u8bdd\u53ef\u4ee5\u77ed\u5230\u4e00\u6761\u4fe1\u606f\uff0c\u4e5f\u53ef\u4ee5\u586b\u6ee1\u5f88\u591a\u9875\u3002 \u901a\u5e38\uff0c\u4f1a\u8bdd\u9996\u5148\u4f7f\u7528\u7cfb\u7edf\u6d88\u606f\u683c\u5f0f\u5316\uff0c\u7136\u540e\u4ea4\u66ff\u4f7f\u7528\u7528\u6237\u6d88\u606f\u548c\u52a9\u624b\u6d88\u606f\u3002 \u7cfb\u7edf\u6d88\u606f\u5e2e\u52a9\u8bbe\u7f6e\u52a9\u624b\u7684\u884c\u4e3a\u3002\u5728\u4e0a\u9762\u7684\u4f8b\u5b50\u4e2d\uff0c\u52a9\u7406\u88ab\u544a\u77e5\u201c\u4f60\u662f\u4e00\u4e2a\u4e50\u4e8e\u52a9\u4eba\u7684\u52a9\u7406\u3002\u201d Tips Gpt-3.5-turbo-0301\u5e76\u4e0d\u603b\u662f\u7279\u522b\u6ce8\u610f\u7cfb\u7edf\u6d88\u606f\u3002\u672a\u6765\u7684\u6a21\u578b\u5c06\u88ab\u8bad\u7ec3\u6210\u5bf9\u7cfb\u7edf\u4fe1\u606f\u66f4\u52a0\u5173\u6ce8\u3002 \u7528\u6237\u6d88\u606f\u5e2e\u52a9\u6307\u793a\u52a9\u624b\u3002\u5b83\u4eec\u53ef\u4ee5\u7531\u5e94\u7528\u7a0b\u5e8f\u7684\u6700\u7ec8\u7528\u6237\u751f\u6210\uff0c\u4e5f\u53ef\u4ee5\u7531\u5f00\u53d1\u4eba\u5458\u8bbe\u7f6e\u4e3a\u6307\u4ee4\u3002 \u8f85\u52a9\u6d88\u606f\u6709\u52a9\u4e8e\u5b58\u50a8\u5148\u524d\u7684\u54cd\u5e94\u3002\u5b83\u4eec\u4e5f\u53ef\u4ee5\u7531\u5f00\u53d1\u4eba\u5458\u7f16\u5199\uff0c\u4ee5\u5e2e\u52a9\u7ed9\u51fa\u6240\u9700\u884c\u4e3a\u7684\u793a\u4f8b\u3002 \u5f53\u7528\u6237\u6307\u793a\u5f15\u7528\u5148\u524d\u7684\u6d88\u606f\u65f6\uff0c\u5305\u542b\u4f1a\u8bdd\u5386\u53f2\u5c06\u6709\u6240\u5e2e\u52a9\u3002 \u5728\u4e0a\u9762\u7684\u4f8b\u5b50\u4e2d\uff0c\u7528\u6237\u7684\u6700\u540e\u4e00\u4e2a\u95ee\u9898\u662f\u201c\u5728\u54ea\u91cc\u64ad\u653e?\u201d\u53ea\u6709\u5728\u4e4b\u524d\u5173\u4e8e 2020 \u5e74\u4e16\u754c\u804c\u4e1a\u68d2\u7403\u5927\u8d5b\u7684\u6d88\u606f\u4e2d\u624d\u6709\u610f\u4e49\u3002 \u56e0\u4e3a\u6a21\u578b\u5bf9\u8fc7\u53bb\u7684\u8bf7\u6c42\u6ca1\u6709\u8bb0\u5fc6\uff0c\u6240\u4ee5\u6240\u6709\u76f8\u5173\u4fe1\u606f\u90fd\u5fc5\u987b\u901a\u8fc7\u5bf9\u8bdd\u63d0\u4f9b\u3002 \u5982\u679c\u4f1a\u8bdd\u4e0d\u80fd\u7b26\u5408\u6a21\u578b\u7684\u4ee4\u724c\u9650\u5236\uff0c\u5219\u9700\u8981\u4ee5\u67d0\u79cd\u65b9\u5f0f\u7f29\u77ed\u4f1a\u8bdd\u3002 \u8fd4\u56de : format \u00b6 \u4e00\u4e2a API \u54cd\u5e94\u793a\u4f8b\u5982\u4e0b: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 { \"id\" : \"chatcmpl-6p9XYPYSTTRi0xEviKjjilqrWU2Ve\" , \"object\" : \"chat.completion\" , \"created\" : 1677649420 , \"model\" : \"gpt-3.5-turbo\" , \"usage\" : { \"prompt_tokens\" : 56 , \"completion_tokens\" : 31 , \"total_tokens\" : 87 }, \"choices\" : [ { \"message\" : { \"role\" : \"assistant\" , \"content\" : \"2020\u5e74\u4e16\u754c\u804c\u4e1a\u68d2\u7403\u5927\u8d5b\u5728\u5fb7\u514b\u8428\u65af\u5dde\u963f\u7075\u987f\u7684\u73af\u7403\u751f\u6d3b\u7403\u573a\u4e3e\u884c\uff0c\u8fd9\u91cc\u662f\u5fb7\u5dde\u6e38\u9a91\u5175\u961f\u7684\u65b0\u4e3b\u573a\u3002\" }, \"finish_reason\" : \"stop\" , \"index\" : 0 } ] } \u5728 Python \u4e2d\uff0c\u52a9\u624b\u7684\u56de\u590d\u53ef\u4ee5\u901a\u8fc7 response[\u2018choices\u2019][0][\u2018message\u2019][\u2018content\u2019] \u63d0\u53d6\u51fa\u6765. \u6bcf\u4e2a\u54cd\u5e94\u90fd\u5c06\u5305\u542b\u4e00\u4e2a\u5b8c\u6210\u539f\u56e0 finish_reason \u3002\u5b8c\u6210\u539f\u56e0\u7684\u53ef\u80fd\u53d6\u503c\u4e3a\uff1a stop: API \u8fd4\u56de\u4e86\u5b8c\u6574\u7684\u6a21\u578b\u8f93\u51fa\u3002 length: max_tokens \u53c2\u6570\u6216\u4ee4\u724c\u9650\u5236\u5bfc\u81f4\u6a21\u578b\u8f93\u51fa\u4e0d\u5b8c\u6574 content_filter: \u7531\u4e8e\u5185\u5bb9\u8fc7\u6ee4\u5668\u7684\u6807\u8bb0\u800c\u7701\u7565\u4e86\u5185\u5bb9 null: API \u54cd\u5e94\u4ecd\u5728\u8fdb\u884c\u4e2d\u6216\u672a\u5b8c\u6210 \u7ba1\u7406\u4ee4\u724c \u00b6 \u8bed\u8a00\u6a21\u578b\u4ee5\u79f0\u4e3a\u6807\u8bb0\u7684\u5757\u8bfb\u53d6\u6587\u672c\u3002\u5728\u82f1\u8bed\u4e2d\uff0c\u4e00\u4e2a\u6807\u8bb0\u53ef\u4ee5\u77ed\u5230\u4e00\u4e2a\u5b57\u7b26\uff0c\u4e5f\u53ef\u4ee5\u957f\u5230\u4e00\u4e2a\u5355\u8bcd(\u4f8b\u5982\uff0ca \u6216 apple)\uff0c\u5728\u4e00\u4e9b\u8bed\u8a00\u4e2d\uff0c\u6807\u8bb0\u751a\u81f3\u53ef\u4ee5\u77ed\u4e8e\u4e00\u4e2a\u5b57\u7b26\uff0c\u751a\u81f3\u957f\u4e8e\u4e00\u4e2a\u5355\u8bcd\u3002 \u4f8b\u5982\uff0c\u5b57\u7b26\u4e32\u201cChatGPT is great!\u201d\u201c\u7f16\u7801\u6210\u516d\u4e2a\u4ee4\u724c:[\u201cChat\u201d, \u201cG\u201d, \u201cPT\u201d, \u201c is\u201d, \u201c great\u201d, \u201c!\u201d].\u3002 API \u8c03\u7528\u4e2d\u7684\u4ee4\u724c\u603b\u6570\u5f71\u54cd: \u5f53\u4f60\u4e3a\u6bcf\u4e2a\u4ee4\u724c\u652f\u4ed8\u65f6\uff0c\u4f60\u7684 API \u8c03\u7528\u6210\u672c\u662f\u591a\u5c11 \u4f60\u7684 API \u8c03\u7528\u9700\u8981\u591a\u957f\u65f6\u95f4\uff0c\u56e0\u4e3a\u5199\u66f4\u591a\u7684\u4ee4\u724c\u9700\u8981\u66f4\u591a\u7684\u65f6\u95f4 \u4f60\u7684 API \u8c03\u7528\u662f\u5426\u6709\u6548\uff0c\u56e0\u4e3a\u603b\u7684\u4ee4\u724c\u5fc5\u987b\u4f4e\u4e8e\u6a21\u578b\u7684\u6700\u5927\u9650\u5236(gpt-3.5-turbo-0301 \u7684 4096 \u4e2a\u4ee4\u724c) API \u8c03\u7528\u4e2d\u7684\u4ee4\u724c\u603b\u6570\u5f71\u54cd:\u5f53\u4f60\u4e3a\u6bcf\u4e2a\u4ee4\u724c\u652f\u4ed8\u8d39\u7528\u65f6\uff0c\u4f60\u7684 API \u8c03\u7528\u9700\u8981\u591a\u957f\u65f6\u95f4\uff0c\u56e0\u4e3a\u5199\u66f4\u591a\u4ee4\u724c\u9700\u8981\u66f4\u591a\u65f6\u95f4\uff0c\u4f60\u7684 API \u8c03\u7528\u662f\u5426\u6709\u6548\uff0c\u56e0\u4e3a\u603b\u4ee4\u724c\u5fc5\u987b\u4f4e\u4e8e\u6a21\u578b\u7684\u6700\u5927\u9650\u5236(gpt-3.5-turbo-0301 \u7684 4096 \u4e2a\u4ee4\u724c) \u8f93\u5165\u548c\u8f93\u51fa\u4ee4\u724c\u90fd\u8ba1\u5165\u8fd9\u4e9b\u6570\u91cf\u3002\u4f8b\u5982\uff0c\u5982\u679c\u60a8\u7684 API \u8c03\u7528\u5728\u6d88\u606f\u8f93\u5165\u4e2d\u4f7f\u7528\u4e86 10 \u4e2a\u4ee4\u724c\uff0c\u800c\u60a8\u5728\u6d88\u606f\u8f93\u51fa\u4e2d\u6536\u5230\u4e86 20 \u4e2a\u4ee4\u724c\uff0c\u90a3\u4e48\u60a8\u5c06\u4e3a 30 \u4e2a\u4ee4\u724c\u4ed8\u8d39\u3002 \u8981\u67e5\u770b API \u8c03\u7528\u4f7f\u7528\u4e86\u591a\u5c11\u4ee4\u724c\uff0c\u8bf7\u68c0\u67e5 API \u54cd\u5e94\u4e2d\u7684 usage \u5b57\u6bb5 (e.g., response[\u2018usage\u2019][\u2018total_tokens\u2019] ). \u8981\u5728\u4e0d\u8c03\u7528 API \u7684\u60c5\u51b5\u4e0b\u67e5\u770b\u6587\u672c\u5b57\u7b26\u4e32\u4e2d\u6709\u591a\u5c11\u4ee4\u724c\uff0c\u8bf7\u4f7f\u7528 OpenAI \u7684 tiktoken Python \u5e93\u3002\u793a\u4f8b\u4ee3\u7801\u53ef\u4ee5\u5728 OpenAI Cookbook \u5173\u4e8e\u5982\u4f55\u4f7f\u7528 tiktoken \u8ba1\u7b97\u4ee4\u724c\u7684\u6307\u5357\u4e2d\u627e\u5230\u3002 \u4f20\u9012\u7ed9 API \u7684\u6bcf\u6761\u6d88\u606f\u90fd\u4f1a\u6d88\u8017\u5185\u5bb9\u3001\u89d2\u8272\u548c\u5176\u4ed6\u5b57\u6bb5\u4e2d\u7684\u4ee4\u724c\u6570\u91cf\uff0c\u4ee5\u53ca\u4e00\u4e9b\u7528\u4e8e\u5e55\u540e\u683c\u5f0f\u5316\u7684\u989d\u5916\u4ee4\u724c\u3002\u8fd9\u79cd\u60c5\u51b5\u5728\u672a\u6765\u53ef\u80fd\u4f1a\u7565\u6709\u6539\u53d8\u3002 \u5982\u679c\u4e00\u4e2a\u5bf9\u8bdd\u6709\u592a\u591a\u7684\u6807\u8bb0\uff0c\u65e0\u6cd5\u6ee1\u8db3\u6a21\u578b\u7684\u6700\u5927\u9650\u5236(\u4f8b\u5982\uff0c\u5bf9\u4e8e gpt-3.5-turbo\uff0c\u8d85\u8fc7 4096 \u4e2a\u6807\u8bb0)\uff0c\u60a8\u5c06\u4e0d\u5f97\u4e0d\u622a\u65ad\u3001\u7701\u7565\u6216\u4ee5\u5176\u4ed6\u65b9\u5f0f\u7f29\u5c0f\u60a8\u7684\u6587\u672c\uff0c\u76f4\u5230\u5b83\u9002\u5408\u3002 \u6ce8\u610f\uff0c\u5982\u679c\u4ece\u6d88\u606f\u8f93\u5165\u4e2d\u5220\u9664\u6d88\u606f\uff0c\u6a21\u578b\u5c06\u5931\u53bb\u5bf9\u8be5\u6d88\u606f\u7684\u6240\u6709\u77e5\u8bc6\u3002 \u8fd8\u8981\u6ce8\u610f\uff0c\u975e\u5e38\u957f\u7684\u5bf9\u8bdd\u66f4\u6709\u53ef\u80fd\u6536\u5230\u4e0d\u5b8c\u6574\u7684\u56de\u590d\u3002\u4f8b\u5982\uff0c\u4e00\u4e2a gpt-3.5-turbo \u4f1a\u8bdd\u6709 4090 \u4e2a\u4ee4\u724c\uff0c\u5b83\u7684\u56de\u590d\u5728 6 \u4e2a\u4ee4\u724c\u540e\u5c31\u4f1a\u88ab\u5207\u65ad\u3002 \u6307\u5bfc\u804a\u5929\u6a21\u578b \u00b6 \u6307\u5bfc\u6a21\u578b\u7684\u6700\u4f73\u5b9e\u8df5\u53ef\u80fd\u4f1a\u56e0\u6a21\u578b\u7248\u672c\u7684\u4e0d\u540c\u800c\u6709\u6240\u4e0d\u540c\u3002\u4ee5\u4e0b\u5efa\u8bae\u9002\u7528\u4e8e gpt-3.5-turbo-0301\uff0c\u53ef\u80fd\u4e0d\u9002\u7528\u4e8e\u672a\u6765\u7684\u578b\u53f7\u3002 \u8bb8\u591a\u5bf9\u8bdd\u90fd\u4ee5\u7cfb\u7edf\u6d88\u606f\u5f00\u59cb\uff0c\u4ee5\u6e29\u548c\u5730\u6307\u793a\u52a9\u624b\u3002\u4f8b\u5982\uff0c\u4e0b\u9762\u662f ChatGPT \u4f7f\u7528\u7684\u4e00\u4e2a\u7cfb\u7edf\u6d88\u606f: ::: \u4f60\u662f ChatGPT, OpenAI \u8bad\u7ec3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u56de\u7b54\u8981\u5c3d\u53ef\u80fd\u7b80\u6d01\u3002\u5f53\u524d\u65e5\u671f:{current_date} \u4e00\u822c\u6765\u8bf4\uff0cgpt-3.5-turbo-0301 \u4e0d\u592a\u5173\u6ce8\u7cfb\u7edf\u6d88\u606f\uff0c\u56e0\u6b64\u91cd\u8981\u7684\u6307\u4ee4\u901a\u5e38\u66f4\u597d\u5730\u653e\u5728\u7528\u6237\u6d88\u606f\u4e2d\u3002 \u5982\u679c\u6a21\u578b\u4e0d\u80fd\u751f\u6210\u60a8\u60f3\u8981\u7684\u8f93\u51fa\uff0c\u53ef\u4ee5\u968f\u610f\u8fed\u4ee3\u5e76\u5c1d\u8bd5\u6f5c\u5728\u7684\u6539\u8fdb\u3002\u4f60\u53ef\u4ee5\u5c1d\u8bd5\u4ee5\u4e0b\u65b9\u6cd5: \u8ba9\u4f60\u7684\u6307\u793a\u66f4\u660e\u786e \u6307\u5b9a\u4f60\u60f3\u8981\u7684\u7b54\u6848\u683c\u5f0f \u8ba9\u6a21\u578b\u4e00\u6b65\u4e00\u6b65\u5730\u601d\u8003\uff0c\u6216\u8005\u5728\u786e\u5b9a\u7b54\u6848\u4e4b\u524d\u8fa9\u8bba\u6b63\u53cd\u4e24\u65b9\u9762 \u60f3\u8981\u83b7\u5f97\u66f4\u591a\u5feb\u901f\u7684\u5de5\u7a0b\u60f3\u6cd5\uff0c\u8bf7\u9605\u8bfb OpenAI Cookbook \u5173\u4e8e\u63d0\u9ad8\u53ef\u9760\u6027\u7684\u6280\u672f\u6307\u5357\u3002 \u9664\u4e86\u7cfb\u7edf\u6d88\u606f\u4e4b\u5916\uff0c\u6e29\u5ea6\u548c\u6700\u5927\u4ee4\u724c\u662f\u5f00\u53d1\u4eba\u5458\u5fc5\u987b\u5f71\u54cd\u804a\u5929\u6a21\u578b\u8f93\u51fa\u7684\u4f17\u591a\u9009\u9879\u4e2d\u7684\u4e24\u4e2a\u3002 \u5bf9\u4e8e\u6e29\u5ea6\uff0c\u8f83\u9ad8\u7684\u503c(\u5982 0.8)\u5c06\u4f7f\u8f93\u51fa\u66f4\u968f\u673a\uff0c\u800c\u8f83\u4f4e\u7684\u503c(\u5982 0.2)\u5c06\u4f7f\u8f93\u51fa\u66f4\u96c6\u4e2d\u548c\u786e\u5b9a\u3002 \u5bf9\u4e8e max \u4ee4\u724c\uff0c\u5982\u679c\u5e0c\u671b\u5c06\u54cd\u5e94\u9650\u5236\u5728\u67d0\u4e2a\u957f\u5ea6\uff0c\u5219\u53ef\u4ee5\u5c06 max \u4ee4\u724c\u8bbe\u7f6e\u4e3a\u4efb\u610f\u6570\u5b57\u3002 \u8fd9\u53ef\u80fd\u4f1a\u5bfc\u81f4\u95ee\u9898\uff0c\u4f8b\u5982\uff0c\u5982\u679c\u60a8\u5c06\u6700\u5927\u4ee4\u724c\u503c\u8bbe\u7f6e\u4e3a 5\uff0c\u56e0\u4e3a\u8f93\u51fa\u5c06\u88ab\u5207\u65ad\uff0c\u7ed3\u679c\u5bf9\u7528\u6237\u6ca1\u6709\u610f\u4e49\u3002 \u804a\u5929\u4e0e\u8865\u5168 \u00b6 \u56e0\u4e3a gpt-3.5-turbo \u7684\u6027\u80fd\u4e0e text-davinci-003 \u76f8\u4f3c\uff0c\u4f46\u6bcf\u4e2a\u4ee4\u724c\u7684\u4ef7\u683c\u662f\u5b83\u7684 10%\uff0c\u6240\u4ee5\u6211\u4eec\u63a8\u8350\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u4f7f\u7528 gpt-3.5-turbo \u3002 \u5bf9\u4e8e\u8bb8\u591a\u5f00\u53d1\u4eba\u5458\u6765\u8bf4\uff0c\u8f6c\u6362\u5c31\u50cf\u91cd\u5199\u548c\u91cd\u65b0\u6d4b\u8bd5\u63d0\u793a\u7b26\u4e00\u6837\u7b80\u5355\u3002 \u4f8b\u5982\uff0c\u5982\u679c\u4f7f\u7528\u4ee5\u4e0b\u8865\u5168\u63d0\u793a\u7b26\u5c06\u82f1\u8bed\u7ffb\u8bd1\u4e3a\u6cd5\u8bed: \u5c06\u4ee5\u4e0b\u82f1\u6587\u6587\u672c\u7ffb\u8bd1\u6210\u6cd5\u6587:\"{text}\" \u4e00\u4e2a\u7b49\u4ef7\u7684\u804a\u5929\u5bf9\u8bdd\u53ef\u4ee5\u662f\u8fd9\u6837\u7684: 1 2 3 4 [ { \u201crole\u201d : \u201csys te m\u201d , \u201cco ntent \u201d : \u201cYou are a help ful assis tant t ha t translates E n glish t o Fre n ch.\u201d }, { \u201crole\u201d : \u201cuser\u201d , \u201cco ntent \u201d : \u2018Tra nslate t he f ollowi n g E n glish te x t t o Fre n ch : \u201c { te x t } \u201d\u2019 } ] \u6216\u8005\u4ec5\u4ec5\u662f\u7528\u6237\u4fe1\u606f: 1 2 3 [ { \u201crole\u201d : \u201cuser\u201d , \u201cco ntent \u201d : \u2018Tra nslate t he f ollowi n g E n glish te x t t o Fre n ch : \u201c { te x t } \u201d\u2019 } ] FAQ \u00b6 \u662f\u5fae\u8c03\u53ef\u7528\u4e8e gpt-3.5-turbo? \u4e0d\u3002\u4ece2023\u5e743\u67081\u65e5\u8d77\uff0c\u60a8\u53ea\u80fd\u5fae\u8c03\u57fa\u7840GPT-3\u6a21\u578b\u3002\u6709\u5173\u5982\u4f55\u4f7f\u7528\u5fae\u8c03\u6a21\u578b\u7684\u66f4\u591a\u7ec6\u8282\uff0c\u8bf7\u53c2\u9605\u5fae\u8c03\u6307\u5357\u3002 \u662f\u5426\u5b58\u50a8\u4f20\u9012\u7ed9 API \u7684\u6570\u636e? \u81ea2023\u5e743\u67081\u65e5\u8d77\uff0c\u6211\u4eec\u5c06\u4fdd\u7559\u60a8\u7684API\u6570\u636e30\u5929\uff0c\u4f46\u4e0d\u518d\u4f7f\u7528\u60a8\u901a\u8fc7API\u53d1\u9001\u7684\u6570\u636e\u6765\u6539\u8fdb\u6211\u4eec\u7684\u6a21\u578b\u3002\u5728\u6211\u4eec\u7684\u6570\u636e\u4f7f\u7528\u653f\u7b56\u4e2d\u4e86\u89e3\u66f4\u591a\u4fe1\u606f\u3002 \u6dfb\u52a0\u8c03\u8282\u5c42 \u5982\u679c\u4f60\u60f3\u5728\u804a\u5929API\u7684\u8f93\u51fa\u4e2d\u6dfb\u52a0\u4e00\u4e2a\u5ba1\u6838\u5c42\uff0c\u4f60\u53ef\u4ee5\u6309\u7167\u6211\u4eec\u7684\u5ba1\u6838\u6307\u5357\u6765\u9632\u6b62\u8fdd\u53cdOpenAI\u4f7f\u7528\u7b56\u7565\u7684\u5185\u5bb9\u88ab\u663e\u793a\u51fa\u6765\u3002","title":"\u804a\u5929\u8865\u5168"},{"location":"guides/chat/#beta","text":"ChatGPT \u7531 OpenAI \u6700\u5148\u8fdb\u7684\u8bed\u8a00\u6a21\u578b gpt-3.5-turbo \u652f\u6301\u3002 \u4f7f\u7528 OpenAI API\uff0c\u4f60\u53ef\u4ee5\u7528 gpt-3.5-turbo \u6784\u5efa\u4f60\u81ea\u5df1\u7684\u5e94\u7528\u7a0b\u5e8f\u6765\u505a\u4ee5\u4e0b\u4e8b\u60c5: \u8d77\u8349\u4e00\u5c01\u7535\u5b50\u90ae\u4ef6\u6216\u5176\u4ed6\u6587\u5b57 \u7f16\u5199 Python \u4ee3\u7801 \u56de\u7b54\u5173\u4e8e\u4e00\u7ec4\u6587\u6863\u7684\u95ee\u9898 \u521b\u5efa\u4f1a\u8bdd\u4ee3\u7406 \u7ed9\u4f60\u7684\u8f6f\u4ef6\u4e00\u4e2a\u81ea\u7136\u8bed\u8a00\u754c\u9762 \u8f85\u5bfc\u5404\u79cd\u5b66\u79d1 \u7ffb\u8bd1\u8bed\u8a00 \u6a21\u62df\u89d2\u8272\u7684\u89c6\u9891\u6e38\u620f\u548c\u66f4\u591a \u672c\u6307\u5357\u89e3\u91ca\u4e86\u5982\u4f55\u5bf9\u57fa\u4e8e\u804a\u5929\u7684\u8bed\u8a00\u6a21\u578b\u8fdb\u884c API \u8c03\u7528\uff0c\u5e76\u5206\u4eab\u4e86\u83b7\u5f97\u826f\u597d\u7ed3\u679c\u7684\u6280\u5de7\u3002\u4f60\u4e5f\u53ef\u4ee5\u5728 OpenAI Playground \u4e2d\u5c1d\u8bd5\u65b0\u7684\u804a\u5929\u683c\u5f0f\u3002","title":"\u804a\u5929\u8865\u5168 Beta"},{"location":"guides/chat/#_1","text":"\u804a\u5929\u6a21\u578b\u4ee5\u4e00\u7cfb\u5217\u6d88\u606f\u4f5c\u4e3a\u8f93\u5165\uff0c\u5e76\u8fd4\u56de\u6a21\u578b\u751f\u6210\u7684\u6d88\u606f\u4f5c\u4e3a\u8f93\u51fa\u3002 \u5c3d\u7ba1\u804a\u5929\u683c\u5f0f\u7684\u8bbe\u8ba1\u662f\u4e3a\u4e86\u7b80\u5316\u591a\u56de\u5408\u5bf9\u8bdd\uff0c\u4f46\u5b83\u5bf9\u4e8e\u6ca1\u6709\u4efb\u4f55\u5bf9\u8bdd\u7684\u5355\u56de\u5408\u4efb\u52a1\u540c\u6837\u6709\u7528(\u4f8b\u5982\u4ee5\u524d\u7531 text-davinci-003 \u7b49\u6307\u4ee4\u9075\u5faa\u6a21\u578b\u63d0\u4f9b\u7684\u4efb\u52a1)\u3002 \u4e00\u4e2a API \u8c03\u7528\u793a\u4f8b\u5982\u4e0b\u6240\u793a: 1 2 3 4 5 6 7 8 9 10 11 12 13 # \u6ce8\u610f:\u4e0b\u9762\u7684\u4ee3\u7801\u9700\u8981\u4f7f\u7528OpenAI Python v0.27.0\u624d\u80fd\u5de5\u4f5c import openai openai . ChatCompletion . create ( model = \"gpt-3.5-turbo\" , messages = [ { \"role\" : \"system\" , \"content\" : \"\u4f60\u662f\u4e2a\u5f88\u6709\u5e2e\u52a9\u7684\u52a9\u624b\u3002\" }, { \"role\" : \"user\" , \"content\" : \"\u8c01\u8d62\u5f97\u4e862020\u5e74\u7684\u4e16\u754c\u804c\u4e1a\u68d2\u7403\u5927\u8d5b?\" }, { \"role\" : \"assistant\" , \"content\" : \"\u6d1b\u6749\u77f6\u9053\u5947\u961f\u57282020\u5e74\u8d62\u5f97\u4e86\u4e16\u754c\u804c\u4e1a\u68d2\u7403\u5927\u8d5b\u51a0\u519b\u3002\" }, { \"role\" : \"user\" , \"content\" : \"\u5728\u54ea\u91cc\u6f14\u51fa\u7684?\" } ] ) \u4e3b\u8981\u8f93\u5165\u662f messages \u53c2\u6570\u3002\u6d88\u606f\u5fc5\u987b\u662f\u6d88\u606f\u5bf9\u8c61\u7684\u6570\u7ec4\uff0c\u5176\u4e2d\u6bcf\u4e2a\u5bf9\u8c61\u90fd\u6709\u4e00\u4e2a\u89d2\u8272(\u201csystem\u201d, \u201cuser\u201d, or \u201cassistant\u201d)\u548c\u5185\u5bb9(\u6d88\u606f\u7684\u5185\u5bb9)\u3002\u5bf9\u8bdd\u53ef\u4ee5\u77ed\u5230\u4e00\u6761\u4fe1\u606f\uff0c\u4e5f\u53ef\u4ee5\u586b\u6ee1\u5f88\u591a\u9875\u3002 \u901a\u5e38\uff0c\u4f1a\u8bdd\u9996\u5148\u4f7f\u7528\u7cfb\u7edf\u6d88\u606f\u683c\u5f0f\u5316\uff0c\u7136\u540e\u4ea4\u66ff\u4f7f\u7528\u7528\u6237\u6d88\u606f\u548c\u52a9\u624b\u6d88\u606f\u3002 \u7cfb\u7edf\u6d88\u606f\u5e2e\u52a9\u8bbe\u7f6e\u52a9\u624b\u7684\u884c\u4e3a\u3002\u5728\u4e0a\u9762\u7684\u4f8b\u5b50\u4e2d\uff0c\u52a9\u7406\u88ab\u544a\u77e5\u201c\u4f60\u662f\u4e00\u4e2a\u4e50\u4e8e\u52a9\u4eba\u7684\u52a9\u7406\u3002\u201d Tips Gpt-3.5-turbo-0301\u5e76\u4e0d\u603b\u662f\u7279\u522b\u6ce8\u610f\u7cfb\u7edf\u6d88\u606f\u3002\u672a\u6765\u7684\u6a21\u578b\u5c06\u88ab\u8bad\u7ec3\u6210\u5bf9\u7cfb\u7edf\u4fe1\u606f\u66f4\u52a0\u5173\u6ce8\u3002 \u7528\u6237\u6d88\u606f\u5e2e\u52a9\u6307\u793a\u52a9\u624b\u3002\u5b83\u4eec\u53ef\u4ee5\u7531\u5e94\u7528\u7a0b\u5e8f\u7684\u6700\u7ec8\u7528\u6237\u751f\u6210\uff0c\u4e5f\u53ef\u4ee5\u7531\u5f00\u53d1\u4eba\u5458\u8bbe\u7f6e\u4e3a\u6307\u4ee4\u3002 \u8f85\u52a9\u6d88\u606f\u6709\u52a9\u4e8e\u5b58\u50a8\u5148\u524d\u7684\u54cd\u5e94\u3002\u5b83\u4eec\u4e5f\u53ef\u4ee5\u7531\u5f00\u53d1\u4eba\u5458\u7f16\u5199\uff0c\u4ee5\u5e2e\u52a9\u7ed9\u51fa\u6240\u9700\u884c\u4e3a\u7684\u793a\u4f8b\u3002 \u5f53\u7528\u6237\u6307\u793a\u5f15\u7528\u5148\u524d\u7684\u6d88\u606f\u65f6\uff0c\u5305\u542b\u4f1a\u8bdd\u5386\u53f2\u5c06\u6709\u6240\u5e2e\u52a9\u3002 \u5728\u4e0a\u9762\u7684\u4f8b\u5b50\u4e2d\uff0c\u7528\u6237\u7684\u6700\u540e\u4e00\u4e2a\u95ee\u9898\u662f\u201c\u5728\u54ea\u91cc\u64ad\u653e?\u201d\u53ea\u6709\u5728\u4e4b\u524d\u5173\u4e8e 2020 \u5e74\u4e16\u754c\u804c\u4e1a\u68d2\u7403\u5927\u8d5b\u7684\u6d88\u606f\u4e2d\u624d\u6709\u610f\u4e49\u3002 \u56e0\u4e3a\u6a21\u578b\u5bf9\u8fc7\u53bb\u7684\u8bf7\u6c42\u6ca1\u6709\u8bb0\u5fc6\uff0c\u6240\u4ee5\u6240\u6709\u76f8\u5173\u4fe1\u606f\u90fd\u5fc5\u987b\u901a\u8fc7\u5bf9\u8bdd\u63d0\u4f9b\u3002 \u5982\u679c\u4f1a\u8bdd\u4e0d\u80fd\u7b26\u5408\u6a21\u578b\u7684\u4ee4\u724c\u9650\u5236\uff0c\u5219\u9700\u8981\u4ee5\u67d0\u79cd\u65b9\u5f0f\u7f29\u77ed\u4f1a\u8bdd\u3002","title":"\u4ecb\u7ecd"},{"location":"guides/chat/#format","text":"\u4e00\u4e2a API \u54cd\u5e94\u793a\u4f8b\u5982\u4e0b: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 { \"id\" : \"chatcmpl-6p9XYPYSTTRi0xEviKjjilqrWU2Ve\" , \"object\" : \"chat.completion\" , \"created\" : 1677649420 , \"model\" : \"gpt-3.5-turbo\" , \"usage\" : { \"prompt_tokens\" : 56 , \"completion_tokens\" : 31 , \"total_tokens\" : 87 }, \"choices\" : [ { \"message\" : { \"role\" : \"assistant\" , \"content\" : \"2020\u5e74\u4e16\u754c\u804c\u4e1a\u68d2\u7403\u5927\u8d5b\u5728\u5fb7\u514b\u8428\u65af\u5dde\u963f\u7075\u987f\u7684\u73af\u7403\u751f\u6d3b\u7403\u573a\u4e3e\u884c\uff0c\u8fd9\u91cc\u662f\u5fb7\u5dde\u6e38\u9a91\u5175\u961f\u7684\u65b0\u4e3b\u573a\u3002\" }, \"finish_reason\" : \"stop\" , \"index\" : 0 } ] } \u5728 Python \u4e2d\uff0c\u52a9\u624b\u7684\u56de\u590d\u53ef\u4ee5\u901a\u8fc7 response[\u2018choices\u2019][0][\u2018message\u2019][\u2018content\u2019] \u63d0\u53d6\u51fa\u6765. \u6bcf\u4e2a\u54cd\u5e94\u90fd\u5c06\u5305\u542b\u4e00\u4e2a\u5b8c\u6210\u539f\u56e0 finish_reason \u3002\u5b8c\u6210\u539f\u56e0\u7684\u53ef\u80fd\u53d6\u503c\u4e3a\uff1a stop: API \u8fd4\u56de\u4e86\u5b8c\u6574\u7684\u6a21\u578b\u8f93\u51fa\u3002 length: max_tokens \u53c2\u6570\u6216\u4ee4\u724c\u9650\u5236\u5bfc\u81f4\u6a21\u578b\u8f93\u51fa\u4e0d\u5b8c\u6574 content_filter: \u7531\u4e8e\u5185\u5bb9\u8fc7\u6ee4\u5668\u7684\u6807\u8bb0\u800c\u7701\u7565\u4e86\u5185\u5bb9 null: API \u54cd\u5e94\u4ecd\u5728\u8fdb\u884c\u4e2d\u6216\u672a\u5b8c\u6210","title":"\u8fd4\u56de: format"},{"location":"guides/chat/#_2","text":"\u8bed\u8a00\u6a21\u578b\u4ee5\u79f0\u4e3a\u6807\u8bb0\u7684\u5757\u8bfb\u53d6\u6587\u672c\u3002\u5728\u82f1\u8bed\u4e2d\uff0c\u4e00\u4e2a\u6807\u8bb0\u53ef\u4ee5\u77ed\u5230\u4e00\u4e2a\u5b57\u7b26\uff0c\u4e5f\u53ef\u4ee5\u957f\u5230\u4e00\u4e2a\u5355\u8bcd(\u4f8b\u5982\uff0ca \u6216 apple)\uff0c\u5728\u4e00\u4e9b\u8bed\u8a00\u4e2d\uff0c\u6807\u8bb0\u751a\u81f3\u53ef\u4ee5\u77ed\u4e8e\u4e00\u4e2a\u5b57\u7b26\uff0c\u751a\u81f3\u957f\u4e8e\u4e00\u4e2a\u5355\u8bcd\u3002 \u4f8b\u5982\uff0c\u5b57\u7b26\u4e32\u201cChatGPT is great!\u201d\u201c\u7f16\u7801\u6210\u516d\u4e2a\u4ee4\u724c:[\u201cChat\u201d, \u201cG\u201d, \u201cPT\u201d, \u201c is\u201d, \u201c great\u201d, \u201c!\u201d].\u3002 API \u8c03\u7528\u4e2d\u7684\u4ee4\u724c\u603b\u6570\u5f71\u54cd: \u5f53\u4f60\u4e3a\u6bcf\u4e2a\u4ee4\u724c\u652f\u4ed8\u65f6\uff0c\u4f60\u7684 API \u8c03\u7528\u6210\u672c\u662f\u591a\u5c11 \u4f60\u7684 API \u8c03\u7528\u9700\u8981\u591a\u957f\u65f6\u95f4\uff0c\u56e0\u4e3a\u5199\u66f4\u591a\u7684\u4ee4\u724c\u9700\u8981\u66f4\u591a\u7684\u65f6\u95f4 \u4f60\u7684 API \u8c03\u7528\u662f\u5426\u6709\u6548\uff0c\u56e0\u4e3a\u603b\u7684\u4ee4\u724c\u5fc5\u987b\u4f4e\u4e8e\u6a21\u578b\u7684\u6700\u5927\u9650\u5236(gpt-3.5-turbo-0301 \u7684 4096 \u4e2a\u4ee4\u724c) API \u8c03\u7528\u4e2d\u7684\u4ee4\u724c\u603b\u6570\u5f71\u54cd:\u5f53\u4f60\u4e3a\u6bcf\u4e2a\u4ee4\u724c\u652f\u4ed8\u8d39\u7528\u65f6\uff0c\u4f60\u7684 API \u8c03\u7528\u9700\u8981\u591a\u957f\u65f6\u95f4\uff0c\u56e0\u4e3a\u5199\u66f4\u591a\u4ee4\u724c\u9700\u8981\u66f4\u591a\u65f6\u95f4\uff0c\u4f60\u7684 API \u8c03\u7528\u662f\u5426\u6709\u6548\uff0c\u56e0\u4e3a\u603b\u4ee4\u724c\u5fc5\u987b\u4f4e\u4e8e\u6a21\u578b\u7684\u6700\u5927\u9650\u5236(gpt-3.5-turbo-0301 \u7684 4096 \u4e2a\u4ee4\u724c) \u8f93\u5165\u548c\u8f93\u51fa\u4ee4\u724c\u90fd\u8ba1\u5165\u8fd9\u4e9b\u6570\u91cf\u3002\u4f8b\u5982\uff0c\u5982\u679c\u60a8\u7684 API \u8c03\u7528\u5728\u6d88\u606f\u8f93\u5165\u4e2d\u4f7f\u7528\u4e86 10 \u4e2a\u4ee4\u724c\uff0c\u800c\u60a8\u5728\u6d88\u606f\u8f93\u51fa\u4e2d\u6536\u5230\u4e86 20 \u4e2a\u4ee4\u724c\uff0c\u90a3\u4e48\u60a8\u5c06\u4e3a 30 \u4e2a\u4ee4\u724c\u4ed8\u8d39\u3002 \u8981\u67e5\u770b API \u8c03\u7528\u4f7f\u7528\u4e86\u591a\u5c11\u4ee4\u724c\uff0c\u8bf7\u68c0\u67e5 API \u54cd\u5e94\u4e2d\u7684 usage \u5b57\u6bb5 (e.g., response[\u2018usage\u2019][\u2018total_tokens\u2019] ). \u8981\u5728\u4e0d\u8c03\u7528 API \u7684\u60c5\u51b5\u4e0b\u67e5\u770b\u6587\u672c\u5b57\u7b26\u4e32\u4e2d\u6709\u591a\u5c11\u4ee4\u724c\uff0c\u8bf7\u4f7f\u7528 OpenAI \u7684 tiktoken Python \u5e93\u3002\u793a\u4f8b\u4ee3\u7801\u53ef\u4ee5\u5728 OpenAI Cookbook \u5173\u4e8e\u5982\u4f55\u4f7f\u7528 tiktoken \u8ba1\u7b97\u4ee4\u724c\u7684\u6307\u5357\u4e2d\u627e\u5230\u3002 \u4f20\u9012\u7ed9 API \u7684\u6bcf\u6761\u6d88\u606f\u90fd\u4f1a\u6d88\u8017\u5185\u5bb9\u3001\u89d2\u8272\u548c\u5176\u4ed6\u5b57\u6bb5\u4e2d\u7684\u4ee4\u724c\u6570\u91cf\uff0c\u4ee5\u53ca\u4e00\u4e9b\u7528\u4e8e\u5e55\u540e\u683c\u5f0f\u5316\u7684\u989d\u5916\u4ee4\u724c\u3002\u8fd9\u79cd\u60c5\u51b5\u5728\u672a\u6765\u53ef\u80fd\u4f1a\u7565\u6709\u6539\u53d8\u3002 \u5982\u679c\u4e00\u4e2a\u5bf9\u8bdd\u6709\u592a\u591a\u7684\u6807\u8bb0\uff0c\u65e0\u6cd5\u6ee1\u8db3\u6a21\u578b\u7684\u6700\u5927\u9650\u5236(\u4f8b\u5982\uff0c\u5bf9\u4e8e gpt-3.5-turbo\uff0c\u8d85\u8fc7 4096 \u4e2a\u6807\u8bb0)\uff0c\u60a8\u5c06\u4e0d\u5f97\u4e0d\u622a\u65ad\u3001\u7701\u7565\u6216\u4ee5\u5176\u4ed6\u65b9\u5f0f\u7f29\u5c0f\u60a8\u7684\u6587\u672c\uff0c\u76f4\u5230\u5b83\u9002\u5408\u3002 \u6ce8\u610f\uff0c\u5982\u679c\u4ece\u6d88\u606f\u8f93\u5165\u4e2d\u5220\u9664\u6d88\u606f\uff0c\u6a21\u578b\u5c06\u5931\u53bb\u5bf9\u8be5\u6d88\u606f\u7684\u6240\u6709\u77e5\u8bc6\u3002 \u8fd8\u8981\u6ce8\u610f\uff0c\u975e\u5e38\u957f\u7684\u5bf9\u8bdd\u66f4\u6709\u53ef\u80fd\u6536\u5230\u4e0d\u5b8c\u6574\u7684\u56de\u590d\u3002\u4f8b\u5982\uff0c\u4e00\u4e2a gpt-3.5-turbo \u4f1a\u8bdd\u6709 4090 \u4e2a\u4ee4\u724c\uff0c\u5b83\u7684\u56de\u590d\u5728 6 \u4e2a\u4ee4\u724c\u540e\u5c31\u4f1a\u88ab\u5207\u65ad\u3002","title":"\u7ba1\u7406\u4ee4\u724c"},{"location":"guides/chat/#_3","text":"\u6307\u5bfc\u6a21\u578b\u7684\u6700\u4f73\u5b9e\u8df5\u53ef\u80fd\u4f1a\u56e0\u6a21\u578b\u7248\u672c\u7684\u4e0d\u540c\u800c\u6709\u6240\u4e0d\u540c\u3002\u4ee5\u4e0b\u5efa\u8bae\u9002\u7528\u4e8e gpt-3.5-turbo-0301\uff0c\u53ef\u80fd\u4e0d\u9002\u7528\u4e8e\u672a\u6765\u7684\u578b\u53f7\u3002 \u8bb8\u591a\u5bf9\u8bdd\u90fd\u4ee5\u7cfb\u7edf\u6d88\u606f\u5f00\u59cb\uff0c\u4ee5\u6e29\u548c\u5730\u6307\u793a\u52a9\u624b\u3002\u4f8b\u5982\uff0c\u4e0b\u9762\u662f ChatGPT \u4f7f\u7528\u7684\u4e00\u4e2a\u7cfb\u7edf\u6d88\u606f: ::: \u4f60\u662f ChatGPT, OpenAI \u8bad\u7ec3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u56de\u7b54\u8981\u5c3d\u53ef\u80fd\u7b80\u6d01\u3002\u5f53\u524d\u65e5\u671f:{current_date} \u4e00\u822c\u6765\u8bf4\uff0cgpt-3.5-turbo-0301 \u4e0d\u592a\u5173\u6ce8\u7cfb\u7edf\u6d88\u606f\uff0c\u56e0\u6b64\u91cd\u8981\u7684\u6307\u4ee4\u901a\u5e38\u66f4\u597d\u5730\u653e\u5728\u7528\u6237\u6d88\u606f\u4e2d\u3002 \u5982\u679c\u6a21\u578b\u4e0d\u80fd\u751f\u6210\u60a8\u60f3\u8981\u7684\u8f93\u51fa\uff0c\u53ef\u4ee5\u968f\u610f\u8fed\u4ee3\u5e76\u5c1d\u8bd5\u6f5c\u5728\u7684\u6539\u8fdb\u3002\u4f60\u53ef\u4ee5\u5c1d\u8bd5\u4ee5\u4e0b\u65b9\u6cd5: \u8ba9\u4f60\u7684\u6307\u793a\u66f4\u660e\u786e \u6307\u5b9a\u4f60\u60f3\u8981\u7684\u7b54\u6848\u683c\u5f0f \u8ba9\u6a21\u578b\u4e00\u6b65\u4e00\u6b65\u5730\u601d\u8003\uff0c\u6216\u8005\u5728\u786e\u5b9a\u7b54\u6848\u4e4b\u524d\u8fa9\u8bba\u6b63\u53cd\u4e24\u65b9\u9762 \u60f3\u8981\u83b7\u5f97\u66f4\u591a\u5feb\u901f\u7684\u5de5\u7a0b\u60f3\u6cd5\uff0c\u8bf7\u9605\u8bfb OpenAI Cookbook \u5173\u4e8e\u63d0\u9ad8\u53ef\u9760\u6027\u7684\u6280\u672f\u6307\u5357\u3002 \u9664\u4e86\u7cfb\u7edf\u6d88\u606f\u4e4b\u5916\uff0c\u6e29\u5ea6\u548c\u6700\u5927\u4ee4\u724c\u662f\u5f00\u53d1\u4eba\u5458\u5fc5\u987b\u5f71\u54cd\u804a\u5929\u6a21\u578b\u8f93\u51fa\u7684\u4f17\u591a\u9009\u9879\u4e2d\u7684\u4e24\u4e2a\u3002 \u5bf9\u4e8e\u6e29\u5ea6\uff0c\u8f83\u9ad8\u7684\u503c(\u5982 0.8)\u5c06\u4f7f\u8f93\u51fa\u66f4\u968f\u673a\uff0c\u800c\u8f83\u4f4e\u7684\u503c(\u5982 0.2)\u5c06\u4f7f\u8f93\u51fa\u66f4\u96c6\u4e2d\u548c\u786e\u5b9a\u3002 \u5bf9\u4e8e max \u4ee4\u724c\uff0c\u5982\u679c\u5e0c\u671b\u5c06\u54cd\u5e94\u9650\u5236\u5728\u67d0\u4e2a\u957f\u5ea6\uff0c\u5219\u53ef\u4ee5\u5c06 max \u4ee4\u724c\u8bbe\u7f6e\u4e3a\u4efb\u610f\u6570\u5b57\u3002 \u8fd9\u53ef\u80fd\u4f1a\u5bfc\u81f4\u95ee\u9898\uff0c\u4f8b\u5982\uff0c\u5982\u679c\u60a8\u5c06\u6700\u5927\u4ee4\u724c\u503c\u8bbe\u7f6e\u4e3a 5\uff0c\u56e0\u4e3a\u8f93\u51fa\u5c06\u88ab\u5207\u65ad\uff0c\u7ed3\u679c\u5bf9\u7528\u6237\u6ca1\u6709\u610f\u4e49\u3002","title":"\u6307\u5bfc\u804a\u5929\u6a21\u578b"},{"location":"guides/chat/#_4","text":"\u56e0\u4e3a gpt-3.5-turbo \u7684\u6027\u80fd\u4e0e text-davinci-003 \u76f8\u4f3c\uff0c\u4f46\u6bcf\u4e2a\u4ee4\u724c\u7684\u4ef7\u683c\u662f\u5b83\u7684 10%\uff0c\u6240\u4ee5\u6211\u4eec\u63a8\u8350\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u4f7f\u7528 gpt-3.5-turbo \u3002 \u5bf9\u4e8e\u8bb8\u591a\u5f00\u53d1\u4eba\u5458\u6765\u8bf4\uff0c\u8f6c\u6362\u5c31\u50cf\u91cd\u5199\u548c\u91cd\u65b0\u6d4b\u8bd5\u63d0\u793a\u7b26\u4e00\u6837\u7b80\u5355\u3002 \u4f8b\u5982\uff0c\u5982\u679c\u4f7f\u7528\u4ee5\u4e0b\u8865\u5168\u63d0\u793a\u7b26\u5c06\u82f1\u8bed\u7ffb\u8bd1\u4e3a\u6cd5\u8bed: \u5c06\u4ee5\u4e0b\u82f1\u6587\u6587\u672c\u7ffb\u8bd1\u6210\u6cd5\u6587:\"{text}\" \u4e00\u4e2a\u7b49\u4ef7\u7684\u804a\u5929\u5bf9\u8bdd\u53ef\u4ee5\u662f\u8fd9\u6837\u7684: 1 2 3 4 [ { \u201crole\u201d : \u201csys te m\u201d , \u201cco ntent \u201d : \u201cYou are a help ful assis tant t ha t translates E n glish t o Fre n ch.\u201d }, { \u201crole\u201d : \u201cuser\u201d , \u201cco ntent \u201d : \u2018Tra nslate t he f ollowi n g E n glish te x t t o Fre n ch : \u201c { te x t } \u201d\u2019 } ] \u6216\u8005\u4ec5\u4ec5\u662f\u7528\u6237\u4fe1\u606f: 1 2 3 [ { \u201crole\u201d : \u201cuser\u201d , \u201cco ntent \u201d : \u2018Tra nslate t he f ollowi n g E n glish te x t t o Fre n ch : \u201c { te x t } \u201d\u2019 } ]","title":"\u804a\u5929\u4e0e\u8865\u5168"},{"location":"guides/chat/#faq","text":"\u662f\u5fae\u8c03\u53ef\u7528\u4e8e gpt-3.5-turbo? \u4e0d\u3002\u4ece2023\u5e743\u67081\u65e5\u8d77\uff0c\u60a8\u53ea\u80fd\u5fae\u8c03\u57fa\u7840GPT-3\u6a21\u578b\u3002\u6709\u5173\u5982\u4f55\u4f7f\u7528\u5fae\u8c03\u6a21\u578b\u7684\u66f4\u591a\u7ec6\u8282\uff0c\u8bf7\u53c2\u9605\u5fae\u8c03\u6307\u5357\u3002 \u662f\u5426\u5b58\u50a8\u4f20\u9012\u7ed9 API \u7684\u6570\u636e? \u81ea2023\u5e743\u67081\u65e5\u8d77\uff0c\u6211\u4eec\u5c06\u4fdd\u7559\u60a8\u7684API\u6570\u636e30\u5929\uff0c\u4f46\u4e0d\u518d\u4f7f\u7528\u60a8\u901a\u8fc7API\u53d1\u9001\u7684\u6570\u636e\u6765\u6539\u8fdb\u6211\u4eec\u7684\u6a21\u578b\u3002\u5728\u6211\u4eec\u7684\u6570\u636e\u4f7f\u7528\u653f\u7b56\u4e2d\u4e86\u89e3\u66f4\u591a\u4fe1\u606f\u3002 \u6dfb\u52a0\u8c03\u8282\u5c42 \u5982\u679c\u4f60\u60f3\u5728\u804a\u5929API\u7684\u8f93\u51fa\u4e2d\u6dfb\u52a0\u4e00\u4e2a\u5ba1\u6838\u5c42\uff0c\u4f60\u53ef\u4ee5\u6309\u7167\u6211\u4eec\u7684\u5ba1\u6838\u6307\u5357\u6765\u9632\u6b62\u8fdd\u53cdOpenAI\u4f7f\u7528\u7b56\u7565\u7684\u5185\u5bb9\u88ab\u663e\u793a\u51fa\u6765\u3002","title":"FAQ"},{"location":"guides/completion/","text":"\u6587\u672c\u8865\u5168 \u00b6 \u5b66\u4e60\u5982\u4f55\u751f\u6210\u6216\u64cd\u4f5c\u6587\u672c Introduction \u00b6 \u5b8c\u6210\u7aef\u70b9\u53ef\u7528\u4e8e\u5404\u79cd\u4efb\u52a1\u3002\u5b83\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u800c\u5f3a\u5927\u7684\u754c\u9762\uff0c\u53ef\u7528\u4e8e\u4efb\u4f55\u6211\u4eec\u7684\u6a21\u578b\u3002 You input some text as a prompt, and the model will generate a text completion that attempts to match whatever context or pattern you gave it. For example, if you give the API the prompt, \"As Descartes said, I think, therefore\", it will return the completion \" I am\" with high probability. The best way to start exploring completions is through our Playground. It's simply a text box where you can submit a prompt to generate a completion. To try it yourself, open this example in Playground: Write a tagline for an ice cream shop. Once you submit, you'll see something like this: Write a tagline for an ice cream shop. We serve up smiles with every scoop! The actual completion you see may differ because the API is non-deterministic by default. This means that you might get a slightly different completion every time you call it, even if your prompt stays the same. Setting temperature to 0 will make the outputs mostly deterministic, but a small amount of variability may remain. This simple text-in, text-out interface means you can \"program\" the model by providing instructions or just a few examples of what you'd like it to do. Its success generally depends on the complexity of the task and quality of your prompt. A good rule of thumb is to think about how you would write a word problem for a middle schooler to solve. A well-written prompt provides enough information for the model to know what you want and how it should respond. This guide covers general prompt design best practices and examples. To learn more about working with code using our Codex models, visit our code guide. Keep in mind that the default models' training data cuts off in 2021, so they may not have knowledge of current events. We plan to add more continuous training in the future. Prompt design \u00b6 Basics \u00b6 Our models can do everything from generating original stories to performing complex text analysis. Because they can do so many things, you have to be explicit in describing what you want. Showing, not just telling, is often the secret to a good prompt. There are three basic guidelines to creating prompts: Show and tell. Make it clear what you want either through instructions, examples, or a combination of the two. If you want the model to rank a list of items in alphabetical order or to classify a paragraph by sentiment, show it that's what you want. Provide quality data. If you're trying to build a classifier or get the model to follow a pattern, make sure that there are enough examples. Be sure to proofread your examples \u2014 the model is usually smart enough to see through basic spelling mistakes and give you a response, but it also might assume this is intentional and it can affect the response. Check your settings. The temperature and top_p settings control how deterministic the model is in generating a response. If you're asking it for a response where there's only one right answer, then you'd want to set these lower. If you're looking for more diverse responses, then you might want to set them higher. The number one mistake people use with these settings is assuming that they're \"cleverness\" or \"creativity\" controls. Troubleshooting \u00b6 If you're having trouble getting the API to perform as expected, follow this checklist: Is it clear what the intended generation should be? Are there enough examples? Did you check your examples for mistakes? (The API won't tell you directly) Are you using temperature and top_p correctly? Classification To create a text classifier with the API, we provide a description of the task and a few examples. In this example, we show how to classify the sentiment of Tweets. Decide whether a Tweet's sentiment is positive, neutral, or negative. Tweet: I loved the new Batman movie! Sentiment: Open in Playground \u00b6 It's worth paying attention to several features in this example: Use plain language to describe your inputs and outputs. We use plain language for the input \"Tweet\" and the expected output \"Sentiment.\" As a best practice, start with plain language descriptions. While you can often use shorthand or keys to indicate the input and output, it's best to start by being as descriptive as possible and then working backwards to remove extra words and see if performance stays consistent. Show the API how to respond to any case. In this example, we include the possible sentiment labels in our instruction. A neutral label is important because there will be many cases where even a human would have a hard time determining if something is positive or negative, and situations where it's neither. You need fewer examples for familiar tasks. For this classifier, we don't provide any examples. This is because the API already has an understanding of sentiment and the concept of a Tweet. If you're building a classifier for something the API might not be familiar with, it might be necessary to provide more examples. Improving the classifier's efficiency Now that we have a grasp of how to build a classifier, let's take that example and make it even more efficient so that we can use it to get multiple results back from one API call. Classify the sentiment in these tweets: \"I can't stand homework\" \"This sucks. I'm bored \ud83d\ude20\" \"I can't wait for Halloween!!!\" \"My cat is adorable \u2764\ufe0f\u2764\ufe0f\" \"I hate chocolate\" Tweet sentiment ratings: Open in Playground We provide a numbered list of Tweets so the API can rate five (and even more) Tweets in just one API call. It's important to note that when you ask the API to create lists or evaluate text you need to pay extra attention to your probability settings (Top P or Temperature) to avoid drift. Make sure your probability setting is calibrated correctly by running multiple tests. Don't make your list too long or the API is likely to drift. Generation \u00b6 One of the most powerful yet simplest tasks you can accomplish with the API is generating new ideas or versions of input. You can ask for anything from story ideas, to business plans, to character descriptions and marketing slogans. In this example, we'll use the API to create ideas for using virtual reality in fitness. Brainstorm some ideas combining VR and fitness: Open in Playground If needed, you can improve the quality of the responses by including some examples in your prompt. Conversation \u00b6 The API is extremely adept at carrying on conversations with humans and even with itself. With just a few lines of instruction, we've seen the API perform as a customer service chatbot that intelligently answers questions without ever getting flustered or a wise-cracking conversation partner that makes jokes and puns. The key is to tell the API how it should behave and then provide a few examples. Here's an example of the API playing the role of an AI answering questions: The following is a conversation with an AI assistant. The assistant is helpful, creative, clever, and very friendly. Human: Hello, who are you? AI: I am an AI created by OpenAI. How can I help you today? Human: Open in Playground This is all it takes to create a chatbot capable of carrying on a conversation. Underneath its simplicity, there are several things going on that are worth paying attention to: We tell the API the intent but we also tell it how to behave. Just like the other prompts, we cue the API into what the example represents, but we also add another key detail: we give it explicit instructions on how to interact with the phrase \"The assistant is helpful, creative, clever, and very friendly.\" Without that instruction the API might stray and mimic the human it's interacting with and become sarcastic or some other behavior we want to avoid. We give the API an identity. At the start we have the API respond as an AI assistant. While the API has no intrinsic identity, this helps it respond in a way that's as close to the truth as possible. You can use identity in other ways to create other kinds of chatbots. If you tell the API to respond as a woman who works as a research scientist in biology, you'll get intelligent and thoughtful comments from the API similar to what you'd expect from someone with that background. In this example we create a chatbot that is a bit sarcastic and reluctantly answers questions: Marv is a chatbot that reluctantly answers questions with sarcastic responses: You: How many pounds are in a kilogram? Marv: This again? There are 2.2 pounds in a kilogram. Please make a note of this. You: What does HTML stand for? Marv: Was Google too busy? Hypertext Markup Language. The T is for try to ask better questions in the future. You: When did the first airplane fly? Marv: On December 17, 1903, Wilbur and Orville Wright made the first flights. I wish they\u2019d come and take me away. You: What is the meaning of life? Marv: I\u2019m not sure. I\u2019ll ask my friend Google. You: Why is the sky blue? Open in Playground To create an amusing and somewhat helpful chatbot, we provide a few examples of questions and answers showing the API how to reply. All it takes is just a few sarcastic responses, and the API is able to pick up the pattern and provide an endless number of snarky responses. Transformation \u00b6 The API is a language model that is familiar with a variety of ways that words and characters can be used to express information. This ranges from natural language text to code and languages other than English. The API is also able to understand content on a level that allows it to summarize, convert and express it in different ways. Translation \u00b6 In this example we show the API how to convert from English to French, Spanish, and Japanese: Translate this into French, Spanish and Japanese: What rooms do you have available? Open in Playground \u00b6 This example works because the API already has a grasp of these languages, so there's no need to try to teach them. If you want to translate from English to a language the API is unfamiliar with, you'd need to provide it with more examples or even fine-tune a model to do it fluently. Conversion \u00b6 In this example we convert the name of a movie into emoji. This shows the adaptability of the API to picking up patterns and working with other characters. Convert movie titles into emoji. Back to the Future: \ud83d\udc68\ud83d\udc74\ud83d\ude97\ud83d\udd52 Batman: \ud83e\udd35\ud83e\udd87 Transformers: \ud83d\ude97\ud83e\udd16 Star Wars: Open in Playground Summarization The API is able to grasp the context of text and rephrase it in different ways. In this example, we create an explanation a child would understand from a longer, more sophisticated text passage. This illustrates that the API has a deep grasp of language. Summarize this for a second-grade student: Jupiter is the fifth planet from the Sun and the largest in the Solar System. It is a gas giant with a mass one-thousandth that of the Sun, but two-and-a-half times that of all the other planets in the Solar System combined. Jupiter is one of the brightest objects visible to the naked eye in the night sky, and has been known to ancient civilizations since before recorded history. It is named after the Roman god Jupiter.[19] When viewed from Earth, Jupiter can be bright enough for its reflected light to cast visible shadows,[20] and is on average the third-brightest natural object in the night sky after the Moon and Venus. Open in Playground Completion While all prompts result in completions, it can be helpful to think of text completion as its own task in instances where you want the API to pick up where you left off. For example, if given this prompt, the API will continue the train of thought about vertical farming. You can lower the temperature setting to keep the API more focused on the intent of the prompt or increase it to let it go off on a tangent. Vertical farming provides a novel solution for producing food locally, reducing transportation costs and Open in Playground This next prompt shows how you can use completion to help write React components. We send some code to the API, and it's able to continue the rest because it has an understanding of the React library. We recommend using our Codex models for tasks that involve understanding or generating code. To learn more, visit our code guide. import React from 'react'; const HeaderComponent = () => ( Open in Playground Factual responses The API has a lot of knowledge that it's learned from the data that it was been trained on. It also has the ability to provide responses that sound very real but are in fact made up. There are two ways to limit the likelihood of the API making up an answer. Provide a ground truth for the API. If you provide the API with a body of text to answer questions about (like a Wikipedia entry) it will be less likely to confabulate a response. Use a low probability and show the API how to say \"I don't know\". If the API understands that in cases where it's less certain about a response that saying \"I don't know\" or some variation is appropriate, it will be less inclined to make up answers. In this example we give the API examples of questions and answers it knows and then examples of things it wouldn't know and provide question marks. We also set the probability to zero so the API is more likely to respond with a \"?\" if there is any doubt. Q: Who is Batman? A: Batman is a fictional comic book character. Q: What is torsalplexity? A: ? Q: What is Devz9? A: ? Q: Who is George Lucas? A: George Lucas is American film director and producer famous for creating Star Wars. Q: What is the capital of California? A: Sacramento. Q: What orbits the Earth? A: The Moon. Q: Who is Fred Rickerson? A: ? Q: What is an atom? A: An atom is a tiny particle that makes up everything. Q: Who is Alvan Muntz? A: ? Q: What is Kozar-09? A: ? Q: How many moons does Mars have? A: Two, Phobos and Deimos. Q: Open in Playground Inserting text Beta \u00b6 The completions endpoint also supports inserting text within text by providing a suffix prompt in addition to the prefix prompt. This need naturally arises when writing long-form text, transitioning between paragraphs, following an outline, or guiding the model towards an ending. This also works on code, and can be used to insert in the middle of a function or file. Visit our code guide to learn more. To illustrate how important suffix context is to our ability to predict, consider the prompt, \u201cToday I decided to make a big change.\u201d There\u2019s many ways one could imagine completing the sentence. But if we now supply the ending of the story: \u201cI\u2019ve gotten many compliments on my new hair!\u201d, the intended completion becomes clear. I went to college at Boston University. After getting my degree, I decided to make a change. A big change! I packed my bags and moved to the west coast of the United States. Now, I can\u2019t get enough of the Pacific Ocean! By providing the model with additional context, it can be much more steerable. However, this is a more constrained and challenging task for the model. Best practices Inserting text is a new feature in beta and you may have to modify the way you use the API for better results. Here are a few best practices: Use max_tokens > 256. The model is better at inserting longer completions. With too small max_tokens, the model may be cut off before it's able to connect to the suffix. Note that you will only be charged for the number of tokens produced even when using larger max_tokens. Prefer finish_reason == \"stop\". When the model reaches a natural stopping point or a user provided stop sequence, it will set finish_reason as \"stop\". This indicates that the model has managed to connect to the suffix well and is a good signal for the quality of a completion. This is especially relevant for choosing between a few completions when using n > 1 or resampling (see the next point). Resample 3-5 times. While almost all completions connect to the prefix, the model may struggle to connect the suffix in harder cases. We find that resampling 3 or 5 times (or using best_of with k=3,5) and picking the samples with \"stop\" as their finish_reason can be an effective way in such cases. While resampling, you would typically want a higher temperatures to increase diversity. Note: if all the returned samples have finish_reason == \"length\", it's likely that max_tokens is too small and model runs out of tokens before it manages to connect the prompt and the suffix naturally. Consider increasing max_tokens before resampling. Try giving more clues. In some cases to better help the model\u2019s generation, you can provide clues by giving a few examples of patterns that the model can follow to decide a natural place to stop. How to make a delicious hot chocolate: Boil water Put hot chocolate in a cup Add boiling water to the cup Enjoy the hot chocolate Dogs are loyal animals. Lions are ferocious animals. Dolphins are playful animals. Horses are majestic animals. Editing text Alpha The edits endpoint can be used to edit text, rather than just completing it. You provide some text and an instruction for how to modify it, and the text-davinci-edit-001 model will attempt to edit it accordingly. This is a natural interface for translating, editing, and tweaking text. This is also useful for refactoring and working with code. Visit our code guide to learn more. During this initial beta period, usage of the edits endpoint is free. Examples INPUT GPT-3 is a very nice AI That's pretty good at writing replies When it's asked a question It gives its suggestion This is a poem it made that rhymes INSTRUCTIONS Make this in the voice of GPT-3 OUTPUT I am a very nice AI I am pretty good at writing replies When I am asked a question I give my suggestion This is a poem I made that rhymes","title":"\u6587\u672c\u8865\u5168"},{"location":"guides/completion/#_1","text":"\u5b66\u4e60\u5982\u4f55\u751f\u6210\u6216\u64cd\u4f5c\u6587\u672c","title":"\u6587\u672c\u8865\u5168"},{"location":"guides/completion/#introduction","text":"\u5b8c\u6210\u7aef\u70b9\u53ef\u7528\u4e8e\u5404\u79cd\u4efb\u52a1\u3002\u5b83\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u800c\u5f3a\u5927\u7684\u754c\u9762\uff0c\u53ef\u7528\u4e8e\u4efb\u4f55\u6211\u4eec\u7684\u6a21\u578b\u3002 You input some text as a prompt, and the model will generate a text completion that attempts to match whatever context or pattern you gave it. For example, if you give the API the prompt, \"As Descartes said, I think, therefore\", it will return the completion \" I am\" with high probability. The best way to start exploring completions is through our Playground. It's simply a text box where you can submit a prompt to generate a completion. To try it yourself, open this example in Playground: Write a tagline for an ice cream shop. Once you submit, you'll see something like this: Write a tagline for an ice cream shop. We serve up smiles with every scoop! The actual completion you see may differ because the API is non-deterministic by default. This means that you might get a slightly different completion every time you call it, even if your prompt stays the same. Setting temperature to 0 will make the outputs mostly deterministic, but a small amount of variability may remain. This simple text-in, text-out interface means you can \"program\" the model by providing instructions or just a few examples of what you'd like it to do. Its success generally depends on the complexity of the task and quality of your prompt. A good rule of thumb is to think about how you would write a word problem for a middle schooler to solve. A well-written prompt provides enough information for the model to know what you want and how it should respond. This guide covers general prompt design best practices and examples. To learn more about working with code using our Codex models, visit our code guide. Keep in mind that the default models' training data cuts off in 2021, so they may not have knowledge of current events. We plan to add more continuous training in the future.","title":"Introduction"},{"location":"guides/completion/#prompt-design","text":"","title":"Prompt design"},{"location":"guides/completion/#basics","text":"Our models can do everything from generating original stories to performing complex text analysis. Because they can do so many things, you have to be explicit in describing what you want. Showing, not just telling, is often the secret to a good prompt. There are three basic guidelines to creating prompts: Show and tell. Make it clear what you want either through instructions, examples, or a combination of the two. If you want the model to rank a list of items in alphabetical order or to classify a paragraph by sentiment, show it that's what you want. Provide quality data. If you're trying to build a classifier or get the model to follow a pattern, make sure that there are enough examples. Be sure to proofread your examples \u2014 the model is usually smart enough to see through basic spelling mistakes and give you a response, but it also might assume this is intentional and it can affect the response. Check your settings. The temperature and top_p settings control how deterministic the model is in generating a response. If you're asking it for a response where there's only one right answer, then you'd want to set these lower. If you're looking for more diverse responses, then you might want to set them higher. The number one mistake people use with these settings is assuming that they're \"cleverness\" or \"creativity\" controls.","title":"Basics"},{"location":"guides/completion/#troubleshooting","text":"If you're having trouble getting the API to perform as expected, follow this checklist: Is it clear what the intended generation should be? Are there enough examples? Did you check your examples for mistakes? (The API won't tell you directly) Are you using temperature and top_p correctly? Classification To create a text classifier with the API, we provide a description of the task and a few examples. In this example, we show how to classify the sentiment of Tweets. Decide whether a Tweet's sentiment is positive, neutral, or negative. Tweet: I loved the new Batman movie! Sentiment:","title":"Troubleshooting"},{"location":"guides/completion/#open-in-playground","text":"It's worth paying attention to several features in this example: Use plain language to describe your inputs and outputs. We use plain language for the input \"Tweet\" and the expected output \"Sentiment.\" As a best practice, start with plain language descriptions. While you can often use shorthand or keys to indicate the input and output, it's best to start by being as descriptive as possible and then working backwards to remove extra words and see if performance stays consistent. Show the API how to respond to any case. In this example, we include the possible sentiment labels in our instruction. A neutral label is important because there will be many cases where even a human would have a hard time determining if something is positive or negative, and situations where it's neither. You need fewer examples for familiar tasks. For this classifier, we don't provide any examples. This is because the API already has an understanding of sentiment and the concept of a Tweet. If you're building a classifier for something the API might not be familiar with, it might be necessary to provide more examples. Improving the classifier's efficiency Now that we have a grasp of how to build a classifier, let's take that example and make it even more efficient so that we can use it to get multiple results back from one API call. Classify the sentiment in these tweets: \"I can't stand homework\" \"This sucks. I'm bored \ud83d\ude20\" \"I can't wait for Halloween!!!\" \"My cat is adorable \u2764\ufe0f\u2764\ufe0f\" \"I hate chocolate\" Tweet sentiment ratings: Open in Playground We provide a numbered list of Tweets so the API can rate five (and even more) Tweets in just one API call. It's important to note that when you ask the API to create lists or evaluate text you need to pay extra attention to your probability settings (Top P or Temperature) to avoid drift. Make sure your probability setting is calibrated correctly by running multiple tests. Don't make your list too long or the API is likely to drift.","title":"Open in Playground"},{"location":"guides/completion/#generation","text":"One of the most powerful yet simplest tasks you can accomplish with the API is generating new ideas or versions of input. You can ask for anything from story ideas, to business plans, to character descriptions and marketing slogans. In this example, we'll use the API to create ideas for using virtual reality in fitness. Brainstorm some ideas combining VR and fitness: Open in Playground If needed, you can improve the quality of the responses by including some examples in your prompt.","title":"Generation"},{"location":"guides/completion/#conversation","text":"The API is extremely adept at carrying on conversations with humans and even with itself. With just a few lines of instruction, we've seen the API perform as a customer service chatbot that intelligently answers questions without ever getting flustered or a wise-cracking conversation partner that makes jokes and puns. The key is to tell the API how it should behave and then provide a few examples. Here's an example of the API playing the role of an AI answering questions: The following is a conversation with an AI assistant. The assistant is helpful, creative, clever, and very friendly. Human: Hello, who are you? AI: I am an AI created by OpenAI. How can I help you today? Human: Open in Playground This is all it takes to create a chatbot capable of carrying on a conversation. Underneath its simplicity, there are several things going on that are worth paying attention to: We tell the API the intent but we also tell it how to behave. Just like the other prompts, we cue the API into what the example represents, but we also add another key detail: we give it explicit instructions on how to interact with the phrase \"The assistant is helpful, creative, clever, and very friendly.\" Without that instruction the API might stray and mimic the human it's interacting with and become sarcastic or some other behavior we want to avoid. We give the API an identity. At the start we have the API respond as an AI assistant. While the API has no intrinsic identity, this helps it respond in a way that's as close to the truth as possible. You can use identity in other ways to create other kinds of chatbots. If you tell the API to respond as a woman who works as a research scientist in biology, you'll get intelligent and thoughtful comments from the API similar to what you'd expect from someone with that background. In this example we create a chatbot that is a bit sarcastic and reluctantly answers questions: Marv is a chatbot that reluctantly answers questions with sarcastic responses: You: How many pounds are in a kilogram? Marv: This again? There are 2.2 pounds in a kilogram. Please make a note of this. You: What does HTML stand for? Marv: Was Google too busy? Hypertext Markup Language. The T is for try to ask better questions in the future. You: When did the first airplane fly? Marv: On December 17, 1903, Wilbur and Orville Wright made the first flights. I wish they\u2019d come and take me away. You: What is the meaning of life? Marv: I\u2019m not sure. I\u2019ll ask my friend Google. You: Why is the sky blue? Open in Playground To create an amusing and somewhat helpful chatbot, we provide a few examples of questions and answers showing the API how to reply. All it takes is just a few sarcastic responses, and the API is able to pick up the pattern and provide an endless number of snarky responses.","title":"Conversation"},{"location":"guides/completion/#transformation","text":"The API is a language model that is familiar with a variety of ways that words and characters can be used to express information. This ranges from natural language text to code and languages other than English. The API is also able to understand content on a level that allows it to summarize, convert and express it in different ways.","title":"Transformation"},{"location":"guides/completion/#translation","text":"In this example we show the API how to convert from English to French, Spanish, and Japanese: Translate this into French, Spanish and Japanese: What rooms do you have available?","title":"Translation"},{"location":"guides/completion/#open-in-playground_1","text":"This example works because the API already has a grasp of these languages, so there's no need to try to teach them. If you want to translate from English to a language the API is unfamiliar with, you'd need to provide it with more examples or even fine-tune a model to do it fluently.","title":"Open in Playground"},{"location":"guides/completion/#conversion","text":"In this example we convert the name of a movie into emoji. This shows the adaptability of the API to picking up patterns and working with other characters. Convert movie titles into emoji. Back to the Future: \ud83d\udc68\ud83d\udc74\ud83d\ude97\ud83d\udd52 Batman: \ud83e\udd35\ud83e\udd87 Transformers: \ud83d\ude97\ud83e\udd16 Star Wars: Open in Playground Summarization The API is able to grasp the context of text and rephrase it in different ways. In this example, we create an explanation a child would understand from a longer, more sophisticated text passage. This illustrates that the API has a deep grasp of language. Summarize this for a second-grade student: Jupiter is the fifth planet from the Sun and the largest in the Solar System. It is a gas giant with a mass one-thousandth that of the Sun, but two-and-a-half times that of all the other planets in the Solar System combined. Jupiter is one of the brightest objects visible to the naked eye in the night sky, and has been known to ancient civilizations since before recorded history. It is named after the Roman god Jupiter.[19] When viewed from Earth, Jupiter can be bright enough for its reflected light to cast visible shadows,[20] and is on average the third-brightest natural object in the night sky after the Moon and Venus. Open in Playground Completion While all prompts result in completions, it can be helpful to think of text completion as its own task in instances where you want the API to pick up where you left off. For example, if given this prompt, the API will continue the train of thought about vertical farming. You can lower the temperature setting to keep the API more focused on the intent of the prompt or increase it to let it go off on a tangent. Vertical farming provides a novel solution for producing food locally, reducing transportation costs and Open in Playground This next prompt shows how you can use completion to help write React components. We send some code to the API, and it's able to continue the rest because it has an understanding of the React library. We recommend using our Codex models for tasks that involve understanding or generating code. To learn more, visit our code guide. import React from 'react'; const HeaderComponent = () => ( Open in Playground Factual responses The API has a lot of knowledge that it's learned from the data that it was been trained on. It also has the ability to provide responses that sound very real but are in fact made up. There are two ways to limit the likelihood of the API making up an answer. Provide a ground truth for the API. If you provide the API with a body of text to answer questions about (like a Wikipedia entry) it will be less likely to confabulate a response. Use a low probability and show the API how to say \"I don't know\". If the API understands that in cases where it's less certain about a response that saying \"I don't know\" or some variation is appropriate, it will be less inclined to make up answers. In this example we give the API examples of questions and answers it knows and then examples of things it wouldn't know and provide question marks. We also set the probability to zero so the API is more likely to respond with a \"?\" if there is any doubt. Q: Who is Batman? A: Batman is a fictional comic book character. Q: What is torsalplexity? A: ? Q: What is Devz9? A: ? Q: Who is George Lucas? A: George Lucas is American film director and producer famous for creating Star Wars. Q: What is the capital of California? A: Sacramento. Q: What orbits the Earth? A: The Moon. Q: Who is Fred Rickerson? A: ? Q: What is an atom? A: An atom is a tiny particle that makes up everything. Q: Who is Alvan Muntz? A: ? Q: What is Kozar-09? A: ? Q: How many moons does Mars have? A: Two, Phobos and Deimos. Q: Open in Playground","title":"Conversion"},{"location":"guides/completion/#inserting-text-beta","text":"The completions endpoint also supports inserting text within text by providing a suffix prompt in addition to the prefix prompt. This need naturally arises when writing long-form text, transitioning between paragraphs, following an outline, or guiding the model towards an ending. This also works on code, and can be used to insert in the middle of a function or file. Visit our code guide to learn more. To illustrate how important suffix context is to our ability to predict, consider the prompt, \u201cToday I decided to make a big change.\u201d There\u2019s many ways one could imagine completing the sentence. But if we now supply the ending of the story: \u201cI\u2019ve gotten many compliments on my new hair!\u201d, the intended completion becomes clear. I went to college at Boston University. After getting my degree, I decided to make a change. A big change! I packed my bags and moved to the west coast of the United States. Now, I can\u2019t get enough of the Pacific Ocean! By providing the model with additional context, it can be much more steerable. However, this is a more constrained and challenging task for the model. Best practices Inserting text is a new feature in beta and you may have to modify the way you use the API for better results. Here are a few best practices: Use max_tokens > 256. The model is better at inserting longer completions. With too small max_tokens, the model may be cut off before it's able to connect to the suffix. Note that you will only be charged for the number of tokens produced even when using larger max_tokens. Prefer finish_reason == \"stop\". When the model reaches a natural stopping point or a user provided stop sequence, it will set finish_reason as \"stop\". This indicates that the model has managed to connect to the suffix well and is a good signal for the quality of a completion. This is especially relevant for choosing between a few completions when using n > 1 or resampling (see the next point). Resample 3-5 times. While almost all completions connect to the prefix, the model may struggle to connect the suffix in harder cases. We find that resampling 3 or 5 times (or using best_of with k=3,5) and picking the samples with \"stop\" as their finish_reason can be an effective way in such cases. While resampling, you would typically want a higher temperatures to increase diversity. Note: if all the returned samples have finish_reason == \"length\", it's likely that max_tokens is too small and model runs out of tokens before it manages to connect the prompt and the suffix naturally. Consider increasing max_tokens before resampling. Try giving more clues. In some cases to better help the model\u2019s generation, you can provide clues by giving a few examples of patterns that the model can follow to decide a natural place to stop. How to make a delicious hot chocolate: Boil water Put hot chocolate in a cup Add boiling water to the cup Enjoy the hot chocolate Dogs are loyal animals. Lions are ferocious animals. Dolphins are playful animals. Horses are majestic animals. Editing text Alpha The edits endpoint can be used to edit text, rather than just completing it. You provide some text and an instruction for how to modify it, and the text-davinci-edit-001 model will attempt to edit it accordingly. This is a natural interface for translating, editing, and tweaking text. This is also useful for refactoring and working with code. Visit our code guide to learn more. During this initial beta period, usage of the edits endpoint is free. Examples INPUT GPT-3 is a very nice AI That's pretty good at writing replies When it's asked a question It gives its suggestion This is a poem it made that rhymes INSTRUCTIONS Make this in the voice of GPT-3 OUTPUT I am a very nice AI I am pretty good at writing replies When I am asked a question I give my suggestion This is a poem I made that rhymes","title":"Inserting text Beta"},{"location":"guides/editing-code/","text":"\u4ee3\u7801\u8865\u5168 Limited beta \u00b6 \u5b66\u4e60\u5982\u4f55\u751f\u6210\u6216\u64cd\u4f5c\u4ee3\u7801 Introduction \u00b6 The Codex model series is a descendant of our GPT-3 series that's been trained on both natural language and billions of lines of code. It's most capable in Python and proficient in over a dozen languages including JavaScript, Go, Perl, PHP, Ruby, Swift, TypeScript, SQL, and even Shell. During this initial limited beta period, Codex usage is free. Learn more. You can use Codex for a variety of tasks including: Turn comments into code Complete your next line or function in context Bring knowledge to you, such as finding a useful library or API call for an application Add comments Rewrite code for efficiency To see Codex in action, check out our Codex JavaScript Sandbox or our other demo videos. Codex JavaScript Sandbox This sample application uses Codex to translate natural language instructions into JavaScript. Quickstart \u00b6 To start using Codex yourself, try opening these examples in the Playground. Saying \"Hello\" (Python) 1 2 3 \"\"\" Ask the user for their name and say \"Hello\" \"\"\" Open in Playground Create random names (Python) 1 2 3 4 5 \"\"\" 1. Create a list of first names 2. Create a list of last names 3. Combine them randomly into a list of 100 full names \"\"\" Open in Playground Create a MySQL query (Python) 1 2 3 4 5 \"\"\" Table customers, columns = [CustomerId, FirstName, LastName, Company, Address, City, State, Country, PostalCode, Phone, Fax, Email, SupportRepId] Create a MySQL query for all customers in Texas named Jane \"\"\" query = Open in Playground Explaining code (JavaScript) \u00b6 1 2 3 4 5 6 7 8 9 10 11 // Function 1 var fullNames = []; for ( var i = 0 ; i < 50 ; i ++ ) { fullNames . push ( names [ Math . floor ( Math . random () * names . length )] + \" \" + lastNames [ Math . floor ( Math . random () * lastNames . length )] ); } // What does Function 1 do? Open in Playground More examples \u00b6 Visit our examples library to explore more prompts designed for Codex. Best practices \u00b6 Start with a comment, data or code. You can experiment using one of the Codex models in our playground (styling instructions as comments when needed.) To get Codex to create a useful completion it's helpful to think about what information a programmer would need to perform a task. This could simply be a clear comment or the data needed to write a useful function, like the names of variables or what class a function handles. 1 # Create a function called 'nameImporter' to add a first and last name to the database Open in Playground In this example we tell Codex what to call the function and what task it's going to perform. This approach scales even to the point where you can provide Codex with a comment and an example of a database schema to get it to write useful query requests for various databases. 1 2 3 4 5 6 7 8 9 10 11 12 13 # Table albums, columns = [AlbumId, Title, ArtistId] # Table artists, columns = [ArtistId, Name] # Table media_types, columns = [MediaTypeId, Name] # Table playlists, columns = [PlaylistId, Name] # Table playlist_track, columns = [PlaylistId, TrackId] # Table tracks, columns = [TrackId, Name, AlbumId, MediaTypeId, GenreId, Composer, Milliseconds, Bytes, UnitPrice] # Create a query for all albums by Adele Open in Playground When you show Codex the database schema it's able to make an informed guess about how to format a query. Specify the language. Codex understands dozens of different programming languages. Many share similar conventions for comments, functions and other programming syntax. By specifying the language and what version in a comment, Codex is better able to provide a completion for what you want. That said, Codex is fairly flexible with style and syntax. 1 2 3 # R language # Calculate the mean distance between an array of points Open in Playground 1 2 3 # Python 3 # Calculate the mean distance between an array of points Open in Playground Prompt Codex with what you want it to do. If you want Codex to create a webpage, placing the first line of code in an HTML document (<!DOCTYPE html>) after your comment tells Codex what it should do next. The same method works for creating a function from a comment (following the comment with a new line starting with func or def). 1 2 <!-- Create a web page with the title 'Kat Katman attorney at paw' --> <!DOCTYPE html> Open in Playground Placing <!DOCTYPE html> after our comment makes it very clear to Codex what we want it to do. 1 2 3 # Create a function to count to 100 def counter Open in Playground If we start writing the function Codex will understand what it needs to do next. Specifying libraries will help Codex understand what you want. Codex is aware of a large number of libraries, APIs and modules. By telling Codex which ones to use, either from a comment or importing them into your code, Codex will make suggestions based upon them instead of alternatives. 1 2 <!-- Use A-Frame version 1.2.0 to create a 3D website --> <!-- https://aframe.io/releases/1.2.0/aframe.min.js --> Open in Playground By specifying the version you can make sure Codex uses the most current library. Note: Codex can suggest helpful libraries and APIs, but always be sure to do your own research to make sure that they're safe for your application. Comment style can affect code quality. With some languages the style of comments can improve the quality of the output. For example, when working with Python, in some cases using doc strings (comments wrapped in triple quotes) can give higher quality results than using the pound (#) symbol. 1 2 3 \"\"\" Create an array of users and email addresses \"\"\" Open in Playground Put comments inside of functions can be helpful. Recommended coding standards usually suggest placing the description of a function inside the function. Using this format helps Codex more clearly understand what you want the function to do. 1 2 3 4 def getUserBalance(id): \"\"\" Look up the user in the database \u2018UserData' and return their current account balance. \"\"\" Open in Playground Provide examples for more precise results. If you have a particular style or format you need Codex to use, providing examples or demonstrating it in the first part of the request will help Codex more accurately match what you need. 1 2 3 4 \"\"\" Create a list of random animals and species \"\"\" animals = [ {\"name\": \"Chomper\", \"species\": \"Hamster\"}, {\"name\": Open in Playground Lower temperatures give more precise results. Setting the API temperature to 0, or close to zero (such as 0.1 or 0.2) tends to give better results in most cases. Unlike GPT-3, where a higher temperature can provide useful creative and random results, higher temperatures with Codex may give you really random or erratic responses. In cases where you need Codex to provide different potential results, start at zero and then increment upwards by .1 until you find suitable variation. Organize tasks into functions. We can get Codex to write functions by specifying what the function should do in as precise terms as possible in comment. By writing the following comment, Codex creates a Javascript timer function that's triggered when a user presses a button: A simple JavaScript timer 1 // Create a timer that creates an alert in 10 seconds Open in Playground We can use Codex to perform common tasks with well known libraries like creating a customer with the Stripe API: Create a Stripe customer in Python 1 # Create a Stripe customer from an email address Open in Playground Creating example data. Testing applications often requires using example data. Because Codgen is a language model that understands how to comprehend and write natural language, you can ask Codex to create data like arrays of made up names, products and other variables. 1 /_ Create an array of weather temperatures for San Francisco _/ Open in Playground Asking Codex to perform this task will produce a table like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 var weather = [ { month : \"January\" , high : 58 , low : 48 }, { month : \"February\" , high : 61 , low : 50 }, { month : \"March\" , high : 64 , low : 53 }, { month : \"April\" , high : 67 , low : 55 }, { month : \"May\" , high : 70 , low : 58 }, { month : \"June\" , high : 73 , low : 61 }, { month : \"July\" , high : 76 , low : 63 }, { month : \"August\" , high : 77 , low : 64 }, { month : \"September\" , high : 76 , low : 63 }, { month : \"October\" , high : 73 , low : 61 }, { month : \"November\" , high : 68 , low : 57 }, { month : \"December\" , high : 64 , low : 54 }, ]; Open in Playground Compound functions and small applications. We can provide Codex with a comment consisting of a complex request like creating a random name generator or performing tasks with user input and Codex can generate the rest provided there are enough tokens. 1 2 3 4 5 /_ Create a list of animals Create a list of cities Use the lists to generate stories about what I saw at the zoo in each city _/ Open in Playground Limit completion size for more precise results or lower latency. Requesting longer completions in Codex can lead to imprecise answers and repetition. Limit the size of the query by reducing max_tokens and setting stop tokens. For instance, add \\n as a stop sequence to limit completions to one line of code. Smaller completions also incur less latency. Use streaming to reduce latency. Large Codex queries can take tens of seconds to complete. To build applications that require lower latency, such as coding assistants that perform autocompletion, consider using streaming. Responses will be returned before the model finishes generating the entire completion. Applications that need only part of a completion can reduce latency by cutting off a completion either programmatically or by using creative values for stop. Users can combine streaming with duplication to reduce latency by requesting more than one solution from the API, and using the first response returned. Do this by setting n > 1. This approach consumes more token quota, so use carefully (e.g., by using reasonable settings for max_tokens and stop). Use Codex to explain code. Codex's ability to create and understand code allows us to use it to perform tasks like explaining what the code in a file does. One way to accomplish this is by putting a comment after a function that starts with \"This function\" or \"This application is.\" Codex will usually interpret this as the start of an explanation and complete the rest of the text. 1 /\\* Explain what the previous function is doing: It Open in Playground Explaining an SQL query. In this example we use Codex to explain in a human readable format what an SQL query is doing. 1 2 3 4 5 6 7 8 9 SELECT DISTINCT department . name FROM department JOIN employee ON department . id = employee . department_id JOIN salary_payments ON employee . id = salary_payments . employee_id WHERE salary_payments . date BETWEEN '2020-06-01' AND '2020-06-30' GROUP BY department . name HAVING COUNT ( employee . id ) > 10 ; -- Explanation of the above query in human readable format -- Open in Playground Writing unit tests. Creating a unit test can be accomplished in Python simply by adding the comment \"Unit test\" and starting a function. 1 2 3 4 5 6 7 8 # Python 3 def sum_numbers ( a , b ): return a + b # Unit test def Open in Playground Checking code for errors. By using examples, you can show Codex how to identify errors in code. In some cases no examples are required, however demonstrating the level and detail to provide a description can help Codex understand what to look for and how to explain it. (A check by Codex for errors should not replace careful review by the user. ) 1 /_ Explain why the previous function doesn't work. _/ Open in Playground Using source data to write database functions. Just as a human programmer would benefit from understanding the database structure and the column names, Codex can use this data to help you write accurate query requests. In this example we insert the schema for a database and tell Codex what to query the database for. 1 2 3 4 5 6 7 8 9 10 11 12 13 # Table albums, columns = [AlbumId, Title, ArtistId] # Table artists, columns = [ArtistId, Name] # Table media_types, columns = [MediaTypeId, Name] # Table playlists, columns = [PlaylistId, Name] # Table playlist_track, columns = [PlaylistId, TrackId] # Table tracks, columns = [TrackId, Name, AlbumId, MediaTypeId, GenreId, Composer, Milliseconds, Bytes, UnitPrice] # Create a query for all albums by Adele Open in Playground Converting between languages. You can get Codex to convert from one language to another by following a simple format where you list the language of the code you want to convert in a comment, followed by the code and then a comment with the language you want it translated into. 1 2 3 4 5 6 7 8 9 # Convert this from Python to R # Python version [ Python code ] # End # R version Open in Playground Rewriting code for a library or framework. If you want Codex to make a function more efficient, you can provide it with the code to rewrite followed by an instruction on what format to use. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // Rewrite this as a React component var input = document.createElement ( 'input' ); input.setAttribute ( 'type' , 'text' ); document.body.appendChild ( input ); var button = document.createElement ( 'button' ); button.innerHTML = 'Say Hello' ; document.body.appendChild ( button ); button.onclick = function () { var name = input.value ; var hello = document.createElement ( 'div' ); hello.innerHTML = 'Hello ' + name ; document.body.appendChild ( hello ); }; // React version : Open in Playground Inserting code Beta The completions endpoint also supports inserting code within code by providing a suffix prompt in addition to the prefix prompt. This can be used to insert a completion in the middle of a function or file. 1 2 3 4 5 6 7 def get_largest_prime_factor ( n ) : if n < 2 : return False def is_prime ( n ) : > for i in range ( 2 , n ) : > if n % i == 0: > return False > return True > largest = 1 for j in range(2, n + 1): if n % j == 0 and is_prime ( j ) : return largest By providing the model with additional context, it can be much more steerable. However, this is a more constrained and challenging task for the model. Best practices \u00b6 Inserting code is a new feature in beta and you may have to modify the way you use the API for better results. Here are a few best practices: Use max_tokens > 256. The model is better at inserting longer completions. With too small max_tokens, the model may be cut off before it's able to connect to the suffix. Note that you will only be charged for the number of tokens produced even when using larger max_tokens. Prefer finish_reason == \"stop\". When the model reaches a natural stopping point or a user provided stop sequence, it will set finish_reason as \"stop\". This indicates that the model has managed to connect to the suffix well and is a good signal for the quality of a completion. This is especially relevant for choosing between a few completions when using n > 1 or resampling (see the next point). Resample 3-5 times. While almost all completions connect to the prefix, the model may struggle to connect the suffix in harder cases. We find that resampling 3 or 5 times (or using best_of with k=3,5) and picking the samples with \"stop\" as their finish_reason can be an effective way in such cases. While resampling, you would typically want a higher temperatures to increase diversity. Note: if all the returned samples have finish_reason == \"length\", it's likely that max_tokens is too small and model runs out of tokens before it manages to connect the prompt and the suffix naturally. Consider increasing max_tokens before resampling. Editing code Beta \u00b6 The edits endpoint can be used to edit code, rather than just completing it. You provide some code and an instruction for how to modify it, and the code-davinci-edit-001 model will attempt to edit it accordingly. This is a natural interface for refactoring and tweaking code. During this initial beta period, usage of the edits endpoint is free. Examples \u00b6 Iteratively build a program \u00b6 Writing code is often an iterative process that requires refining the text along the way. Editing makes it natural to continuously refine the output of the model until the final result is polished. In this example, we use fibonacci as an example of how to iteratively build upon code. 1Write a function INPUT No input INSTRUCTIONS Write a function in python that calculates fibonacci OUTPUT 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def fibonacci ( num ) : a = 0 b = 1 if num == 1 : print ( a ) else : print ( a ) print ( b ) #the sequence starts with 0,1 for i in range ( 2 , num ) : c = a + b a = b b = c print ( c ) fibonacci ( 10 ) 2Refactor it INPUT 1 2 3 4 5 6 7 8 9 10 11 12 13 if num == 1 : print ( a ) else : print ( a ) print ( b ) #the sequence starts with 0,1 for i in range ( 2 , num ): c = a + b a = b b = c print ( c ) fibonacci ( 10 ) INSTRUCTIONS Refactor this function to be recursive OUTPUT 1 2 3 4 5 6 def fibonacci ( num ): if num <= 1 : return num else : return fib ( num - 1 ) + fib ( num - 2 ) print ( fibonacci ( 10 )) 3Rename the function INPUT 1 2 3 4 5 6 def fibonacci ( num ): if num <= 1 : return num else : return fib ( num - 1 ) + fib ( num - 2 ) print ( fibonacci ( 10 )) INSTRUCTIONS Rename the function to fib OUTPUT 1 2 3 4 5 6 def fib ( num ) : if num <= 1 : return num else : return fib ( num -1 ) + fib ( num -2 ) print ( fib ( 10 )) 4Add documentation INPUT 1 2 3 4 5 6 def fib ( num ): if num <= 1 : return num else : return fib ( num - 1 ) + fib ( num - 2 ) print ( fib ( 10 )) INSTRUCTIONS Add documentation OUTPUT 1 2 3 4 5 6 7 def fib ( num ) : \"\"\" Print the nth number in the fibonacci sequence \"\"\" if num <= 1 : return num else : return fib ( num -1 ) + fib ( num -2 ) print ( fib ( 10 )) Best practices \u00b6 The edits endpoint is still in alpha, we suggest following these best practices. Consider using an empty prompt! In this case, editing can be used similarly to completion. Be as specific with the instruction as possible. Sometimes, the model cannot find a solution and will result in an error. We suggest rewording your instruction or input.","title":"\u4ee3\u7801\u8865\u5168"},{"location":"guides/editing-code/#limited-beta","text":"\u5b66\u4e60\u5982\u4f55\u751f\u6210\u6216\u64cd\u4f5c\u4ee3\u7801","title":"\u4ee3\u7801\u8865\u5168 Limited beta"},{"location":"guides/editing-code/#introduction","text":"The Codex model series is a descendant of our GPT-3 series that's been trained on both natural language and billions of lines of code. It's most capable in Python and proficient in over a dozen languages including JavaScript, Go, Perl, PHP, Ruby, Swift, TypeScript, SQL, and even Shell. During this initial limited beta period, Codex usage is free. Learn more. You can use Codex for a variety of tasks including: Turn comments into code Complete your next line or function in context Bring knowledge to you, such as finding a useful library or API call for an application Add comments Rewrite code for efficiency To see Codex in action, check out our Codex JavaScript Sandbox or our other demo videos. Codex JavaScript Sandbox This sample application uses Codex to translate natural language instructions into JavaScript.","title":"Introduction"},{"location":"guides/editing-code/#quickstart","text":"To start using Codex yourself, try opening these examples in the Playground. Saying \"Hello\" (Python) 1 2 3 \"\"\" Ask the user for their name and say \"Hello\" \"\"\" Open in Playground Create random names (Python) 1 2 3 4 5 \"\"\" 1. Create a list of first names 2. Create a list of last names 3. Combine them randomly into a list of 100 full names \"\"\" Open in Playground Create a MySQL query (Python) 1 2 3 4 5 \"\"\" Table customers, columns = [CustomerId, FirstName, LastName, Company, Address, City, State, Country, PostalCode, Phone, Fax, Email, SupportRepId] Create a MySQL query for all customers in Texas named Jane \"\"\" query = Open in Playground","title":"Quickstart"},{"location":"guides/editing-code/#explaining-code-javascript","text":"1 2 3 4 5 6 7 8 9 10 11 // Function 1 var fullNames = []; for ( var i = 0 ; i < 50 ; i ++ ) { fullNames . push ( names [ Math . floor ( Math . random () * names . length )] + \" \" + lastNames [ Math . floor ( Math . random () * lastNames . length )] ); } // What does Function 1 do? Open in Playground","title":"Explaining code (JavaScript)"},{"location":"guides/editing-code/#more-examples","text":"Visit our examples library to explore more prompts designed for Codex.","title":"More examples"},{"location":"guides/editing-code/#best-practices","text":"Start with a comment, data or code. You can experiment using one of the Codex models in our playground (styling instructions as comments when needed.) To get Codex to create a useful completion it's helpful to think about what information a programmer would need to perform a task. This could simply be a clear comment or the data needed to write a useful function, like the names of variables or what class a function handles. 1 # Create a function called 'nameImporter' to add a first and last name to the database Open in Playground In this example we tell Codex what to call the function and what task it's going to perform. This approach scales even to the point where you can provide Codex with a comment and an example of a database schema to get it to write useful query requests for various databases. 1 2 3 4 5 6 7 8 9 10 11 12 13 # Table albums, columns = [AlbumId, Title, ArtistId] # Table artists, columns = [ArtistId, Name] # Table media_types, columns = [MediaTypeId, Name] # Table playlists, columns = [PlaylistId, Name] # Table playlist_track, columns = [PlaylistId, TrackId] # Table tracks, columns = [TrackId, Name, AlbumId, MediaTypeId, GenreId, Composer, Milliseconds, Bytes, UnitPrice] # Create a query for all albums by Adele Open in Playground When you show Codex the database schema it's able to make an informed guess about how to format a query. Specify the language. Codex understands dozens of different programming languages. Many share similar conventions for comments, functions and other programming syntax. By specifying the language and what version in a comment, Codex is better able to provide a completion for what you want. That said, Codex is fairly flexible with style and syntax. 1 2 3 # R language # Calculate the mean distance between an array of points Open in Playground 1 2 3 # Python 3 # Calculate the mean distance between an array of points Open in Playground Prompt Codex with what you want it to do. If you want Codex to create a webpage, placing the first line of code in an HTML document (<!DOCTYPE html>) after your comment tells Codex what it should do next. The same method works for creating a function from a comment (following the comment with a new line starting with func or def). 1 2 <!-- Create a web page with the title 'Kat Katman attorney at paw' --> <!DOCTYPE html> Open in Playground Placing <!DOCTYPE html> after our comment makes it very clear to Codex what we want it to do. 1 2 3 # Create a function to count to 100 def counter Open in Playground If we start writing the function Codex will understand what it needs to do next. Specifying libraries will help Codex understand what you want. Codex is aware of a large number of libraries, APIs and modules. By telling Codex which ones to use, either from a comment or importing them into your code, Codex will make suggestions based upon them instead of alternatives. 1 2 <!-- Use A-Frame version 1.2.0 to create a 3D website --> <!-- https://aframe.io/releases/1.2.0/aframe.min.js --> Open in Playground By specifying the version you can make sure Codex uses the most current library. Note: Codex can suggest helpful libraries and APIs, but always be sure to do your own research to make sure that they're safe for your application. Comment style can affect code quality. With some languages the style of comments can improve the quality of the output. For example, when working with Python, in some cases using doc strings (comments wrapped in triple quotes) can give higher quality results than using the pound (#) symbol. 1 2 3 \"\"\" Create an array of users and email addresses \"\"\" Open in Playground Put comments inside of functions can be helpful. Recommended coding standards usually suggest placing the description of a function inside the function. Using this format helps Codex more clearly understand what you want the function to do. 1 2 3 4 def getUserBalance(id): \"\"\" Look up the user in the database \u2018UserData' and return their current account balance. \"\"\" Open in Playground Provide examples for more precise results. If you have a particular style or format you need Codex to use, providing examples or demonstrating it in the first part of the request will help Codex more accurately match what you need. 1 2 3 4 \"\"\" Create a list of random animals and species \"\"\" animals = [ {\"name\": \"Chomper\", \"species\": \"Hamster\"}, {\"name\": Open in Playground Lower temperatures give more precise results. Setting the API temperature to 0, or close to zero (such as 0.1 or 0.2) tends to give better results in most cases. Unlike GPT-3, where a higher temperature can provide useful creative and random results, higher temperatures with Codex may give you really random or erratic responses. In cases where you need Codex to provide different potential results, start at zero and then increment upwards by .1 until you find suitable variation. Organize tasks into functions. We can get Codex to write functions by specifying what the function should do in as precise terms as possible in comment. By writing the following comment, Codex creates a Javascript timer function that's triggered when a user presses a button: A simple JavaScript timer 1 // Create a timer that creates an alert in 10 seconds Open in Playground We can use Codex to perform common tasks with well known libraries like creating a customer with the Stripe API: Create a Stripe customer in Python 1 # Create a Stripe customer from an email address Open in Playground Creating example data. Testing applications often requires using example data. Because Codgen is a language model that understands how to comprehend and write natural language, you can ask Codex to create data like arrays of made up names, products and other variables. 1 /_ Create an array of weather temperatures for San Francisco _/ Open in Playground Asking Codex to perform this task will produce a table like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 var weather = [ { month : \"January\" , high : 58 , low : 48 }, { month : \"February\" , high : 61 , low : 50 }, { month : \"March\" , high : 64 , low : 53 }, { month : \"April\" , high : 67 , low : 55 }, { month : \"May\" , high : 70 , low : 58 }, { month : \"June\" , high : 73 , low : 61 }, { month : \"July\" , high : 76 , low : 63 }, { month : \"August\" , high : 77 , low : 64 }, { month : \"September\" , high : 76 , low : 63 }, { month : \"October\" , high : 73 , low : 61 }, { month : \"November\" , high : 68 , low : 57 }, { month : \"December\" , high : 64 , low : 54 }, ]; Open in Playground Compound functions and small applications. We can provide Codex with a comment consisting of a complex request like creating a random name generator or performing tasks with user input and Codex can generate the rest provided there are enough tokens. 1 2 3 4 5 /_ Create a list of animals Create a list of cities Use the lists to generate stories about what I saw at the zoo in each city _/ Open in Playground Limit completion size for more precise results or lower latency. Requesting longer completions in Codex can lead to imprecise answers and repetition. Limit the size of the query by reducing max_tokens and setting stop tokens. For instance, add \\n as a stop sequence to limit completions to one line of code. Smaller completions also incur less latency. Use streaming to reduce latency. Large Codex queries can take tens of seconds to complete. To build applications that require lower latency, such as coding assistants that perform autocompletion, consider using streaming. Responses will be returned before the model finishes generating the entire completion. Applications that need only part of a completion can reduce latency by cutting off a completion either programmatically or by using creative values for stop. Users can combine streaming with duplication to reduce latency by requesting more than one solution from the API, and using the first response returned. Do this by setting n > 1. This approach consumes more token quota, so use carefully (e.g., by using reasonable settings for max_tokens and stop). Use Codex to explain code. Codex's ability to create and understand code allows us to use it to perform tasks like explaining what the code in a file does. One way to accomplish this is by putting a comment after a function that starts with \"This function\" or \"This application is.\" Codex will usually interpret this as the start of an explanation and complete the rest of the text. 1 /\\* Explain what the previous function is doing: It Open in Playground Explaining an SQL query. In this example we use Codex to explain in a human readable format what an SQL query is doing. 1 2 3 4 5 6 7 8 9 SELECT DISTINCT department . name FROM department JOIN employee ON department . id = employee . department_id JOIN salary_payments ON employee . id = salary_payments . employee_id WHERE salary_payments . date BETWEEN '2020-06-01' AND '2020-06-30' GROUP BY department . name HAVING COUNT ( employee . id ) > 10 ; -- Explanation of the above query in human readable format -- Open in Playground Writing unit tests. Creating a unit test can be accomplished in Python simply by adding the comment \"Unit test\" and starting a function. 1 2 3 4 5 6 7 8 # Python 3 def sum_numbers ( a , b ): return a + b # Unit test def Open in Playground Checking code for errors. By using examples, you can show Codex how to identify errors in code. In some cases no examples are required, however demonstrating the level and detail to provide a description can help Codex understand what to look for and how to explain it. (A check by Codex for errors should not replace careful review by the user. ) 1 /_ Explain why the previous function doesn't work. _/ Open in Playground Using source data to write database functions. Just as a human programmer would benefit from understanding the database structure and the column names, Codex can use this data to help you write accurate query requests. In this example we insert the schema for a database and tell Codex what to query the database for. 1 2 3 4 5 6 7 8 9 10 11 12 13 # Table albums, columns = [AlbumId, Title, ArtistId] # Table artists, columns = [ArtistId, Name] # Table media_types, columns = [MediaTypeId, Name] # Table playlists, columns = [PlaylistId, Name] # Table playlist_track, columns = [PlaylistId, TrackId] # Table tracks, columns = [TrackId, Name, AlbumId, MediaTypeId, GenreId, Composer, Milliseconds, Bytes, UnitPrice] # Create a query for all albums by Adele Open in Playground Converting between languages. You can get Codex to convert from one language to another by following a simple format where you list the language of the code you want to convert in a comment, followed by the code and then a comment with the language you want it translated into. 1 2 3 4 5 6 7 8 9 # Convert this from Python to R # Python version [ Python code ] # End # R version Open in Playground Rewriting code for a library or framework. If you want Codex to make a function more efficient, you can provide it with the code to rewrite followed by an instruction on what format to use. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // Rewrite this as a React component var input = document.createElement ( 'input' ); input.setAttribute ( 'type' , 'text' ); document.body.appendChild ( input ); var button = document.createElement ( 'button' ); button.innerHTML = 'Say Hello' ; document.body.appendChild ( button ); button.onclick = function () { var name = input.value ; var hello = document.createElement ( 'div' ); hello.innerHTML = 'Hello ' + name ; document.body.appendChild ( hello ); }; // React version : Open in Playground Inserting code Beta The completions endpoint also supports inserting code within code by providing a suffix prompt in addition to the prefix prompt. This can be used to insert a completion in the middle of a function or file. 1 2 3 4 5 6 7 def get_largest_prime_factor ( n ) : if n < 2 : return False def is_prime ( n ) : > for i in range ( 2 , n ) : > if n % i == 0: > return False > return True > largest = 1 for j in range(2, n + 1): if n % j == 0 and is_prime ( j ) : return largest By providing the model with additional context, it can be much more steerable. However, this is a more constrained and challenging task for the model.","title":"Best practices"},{"location":"guides/editing-code/#best-practices_1","text":"Inserting code is a new feature in beta and you may have to modify the way you use the API for better results. Here are a few best practices: Use max_tokens > 256. The model is better at inserting longer completions. With too small max_tokens, the model may be cut off before it's able to connect to the suffix. Note that you will only be charged for the number of tokens produced even when using larger max_tokens. Prefer finish_reason == \"stop\". When the model reaches a natural stopping point or a user provided stop sequence, it will set finish_reason as \"stop\". This indicates that the model has managed to connect to the suffix well and is a good signal for the quality of a completion. This is especially relevant for choosing between a few completions when using n > 1 or resampling (see the next point). Resample 3-5 times. While almost all completions connect to the prefix, the model may struggle to connect the suffix in harder cases. We find that resampling 3 or 5 times (or using best_of with k=3,5) and picking the samples with \"stop\" as their finish_reason can be an effective way in such cases. While resampling, you would typically want a higher temperatures to increase diversity. Note: if all the returned samples have finish_reason == \"length\", it's likely that max_tokens is too small and model runs out of tokens before it manages to connect the prompt and the suffix naturally. Consider increasing max_tokens before resampling.","title":"Best practices"},{"location":"guides/editing-code/#editing-code-beta","text":"The edits endpoint can be used to edit code, rather than just completing it. You provide some code and an instruction for how to modify it, and the code-davinci-edit-001 model will attempt to edit it accordingly. This is a natural interface for refactoring and tweaking code. During this initial beta period, usage of the edits endpoint is free.","title":"Editing code Beta"},{"location":"guides/editing-code/#examples","text":"","title":"Examples"},{"location":"guides/editing-code/#iteratively-build-a-program","text":"Writing code is often an iterative process that requires refining the text along the way. Editing makes it natural to continuously refine the output of the model until the final result is polished. In this example, we use fibonacci as an example of how to iteratively build upon code. 1Write a function INPUT No input INSTRUCTIONS Write a function in python that calculates fibonacci OUTPUT 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def fibonacci ( num ) : a = 0 b = 1 if num == 1 : print ( a ) else : print ( a ) print ( b ) #the sequence starts with 0,1 for i in range ( 2 , num ) : c = a + b a = b b = c print ( c ) fibonacci ( 10 ) 2Refactor it INPUT 1 2 3 4 5 6 7 8 9 10 11 12 13 if num == 1 : print ( a ) else : print ( a ) print ( b ) #the sequence starts with 0,1 for i in range ( 2 , num ): c = a + b a = b b = c print ( c ) fibonacci ( 10 ) INSTRUCTIONS Refactor this function to be recursive OUTPUT 1 2 3 4 5 6 def fibonacci ( num ): if num <= 1 : return num else : return fib ( num - 1 ) + fib ( num - 2 ) print ( fibonacci ( 10 )) 3Rename the function INPUT 1 2 3 4 5 6 def fibonacci ( num ): if num <= 1 : return num else : return fib ( num - 1 ) + fib ( num - 2 ) print ( fibonacci ( 10 )) INSTRUCTIONS Rename the function to fib OUTPUT 1 2 3 4 5 6 def fib ( num ) : if num <= 1 : return num else : return fib ( num -1 ) + fib ( num -2 ) print ( fib ( 10 )) 4Add documentation INPUT 1 2 3 4 5 6 def fib ( num ): if num <= 1 : return num else : return fib ( num - 1 ) + fib ( num - 2 ) print ( fib ( 10 )) INSTRUCTIONS Add documentation OUTPUT 1 2 3 4 5 6 7 def fib ( num ) : \"\"\" Print the nth number in the fibonacci sequence \"\"\" if num <= 1 : return num else : return fib ( num -1 ) + fib ( num -2 ) print ( fib ( 10 ))","title":"Iteratively build a program"},{"location":"guides/editing-code/#best-practices_2","text":"The edits endpoint is still in alpha, we suggest following these best practices. Consider using an empty prompt! In this case, editing can be used similarly to completion. Be as specific with the instruction as possible. Sometimes, the model cannot find a solution and will result in an error. We suggest rewording your instruction or input.","title":"Best practices"},{"location":"guides/embedding/","text":"\u5d4c\u5165 \u00b6 What are embeddings? OpenAI\u2019s text embeddings measure the relatedness of text strings. Embeddings are commonly used for: Search (where results are ranked by relevance to a query string) Clustering (where text strings are grouped by similarity) Recommendations (where items with related text strings are recommended) Anomaly detection (where outliers with little relatedness are identified) Diversity measurement (where similarity distributions are analyzed) Classification (where text strings are classified by their most similar label) An embedding is a vector (list) of floating point numbers. The distance between two vectors measures their relatedness. Small distances suggest high relatedness and large distances suggest low relatedness. Visit our pricing page to learn about Embeddings pricing. Requests are billed based on the number of tokens in the input sent. To see embeddings in action, check out our code samples Classification Topic clustering Search Recommendations How to get embeddings To get an embedding, send your text string to the embeddings API endpoint along with a choice of embedding model ID (e.g., text-embedding-ada-002). The response will contain an embedding, which you can extract, save, and use. \u793a\u4f8b :s: Example: Getting embeddings curl curl 1 2 3 4 5 curl https://api.openai.com/v1/embeddings \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer $OPENAI_API_KEY \" \\ -d '{\"input\": \"Your text string goes here\", \"model\":\"text-embedding-ada-002\"}' Example response: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 { \"data\" : [ { \"embedding\" : [ -0.006929283495992422 , -0.005336422007530928 , ... -4.547132266452536e-5 , -0.024047505110502243 ], \"index\" : 0 , \"object\" : \"embedding\" } ], \"model\" : \"text-embedding-ada-002\" , \"object\" : \"list\" , \"usage\" : { \"prompt_tokens\" : 5 , \"total_tokens\" : 5 } } See more Python code examples in the OpenAI Cookbook. When using OpenAI embeddings, please keep in mind their limitations and risks. Embedding models OpenAI offers one second-generation embedding model (denoted by -002 in the model ID) and 16 first-generation models (denoted by -001 in the model ID). We recommend using text-embedding-ada-002 for nearly all use cases. It\u2019s better, cheaper, and simpler to use. Read the blog post announcement. MODEL GENERATION TOKENIZER MAX INPUT TOKENS KNOWLEDGE CUTOFF V2 cl100k_base 8191 Sep 2021 V1 GPT-2/GPT-3 2046 Aug 2020 Usage is priced per input token, at a rate of $0.0004 per 1000 tokens, or about ~3,000 pages per US dollar (assuming ~800 tokens per page): MODEL ROUGH PAGES PER DOLLAR EXAMPLE PERFORMANCE ON BEIR SEARCH EVAL text-embedding-ada-002 3000 53.9 -davinci- -001 6 52.8 -curie- -001 60 50.9 -babbage- -001 240 50.4 -ada- -001 300 49.0 Second-generation models MODEL NAME TOKENIZER MAX INPUT TOKENS OUTPUT DIMENSIONS text-embedding-ada-002 cl100k_base 8191 1536 First-generation models (not recommended) Use cases Here we show some representative use cases. We will use the Amazon fine-food reviews dataset for the following examples. Obtaining the embeddings The dataset contains a total of 568,454 food reviews Amazon users left up to October 2012. We will use a subset of 1,000 most recent reviews for illustration purposes. The reviews are in English and tend to be positive or negative. Each review has a ProductId, UserId, Score, review title (Summary) and review body (Text). For example: PRODUCT ID USER ID SCORE SUMMARY TEXT B001E4KFG0 A3SGXH7AUHU8GW 5 Good Quality Dog Food I have bought several of the Vitality canned... B00813GRG4 A1D87F6ZCVE5NK 1 Not as Advertised Product arrived labeled as Jumbo Salted Peanut... We will combine the review summary and review text into a single combined text. The model will encode this combined text and output a single vector embedding. Obtain_dataset.ipynb 1 2 3 4 5 6 def get_embedding ( text , model = \"text-embedding-ada-002\" ): text = text . replace ( \" \\n \" , \" \" ) return openai . Embedding . create ( input = [ text ], model = model )[ 'data' ][ 0 ][ 'embedding' ] df [ 'ada_embedding' ] = df . combined . apply ( lambda x : get_embedding ( x , model = 'text-embedding-ada-002' )) df . to_csv ( 'output/embedded_1k_reviews.csv' , index = False ) To load the data from a saved file, you can run the following: 1 2 3 4 import pandas as pd df = pd . read_csv ( 'output/embedded_1k_reviews.csv' ) df [ 'ada_embedding' ] = df . ada_embedding . apply ( eval ) . apply ( np . array ) Data visualization in 2D Embedding as a text feature encoder for ML algorithms Classification using the embedding features Zero-shot classification Obtaining user and product embeddings for cold-start recommendation Clustering Text search using embeddings Code search using embeddings Recommendations using embeddings Limitations & risks Our embedding models may be unreliable or pose social risks in certain cases, and may cause harm in the absence of mitigations. Social bias Limitation: The models encode social biases, e.g. via stereotypes or negative sentiment towards certain groups. We found evidence of bias in our models via running the SEAT (May et al, 2019) and the Winogender (Rudinger et al, 2018) benchmarks. Together, these benchmarks consist of 7 tests that measure whether models contain implicit biases when applied to gendered names, regional names, and some stereotypes. For example, we found that our models more strongly associate (a) European American names with positive sentiment, when compared to African American names, and (b) negative stereotypes with black women. These benchmarks are limited in several ways: (a) they may not generalize to your particular use case, and (b) they only test for a very small slice of possible social bias. These tests are preliminary, and we recommend running tests for your specific use cases. These results should be taken as evidence of the existence of the phenomenon, not a definitive characterization of it for your use case. Please see our usage policies for more details and guidance. Please contact our support team via chat if you have any questions; we are happy to advise on this. Blindness to recent events Limitation: Models lack knowledge of events that occurred after August 2020. Our models are trained on datasets that contain some information about real world events up until 8/2020. If you rely on the models representing recent events, then they may not perform well. Frequently asked questions How can I tell how many tokens a string has before I embed it? In Python, you can split a string into tokens with OpenAI's tokenizer tiktoken. Example code: 1 2 3 4 5 6 7 8 9 import tiktoken def num_tokens_from_string ( string : str , encoding_name : str ) -> int : \"\"\"Returns the number of tokens in a text string.\"\"\" encoding = tiktoken . get_encoding ( encoding_name ) num_tokens = len ( encoding . encode ( string )) return num_tokens num_tokens_from_string ( \"tiktoken is great!\" , \"cl100k_base\" ) For second-generation embedding models like text-embedding-ada-002, use the cl100k_base encoding. More details and example code are in the OpenAI Cookbook guide how to count tokens with tiktoken. How can I retrieve K nearest embedding vectors quickly? For searching over many vectors quickly, we recommend using a vector database. You can find examples of working with vector databases and the OpenAI API in our Cookbook on GitHub. Vector database options include: Pinecone, a fully managed vector database Weaviate, an open-source vector search engine Faiss, a vector search algorithm by Facebook Redis as a vector database Qdrant, a vector search engine Typesense, open source search engine, with vector search Which distance function should I use? We recommend cosine similarity. The choice of distance function typically doesn\u2019t matter much. OpenAI embeddings are normalized to length 1, which means that: Cosine similarity can be computed slightly faster using just a dot product Cosine similarity and Euclidean distance will result in the identical rankings","title":"\u5d4c\u5165"},{"location":"guides/embedding/#_1","text":"What are embeddings? OpenAI\u2019s text embeddings measure the relatedness of text strings. Embeddings are commonly used for: Search (where results are ranked by relevance to a query string) Clustering (where text strings are grouped by similarity) Recommendations (where items with related text strings are recommended) Anomaly detection (where outliers with little relatedness are identified) Diversity measurement (where similarity distributions are analyzed) Classification (where text strings are classified by their most similar label) An embedding is a vector (list) of floating point numbers. The distance between two vectors measures their relatedness. Small distances suggest high relatedness and large distances suggest low relatedness. Visit our pricing page to learn about Embeddings pricing. Requests are billed based on the number of tokens in the input sent. To see embeddings in action, check out our code samples Classification Topic clustering Search Recommendations How to get embeddings To get an embedding, send your text string to the embeddings API endpoint along with a choice of embedding model ID (e.g., text-embedding-ada-002). The response will contain an embedding, which you can extract, save, and use. \u793a\u4f8b :s: Example: Getting embeddings curl curl 1 2 3 4 5 curl https://api.openai.com/v1/embeddings \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer $OPENAI_API_KEY \" \\ -d '{\"input\": \"Your text string goes here\", \"model\":\"text-embedding-ada-002\"}' Example response: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 { \"data\" : [ { \"embedding\" : [ -0.006929283495992422 , -0.005336422007530928 , ... -4.547132266452536e-5 , -0.024047505110502243 ], \"index\" : 0 , \"object\" : \"embedding\" } ], \"model\" : \"text-embedding-ada-002\" , \"object\" : \"list\" , \"usage\" : { \"prompt_tokens\" : 5 , \"total_tokens\" : 5 } } See more Python code examples in the OpenAI Cookbook. When using OpenAI embeddings, please keep in mind their limitations and risks. Embedding models OpenAI offers one second-generation embedding model (denoted by -002 in the model ID) and 16 first-generation models (denoted by -001 in the model ID). We recommend using text-embedding-ada-002 for nearly all use cases. It\u2019s better, cheaper, and simpler to use. Read the blog post announcement. MODEL GENERATION TOKENIZER MAX INPUT TOKENS KNOWLEDGE CUTOFF V2 cl100k_base 8191 Sep 2021 V1 GPT-2/GPT-3 2046 Aug 2020 Usage is priced per input token, at a rate of $0.0004 per 1000 tokens, or about ~3,000 pages per US dollar (assuming ~800 tokens per page): MODEL ROUGH PAGES PER DOLLAR EXAMPLE PERFORMANCE ON BEIR SEARCH EVAL text-embedding-ada-002 3000 53.9 -davinci- -001 6 52.8 -curie- -001 60 50.9 -babbage- -001 240 50.4 -ada- -001 300 49.0 Second-generation models MODEL NAME TOKENIZER MAX INPUT TOKENS OUTPUT DIMENSIONS text-embedding-ada-002 cl100k_base 8191 1536 First-generation models (not recommended) Use cases Here we show some representative use cases. We will use the Amazon fine-food reviews dataset for the following examples. Obtaining the embeddings The dataset contains a total of 568,454 food reviews Amazon users left up to October 2012. We will use a subset of 1,000 most recent reviews for illustration purposes. The reviews are in English and tend to be positive or negative. Each review has a ProductId, UserId, Score, review title (Summary) and review body (Text). For example: PRODUCT ID USER ID SCORE SUMMARY TEXT B001E4KFG0 A3SGXH7AUHU8GW 5 Good Quality Dog Food I have bought several of the Vitality canned... B00813GRG4 A1D87F6ZCVE5NK 1 Not as Advertised Product arrived labeled as Jumbo Salted Peanut... We will combine the review summary and review text into a single combined text. The model will encode this combined text and output a single vector embedding. Obtain_dataset.ipynb 1 2 3 4 5 6 def get_embedding ( text , model = \"text-embedding-ada-002\" ): text = text . replace ( \" \\n \" , \" \" ) return openai . Embedding . create ( input = [ text ], model = model )[ 'data' ][ 0 ][ 'embedding' ] df [ 'ada_embedding' ] = df . combined . apply ( lambda x : get_embedding ( x , model = 'text-embedding-ada-002' )) df . to_csv ( 'output/embedded_1k_reviews.csv' , index = False ) To load the data from a saved file, you can run the following: 1 2 3 4 import pandas as pd df = pd . read_csv ( 'output/embedded_1k_reviews.csv' ) df [ 'ada_embedding' ] = df . ada_embedding . apply ( eval ) . apply ( np . array ) Data visualization in 2D Embedding as a text feature encoder for ML algorithms Classification using the embedding features Zero-shot classification Obtaining user and product embeddings for cold-start recommendation Clustering Text search using embeddings Code search using embeddings Recommendations using embeddings Limitations & risks Our embedding models may be unreliable or pose social risks in certain cases, and may cause harm in the absence of mitigations. Social bias Limitation: The models encode social biases, e.g. via stereotypes or negative sentiment towards certain groups. We found evidence of bias in our models via running the SEAT (May et al, 2019) and the Winogender (Rudinger et al, 2018) benchmarks. Together, these benchmarks consist of 7 tests that measure whether models contain implicit biases when applied to gendered names, regional names, and some stereotypes. For example, we found that our models more strongly associate (a) European American names with positive sentiment, when compared to African American names, and (b) negative stereotypes with black women. These benchmarks are limited in several ways: (a) they may not generalize to your particular use case, and (b) they only test for a very small slice of possible social bias. These tests are preliminary, and we recommend running tests for your specific use cases. These results should be taken as evidence of the existence of the phenomenon, not a definitive characterization of it for your use case. Please see our usage policies for more details and guidance. Please contact our support team via chat if you have any questions; we are happy to advise on this. Blindness to recent events Limitation: Models lack knowledge of events that occurred after August 2020. Our models are trained on datasets that contain some information about real world events up until 8/2020. If you rely on the models representing recent events, then they may not perform well. Frequently asked questions How can I tell how many tokens a string has before I embed it? In Python, you can split a string into tokens with OpenAI's tokenizer tiktoken. Example code: 1 2 3 4 5 6 7 8 9 import tiktoken def num_tokens_from_string ( string : str , encoding_name : str ) -> int : \"\"\"Returns the number of tokens in a text string.\"\"\" encoding = tiktoken . get_encoding ( encoding_name ) num_tokens = len ( encoding . encode ( string )) return num_tokens num_tokens_from_string ( \"tiktoken is great!\" , \"cl100k_base\" ) For second-generation embedding models like text-embedding-ada-002, use the cl100k_base encoding. More details and example code are in the OpenAI Cookbook guide how to count tokens with tiktoken. How can I retrieve K nearest embedding vectors quickly? For searching over many vectors quickly, we recommend using a vector database. You can find examples of working with vector databases and the OpenAI API in our Cookbook on GitHub. Vector database options include: Pinecone, a fully managed vector database Weaviate, an open-source vector search engine Faiss, a vector search algorithm by Facebook Redis as a vector database Qdrant, a vector search engine Typesense, open source search engine, with vector search Which distance function should I use? We recommend cosine similarity. The choice of distance function typically doesn\u2019t matter much. OpenAI embeddings are normalized to length 1, which means that: Cosine similarity can be computed slightly faster using just a dot product Cosine similarity and Euclidean distance will result in the identical rankings","title":"\u5d4c\u5165"},{"location":"guides/error-codes/","text":"\u9519\u8bef\u4ee3\u7801 \u00b6 This guide includes an overview on error codes you might see from both the API and our official Python library. Each error code mentioned in the overview has a dedicated section with further guidance. API errors CODE OVERVIEW 401 - Invalid Authentication Cause: Invalid Authentication Solution: Ensure the correct API key and requesting organization are being used. 401 - Incorrect API key provided Cause: The requesting API key is not correct. Solution: Ensure the API key used is correct, clear your browser cache, or generate a new one. 401 - You must be a member of an organization to use the API Cause: Your account is not part of an organization. Solution: Contact us to get added to a new organization or ask your organization manager to invite you to an organization. 429 - Rate limit reached for requests Cause: You are sending requests too quickly. Solution: Pace your requests. Read the Rate limit guide. 429 - You exceeded your current quota, please check your plan and billing details Cause: You have hit your maximum monthly spend (hard limit) which you can view in the account billing section. Solution: Apply for a quota increase. 429 - The engine is currently overloaded, please try again later Cause: Our servers are experiencing high traffic. Solution: Please retry your requests after a brief wait. 500 - The server had an error while processing your request Cause: Issue on our servers. Solution: Retry your request after a brief wait and contact us if the issue persists. Check the status page. 401 - Invalid Authentication 401 - Incorrect API key provided 401 - You must be a member of an organization to use the API 429 - Rate limit reached for requests 429 - You exceeded your current quota, please check your plan and billing details 429 - The engine is currently overloaded, please try again later Python library error types TYPE OVERVIEW APIError Cause: Issue on our side. Solution: Retry your request after a brief wait and contact us if the issue persists. Timeout Cause: Request timed out. Solution: Retry your request after a brief wait and contact us if the issue persists. RateLimitError Cause: You have hit your assigned rate limit. Solution: Pace your requests. Read more in our Rate limit guide. APIConnectionError Cause: Issue connecting to our services. Solution: Check your network settings, proxy configuration, SSL certificates, or firewall rules. InvalidRequestError Cause: Your request was malformed or missing some required parameters, such as a token or an input. Solution: The error message should advise you on the specific error made. Check the documentation for the specific API method you are calling and make sure you are sending valid and complete parameters. You may also need to check the encoding, format, or size of your request data. AuthenticationError Cause: Your API key or token was invalid, expired, or revoked. Solution: Check your API key or token and make sure it is correct and active. You may need to generate a new one from your account dashboard. ServiceUnavailableError Cause: Issue on our servers. Solution: Retry your request after a brief wait and contact us if the issue persists. Check the status page. APIError Timeout RateLimitError APIConnectionError InvalidRequestError AuthenticationError ServiceUnavailableError Persistent errors If the issue persists, contact our support team via chat and provide them with the following information: The model you were using The error message and code you received The request data and headers you sent The timestamp and timezone of your request Any other relevant details that may help us diagnose the issue Our support team will investigate the issue and get back to you as soon as possible. Note that our support queue times may be long due to high demand. You can also post in our Community Forum but be sure to omit any sensitive information. Handling errors We advise you to programmatically handle errors returned by the API. To do so, you may want to use a code snippet like below: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 try: Make your OpenAI API request here \u00b6 response = openai.Completion.create(prompt=\"Hello world\", model=\"text-davinci-003\") except openai.error.APIError as e: Handle API error here, e.g. retry or log \u00b6 print(f\"OpenAI API returned an API Error: {e}\") pass except openai.error.APIConnectionError as e: Handle connection error here \u00b6 print(f\"Failed to connect to OpenAI API: {e}\") pass except openai.error.RateLimitError as e: Handle rate limit error (we recommend using exponential backoff) \u00b6 print(f\"OpenAI API request exceeded rate limit: {e}\") pass","title":"\u9519\u8bef\u4ee3\u7801"},{"location":"guides/error-codes/#_1","text":"This guide includes an overview on error codes you might see from both the API and our official Python library. Each error code mentioned in the overview has a dedicated section with further guidance. API errors CODE OVERVIEW 401 - Invalid Authentication Cause: Invalid Authentication Solution: Ensure the correct API key and requesting organization are being used. 401 - Incorrect API key provided Cause: The requesting API key is not correct. Solution: Ensure the API key used is correct, clear your browser cache, or generate a new one. 401 - You must be a member of an organization to use the API Cause: Your account is not part of an organization. Solution: Contact us to get added to a new organization or ask your organization manager to invite you to an organization. 429 - Rate limit reached for requests Cause: You are sending requests too quickly. Solution: Pace your requests. Read the Rate limit guide. 429 - You exceeded your current quota, please check your plan and billing details Cause: You have hit your maximum monthly spend (hard limit) which you can view in the account billing section. Solution: Apply for a quota increase. 429 - The engine is currently overloaded, please try again later Cause: Our servers are experiencing high traffic. Solution: Please retry your requests after a brief wait. 500 - The server had an error while processing your request Cause: Issue on our servers. Solution: Retry your request after a brief wait and contact us if the issue persists. Check the status page. 401 - Invalid Authentication 401 - Incorrect API key provided 401 - You must be a member of an organization to use the API 429 - Rate limit reached for requests 429 - You exceeded your current quota, please check your plan and billing details 429 - The engine is currently overloaded, please try again later Python library error types TYPE OVERVIEW APIError Cause: Issue on our side. Solution: Retry your request after a brief wait and contact us if the issue persists. Timeout Cause: Request timed out. Solution: Retry your request after a brief wait and contact us if the issue persists. RateLimitError Cause: You have hit your assigned rate limit. Solution: Pace your requests. Read more in our Rate limit guide. APIConnectionError Cause: Issue connecting to our services. Solution: Check your network settings, proxy configuration, SSL certificates, or firewall rules. InvalidRequestError Cause: Your request was malformed or missing some required parameters, such as a token or an input. Solution: The error message should advise you on the specific error made. Check the documentation for the specific API method you are calling and make sure you are sending valid and complete parameters. You may also need to check the encoding, format, or size of your request data. AuthenticationError Cause: Your API key or token was invalid, expired, or revoked. Solution: Check your API key or token and make sure it is correct and active. You may need to generate a new one from your account dashboard. ServiceUnavailableError Cause: Issue on our servers. Solution: Retry your request after a brief wait and contact us if the issue persists. Check the status page. APIError Timeout RateLimitError APIConnectionError InvalidRequestError AuthenticationError ServiceUnavailableError Persistent errors If the issue persists, contact our support team via chat and provide them with the following information: The model you were using The error message and code you received The request data and headers you sent The timestamp and timezone of your request Any other relevant details that may help us diagnose the issue Our support team will investigate the issue and get back to you as soon as possible. Note that our support queue times may be long due to high demand. You can also post in our Community Forum but be sure to omit any sensitive information. Handling errors We advise you to programmatically handle errors returned by the API. To do so, you may want to use a code snippet like below: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 try:","title":"\u9519\u8bef\u4ee3\u7801"},{"location":"guides/error-codes/#make-your-openai-api-request-here","text":"response = openai.Completion.create(prompt=\"Hello world\", model=\"text-davinci-003\") except openai.error.APIError as e:","title":"Make your OpenAI API request here"},{"location":"guides/error-codes/#handle-api-error-here-eg-retry-or-log","text":"print(f\"OpenAI API returned an API Error: {e}\") pass except openai.error.APIConnectionError as e:","title":"Handle API error here, e.g. retry or log"},{"location":"guides/error-codes/#handle-connection-error-here","text":"print(f\"Failed to connect to OpenAI API: {e}\") pass except openai.error.RateLimitError as e:","title":"Handle connection error here"},{"location":"guides/error-codes/#handle-rate-limit-error-we-recommend-using-exponential-backoff","text":"print(f\"OpenAI API request exceeded rate limit: {e}\") pass","title":"Handle rate limit error (we recommend using exponential backoff)"},{"location":"guides/fine-tuning/","text":"\u5fae\u8c03 \u00b6 \u4e86\u89e3\u5982\u4f55\u4e3a\u60a8\u7684\u5e94\u7528\u7a0b\u5e8f\u5b9a\u5236\u6a21\u578b\u3002 \u7b80\u4ecb \u00b6 \u5fae\u8c03\u53ef\u4ee5\u8ba9\u4f60\u4ece API \u63d0\u4f9b\u7684\u6a21\u578b\u4e2d\u83b7\u5f97\u66f4\u591a: \u9ad8\u8d28\u91cf\u7684\u7ed3\u679c\u6bd4\u53ca\u65f6\u7684\u8bbe\u8ba1 \u80fd\u591f\u8bad\u7ec3\u66f4\u591a\u7684\u4f8b\u5b50\uff0c\u8d85\u8fc7\u53ef\u4ee5\u5728\u63d0\u793a \u7531\u4e8e\u66f4\u77ed\u7684\u63d0\u793a\uff0c\u4ee4\u724c\u8282\u7701 \u4f4e\u5ef6\u8fdf\u8bf7\u6c42 GPT-3 \u5df2\u7ecf\u5728\u6765\u81ea\u5f00\u653e\u4e92\u8054\u7f51\u7684\u5927\u91cf\u6587\u672c\u4e0a\u8fdb\u884c\u4e86\u9884\u8bad\u7ec3\u3002 \u5f53\u7ed9\u51fa\u4e00\u4e2a\u53ea\u6709\u51e0\u4e2a\u4f8b\u5b50\u7684\u63d0\u793a\u65f6\uff0c\u5b83\u901a\u5e38\u53ef\u4ee5\u51ed\u76f4\u89c9\u5224\u65ad\u51fa\u4f60\u8981\u6267\u884c\u7684\u4efb\u52a1\uff0c\u5e76\u751f\u6210\u4e00\u4e2a\u5408\u7406\u7684\u5b8c\u6210\u7ed3\u679c\u3002 \u8fd9\u901a\u5e38\u88ab\u79f0\u4e3a\u201c\u51e0\u6b21\u5b66\u4e60\u201d\u3002 \u901a\u8fc7\u5bf9\u6bd4\u63d0\u793a\u4e2d\u9002\u5408\u7684\u66f4\u591a\u7684\u793a\u4f8b\u8fdb\u884c\u8bad\u7ec3\uff0c\u5fae\u8c03\u6539\u8fdb\u4e86\u5c11\u6570\u955c\u5934\u5b66\u4e60\uff0c\u8ba9\u60a8\u5728\u5927\u91cf\u4efb\u52a1\u4e2d\u83b7\u5f97\u66f4\u597d\u7684\u7ed3\u679c\u3002 \u4e00\u65e6\u5bf9\u6a21\u578b\u8fdb\u884c\u4e86\u5fae\u8c03\uff0c\u5c31\u4e0d\u518d\u9700\u8981\u5728\u63d0\u793a\u7b26\u4e2d\u63d0\u4f9b\u793a\u4f8b\u4e86\u3002 \u8fd9\u8282\u7701\u4e86\u6210\u672c\u5e76\u652f\u6301\u8f83\u4f4e\u7684\u5ef6\u8fdf\u8bf7\u6c42\u3002 \u5728\u9ad8\u5c42\u6b21\u4e0a\uff0c\u5fae\u8c03\u5305\u62ec\u4ee5\u4e0b\u6b65\u9aa4: \u51c6\u5907\u548c\u4e0a\u4f20\u57f9\u8bad\u6570\u636e \u8bad\u7ec3\u4e00\u4e2a\u65b0\u7684\u5fae\u8c03\u6a21\u578b \u4f7f\u7528\u60a8\u7684\u5fae\u8c03\u6a21\u578b \u8bbf\u95ee\u6211\u4eec\u7684\u5b9a\u4ef7\u9875\u9762\uff0c\u4e86\u89e3\u66f4\u591a\u5173\u4e8e\u5fae\u8c03\u6a21\u578b\u8bad\u7ec3\u548c\u4f7f\u7528\u662f\u5982\u4f55\u8ba1\u8d39\u7684\u3002 \u54ea\u4e9b\u6a21\u578b\u53ef\u4ee5\u5fae\u8c03? \u00b6 \u5fae\u8c03\u76ee\u524d\u53ea\u9002\u7528\u4e8e\u4ee5\u4e0b\u57fa\u672c\u6a21\u578b:davinci, curie, babbage, and ada. \u8fd9\u4e9b\u662f\u539f\u59cb\u6a21\u578b\uff0c\u5728\u8bad\u7ec3\u4e4b\u540e\u6ca1\u6709\u4efb\u4f55\u6307\u4ee4(\u4f8b\u5982 text-davinci-003)\u3002 \u60a8\u8fd8\u53ef\u4ee5\u7ee7\u7eed\u5bf9\u5df2\u8c03\u4f18\u7684\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u6dfb\u52a0\u989d\u5916\u7684\u6570\u636e\uff0c\u800c\u4e0d\u5fc5\u4ece\u5934\u5f00\u59cb\u3002 \u5b89\u88c5 \u00b6 \u6211\u4eec\u5efa\u8bae\u4f7f\u7528 OpenAI \u547d\u4ee4\u884c\u754c\u9762(CLI)\u3002\u8981\u5b89\u88c5\u6b64\u7a0b\u5e8f\uff0c\u8bf7\u8fd0\u884c 1 pip install --upgrade openai (\u4ee5\u4e0b\u8bf4\u660e\u9002\u7528\u4e8e 0.9.4 \u53ca\u66f4\u9ad8\u7248\u672c\u3002\u53e6\u5916\uff0cOpenAI CLI \u9700\u8981 python 3\u3002) \u901a\u8fc7\u5728\u4f60\u7684 shell \u521d\u59cb\u5316\u811a\u672c(\u4f8b\u5982 .bashrc, .zshrc \u7b49)\u4e2d\u6dfb\u52a0\u4ee5\u4e0b\u884c\u6765\u8bbe\u7f6e\u4f60\u7684 OPENAI_API_KEY \u73af\u5883\u53d8\u91cf\uff0c\u6216\u8005\u5728\u8c03\u4f18\u547d\u4ee4\u4e4b\u524d\u5728\u547d\u4ee4\u884c\u4e2d\u8fd0\u884c\u5b83: 1 export OPENAI_API_KEY = \"<OPENAI_API_KEY>\" \u51c6\u5907\u57f9\u8bad\u6570\u636e \u00b6 \u8bad\u7ec3\u6570\u636e\u662f\u4f60\u5982\u4f55\u6559 GPT-3 \u4f60\u60f3\u8ba9\u5b83\u8bf4\u4ec0\u4e48\u3002 \u60a8\u7684\u6570\u636e\u5fc5\u987b\u662f JSONL \u6587\u6863\uff0c\u5176\u4e2d\u6bcf\u4e00\u884c\u90fd\u662f\u5bf9\u5e94\u4e8e\u4e00\u4e2a\u8bad\u7ec3\u793a\u4f8b\u7684\u63d0\u793a-\u8865\u5168\u5bf9\u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528\u6211\u4eec\u7684 CLI \u6570\u636e\u51c6\u5907\u5de5\u5177\u8f7b\u677e\u5730\u5c06\u6570\u636e\u8f6c\u6362\u4e3a\u8fd9\u79cd\u6587\u4ef6\u683c\u5f0f\u3002 1 2 3 4 { \"prompt\" : \"<prompt text>\" , \"completion\" : \"<ideal generated text>\" } { \"prompt\" : \"<prompt text>\" , \"completion\" : \"<ideal generated text>\" } { \"prompt\" : \"<prompt text>\" , \"completion\" : \"<ideal generated text>\" } ... \u8bbe\u8ba1\u7528\u4e8e\u5fae\u8c03\u7684\u63d0\u793a\u7b26\u548c\u8865\u5168\u4e0e\u8bbe\u8ba1\u7528\u4e8e\u57fa\u672c\u6a21\u578b(Davinci, Curie, Babbage, Ada)\u7684\u63d0\u793a\u7b26\u4e0d\u540c\u3002 \u7279\u522b\u662f\uff0c\u867d\u7136\u57fa\u672c\u6a21\u578b\u7684\u63d0\u793a\u901a\u5e38\u7531\u591a\u4e2a\u793a\u4f8b\u7ec4\u6210(\u201c\u5c11\u955c\u5934\u5b66\u4e60\u201d)\uff0c\u4f46\u4e3a\u4e86\u8fdb\u884c\u5fae\u8c03\uff0c\u6bcf\u4e2a\u8bad\u7ec3\u793a\u4f8b\u901a\u5e38\u7531\u5355\u4e2a\u8f93\u5165\u793a\u4f8b\u53ca\u5176\u76f8\u5173\u8f93\u51fa\u7ec4\u6210\uff0c\u800c\u4e0d\u9700\u8981\u7ed9\u51fa\u8be6\u7ec6\u7684\u8bf4\u660e\u6216\u5728\u540c\u4e00\u63d0\u793a\u4e2d\u5305\u542b\u591a\u4e2a\u793a\u4f8b\u3002 \u6709\u5173\u5982\u4f55\u4e3a\u5404\u79cd\u4efb\u52a1\u51c6\u5907\u8bad\u7ec3\u6570\u636e\u7684\u66f4\u8be6\u7ec6\u6307\u5bfc\uff0c\u8bf7\u53c2\u9605\u6211\u4eec\u7684\u51c6\u5907\u6570\u636e\u96c6\u6700\u4f73\u5b9e\u8df5\u3002 \u8bad\u7ec3\u7684\u4f8b\u5b50\u8d8a\u591a\u8d8a\u597d\u3002\u6211\u4eec\u5efa\u8bae\u81f3\u5c11\u6709\u51e0\u767e\u4e2a\u4f8b\u5b50\u3002\u4e00\u822c\u6765\u8bf4\uff0c\u6211\u4eec\u53d1\u73b0\u6570\u636e\u96c6\u5927\u5c0f\u6bcf\u589e\u52a0\u4e00\u500d\uff0c\u5c31\u4f1a\u5bfc\u81f4\u6a21\u578b\u8d28\u91cf\u7684\u7ebf\u6027\u589e\u957f\u3002 CLI \u6570\u636e\u51c6\u5907\u5de5\u5177 \u00b6 \u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u5de5\u5177\uff0c\u53ef\u4ee5\u9a8c\u8bc1\u3001\u63d0\u4f9b\u5efa\u8bae\u5e76\u91cd\u65b0\u683c\u5f0f\u5316\u60a8\u7684\u6570\u636e: 1 openai tools fine_tunes.prepare_data -f <LOCAL_FILE> \u8be5\u5de5\u5177\u63a5\u53d7\u4e0d\u540c\u7684\u683c\u5f0f\uff0c\u552f\u4e00\u7684\u8981\u6c42\u662f\u5b83\u4eec\u5305\u542b\u63d0\u793a\u7b26\u548c\u5b8c\u6210\u5217/\u952e\u3002 \u60a8\u53ef\u4ee5\u4f20\u9012\u4e00\u4e2a CSV\u3001TSV\u3001XLSX\u3001JSON \u6216 JSONL \u6587\u4ef6\uff0c\u5728\u6307\u5bfc\u60a8\u5b8c\u6210\u5efa\u8bae\u7684\u66f4\u6539\u8fc7\u7a0b\u540e\uff0c\u5b83\u4f1a\u5c06\u8f93\u51fa\u4fdd\u5b58\u5230\u4e00\u4e2a JSONL \u6587\u4ef6\u4e2d\uff0c\u4ee5\u4fbf\u8fdb\u884c\u5fae\u8c03\u3002 \u521b\u5efa\u4e00\u4e2a\u5fae\u8c03\u6a21\u578b \u00b6 \u4e0b\u9762\u5047\u8bbe\u4f60\u5df2\u7ecf\u6309\u7167\u4e0a\u9762\u7684\u8bf4\u660e\u51c6\u5907\u597d\u4e86\u8bad\u7ec3\u6570\u636e\u3002 \u4f7f\u7528 OpenAI \u547d\u4ee4\u884c\u5f00\u59cb\u4f60\u7684\u8c03\u4f18\u5de5\u4f5c: 1 openai api fine_tunes.create -t <TRAIN_FILE_ID_OR_PATH> -m <BASE_MODEL> \u5176\u4e2d BASE_MODEL \u662f\u60a8\u5f00\u59cb\u7684\u57fa\u672c\u6a21\u578b\u7684\u540d\u79f0(ada, babbage, curie \u6216 davinci)\u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528 suffix \u53c2\u6570\u81ea\u5b9a\u4e49\u7ecf\u8fc7\u5fae\u8c03\u7684\u6a21\u578b\u7684\u540d\u79f0\u3002 \u8fd0\u884c\u4e0a\u9762\u7684\u547d\u4ee4\u4f1a\u505a\u51e0\u4ef6\u4e8b: \u4f7f\u7528 files API \u4e0a\u4f20\u6587\u4ef6(\u6216\u4f7f\u7528\u5df2\u7ecf\u4e0a\u4f20\u7684\u6587\u4ef6) \u521b\u5efa\u5fae\u8c03\u4f5c\u4e1a \u6d41\u4e8b\u4ef6\u76f4\u5230\u4f5c\u4e1a\u5b8c\u6210(\u8fd9\u901a\u5e38\u9700\u8981\u51e0\u5206\u949f\uff0c\u4f46\u5982\u679c\u961f\u5217\u4e2d\u6709\u5f88\u591a\u4f5c\u4e1a\u6216\u6570\u636e\u96c6\u5f88\u5927\uff0c\u5219\u53ef\u80fd\u9700\u8981\u51e0\u4e2a\u5c0f\u65f6) \u6bcf\u4e2a\u5fae\u8c03\u5de5\u4f5c\u90fd\u4ece\u4e00\u4e2a\u57fa\u672c\u6a21\u578b\u5f00\u59cb\uff0c\u9ed8\u8ba4\u4e3a curie \u6a21\u578b\u3002 \u6a21\u578b\u7684\u9009\u62e9\u65e2\u5f71\u54cd\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e5f\u5f71\u54cd\u8fd0\u884c\u7ecf\u8fc7\u5fae\u8c03\u7684\u6a21\u578b\u7684\u6210\u672c\u3002 \u4f60\u7684\u6a21\u578b\u53ef\u4ee5\u662f ada, babbage, curie, or davinci\u3002 \u8bf7\u8bbf\u95ee\u6211\u4eec\u7684\u5b9a\u4ef7\u9875\u9762\u4e86\u89e3\u5fae\u8c03\u8d39\u7387\u7684\u8be6\u7ec6\u4fe1\u606f\u3002 \u5728\u5f00\u59cb\u4e00\u9879\u5fae\u8c03\u5de5\u4f5c\u4e4b\u540e\uff0c\u53ef\u80fd\u9700\u8981\u4e00\u4e9b\u65f6\u95f4\u624d\u80fd\u5b8c\u6210\u3002 \u4f60\u7684\u4f5c\u4e1a\u53ef\u80fd\u6392\u5728\u6211\u4eec\u7cfb\u7edf\u4e0a\u5176\u4ed6\u4f5c\u4e1a\u7684\u540e\u9762\uff0c\u8bad\u7ec3\u6211\u4eec\u7684\u6a21\u578b\u53ef\u80fd\u9700\u8981\u51e0\u5206\u949f\u6216\u51e0\u5c0f\u65f6\uff0c\u8fd9\u53d6\u51b3\u4e8e\u6a21\u578b\u548c\u6570\u636e\u96c6\u5927\u5c0f\u3002 \u5982\u679c\u4e8b\u4ef6\u6d41\u56e0\u4efb\u4f55\u539f\u56e0\u4e2d\u65ad\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6062\u590d\u5b83: 1 openai api fine_tunes.follow -i <YOUR_FINE_TUNE_JOB_ID> \u5f53\u5de5\u4f5c\u5b8c\u6210\u65f6\uff0c\u5b83\u5e94\u8be5\u663e\u793a\u7ecf\u8fc7\u5fae\u8c03\u7684\u6a21\u578b\u7684\u540d\u79f0\u3002 \u9664\u4e86\u521b\u5efa\u5fae\u8c03\u4f5c\u4e1a\u5916\uff0c\u60a8\u8fd8\u53ef\u4ee5\u5217\u51fa\u73b0\u6709\u4f5c\u4e1a\u3001\u68c0\u7d22\u4f5c\u4e1a\u7684\u72b6\u6001\u6216\u53d6\u6d88\u4f5c\u4e1a\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # List all created fine-tunes openai api fine_tunes.list # Retrieve the state of a fine-tune. The resulting object includes # job status (which can be one of pending, running, succeeded, or failed) # and other information openai api fine_tunes.get -i <YOUR_FINE_TUNE_JOB_ID> # Cancel a job openai api fine_tunes.cancel -i <YOUR_FINE_TUNE_JOB_ID> \u4f7f\u7528\u7ecf\u8fc7\u5fae\u8c03\u7684\u6a21\u578b \u00b6 \u5f53\u4f5c\u4e1a\u6210\u529f\u65f6\uff0cfine_tuned_model \u5b57\u6bb5\u5c06\u586b\u5145\u6a21\u578b\u7684\u540d\u79f0\u3002\u73b0\u5728\u4f60\u53ef\u4ee5\u5c06\u8fd9\u4e2a\u6a21\u578b\u6307\u5b9a\u4e3a Completions API \u7684\u53c2\u6570\uff0c\u5e76\u4f7f\u7528 Playground \u5411\u5b83\u53d1\u51fa\u8bf7\u6c42\u3002 \u5728\u60a8\u7684\u4efb\u52a1\u7b2c\u4e00\u6b21\u5b8c\u6210\u540e\uff0c\u60a8\u7684\u6a21\u578b\u53ef\u80fd\u9700\u8981\u51e0\u5206\u949f\u624d\u80fd\u51c6\u5907\u597d\u5904\u7406\u8bf7\u6c42\u3002\u5982\u679c\u5bf9\u6a21\u578b\u7684\u5b8c\u6210\u8bf7\u6c42\u8d85\u65f6\uff0c\u5f88\u53ef\u80fd\u662f\u56e0\u4e3a\u6a21\u578b\u4ecd\u5728\u52a0\u8f7d\u4e2d\u3002\u5982\u679c\u53d1\u751f\u8fd9\u79cd\u60c5\u51b5\uff0c\u51e0\u5206\u949f\u540e\u518d\u8bd5\u4e00\u6b21\u3002 \u4f60\u53ef\u4ee5\u901a\u8fc7\u4f20\u9012\u6a21\u578b\u540d\u4f5c\u4e3a\u5b8c\u6210\u8bf7\u6c42\u7684\u6a21\u578b\u53c2\u6570\u6765\u5f00\u59cb\u8bf7\u6c42: OpenAI CLI: 1 openai api completions.create -m <FINE_TUNED_MODEL> -p <YOUR_PROMPT> cURL: 1 2 3 4 curl https://api.openai.com/v1/completions \\ -H \"Authorization: Bearer $OPENAI_API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{\"prompt\": YOUR_PROMPT, \"model\": FINE_TUNED_MODEL}' Python: 1 2 3 4 import openai openai . Completion . create ( model = FINE_TUNED_MODEL , prompt = YOUR_PROMPT ) Node.js: 1 2 3 4 const response = await openai . createCompletion ({ model : FINE_TUNED_MODEL prompt : YOUR_PROMPT , }); \u4f60\u53ef\u4ee5\u7ee7\u7eed\u4f7f\u7528\u6240\u6709\u5176\u4ed6\u7684 Completions \u53c2\u6570\uff0c\u6bd4\u5982 temperature, frequency_penalty, presence_penalty \u7b49\uff0c\u5bf9\u8fd9\u4e9b\u8bf7\u6c42\u8fdb\u884c\u5fae\u8c03\u6a21\u578b\u3002 \u5220\u9664\u4e00\u4e2a\u7ecf\u8fc7\u5fae\u8c03\u7684\u6a21\u578b \u00b6 \u8981\u5220\u9664\u4e00\u4e2a\u7ecf\u8fc7\u5fae\u8c03\u7684\u6a21\u578b\uff0c\u60a8\u5fc5\u987b\u5728\u7ec4\u7ec7\u4e2d\u6307\u5b9a\u4e00\u4e2a\u201c\u6240\u6709\u8005\u201d\u3002 OpenAI CLI: 1 openai api models.delete -i <FINE_TUNED_MODEL> cURL: 1 2 curl -X \"DELETE\" https://api.openai.com/v1/models/<FINE_TUNED_MODEL> \\ -H \"Authorization: Bearer $OPENAI_API_KEY \" Python: 1 2 import openai openai . Model . delete ( FINE_TUNED_MODEL ) \u51c6\u5907\u6570\u636e\u96c6 \u00b6 \u5fae\u8c03\u662f\u521b\u5efa\u7279\u5b9a\u4e8e\u7528\u4f8b\u7684\u65b0\u6a21\u578b\u7684\u5f3a\u5927\u6280\u672f\u3002\u5728\u5bf9\u60a8\u7684\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u4e4b\u524d\uff0c\u6211\u4eec\u5f3a\u70c8\u5efa\u8bae\u60a8\u9605\u8bfb\u4ee5\u4e0b\u7528\u4f8b\u7684\u6700\u4f73\u5b9e\u8df5\u548c\u5177\u4f53\u6307\u5357\u3002 \u6570\u636e\u683c\u5f0f \u00b6 \u8981\u5bf9\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u60a8\u9700\u8981\u4e00\u7ec4\u8bad\u7ec3\u793a\u4f8b\uff0c\u6bcf\u4e2a\u793a\u4f8b\u7531\u5355\u4e2a\u8f93\u5165(\u201c\u63d0\u793a\u201d)\u53ca\u5176\u76f8\u5173\u8f93\u51fa(\u201c\u5b8c\u6210\u201d)\u7ec4\u6210\u3002\u8fd9\u4e0e\u4f7f\u7528\u6211\u4eec\u7684\u57fa\u672c\u6a21\u578b\u660e\u663e\u4e0d\u540c\uff0c\u5728\u57fa\u672c\u6a21\u578b\u4e2d\uff0c\u60a8\u53ef\u4ee5\u5728\u4e00\u4e2a\u63d0\u793a\u7b26\u4e2d\u8f93\u5165\u8be6\u7ec6\u7684\u8bf4\u660e\u6216\u591a\u4e2a\u793a\u4f8b\u3002 \u6bcf\u4e2a\u63d0\u793a\u7b26\u5e94\u8be5\u4ee5\u4e00\u4e2a\u56fa\u5b9a\u7684\u5206\u9694\u7b26\u7ed3\u675f\uff0c\u4ee5\u901a\u77e5\u6a21\u578b\u63d0\u793a\u7b26\u4f55\u65f6\u7ed3\u675f\u548c\u5b8c\u6210\u5f00\u59cb\u3002\u4e00\u4e2a\u7b80\u5355\u7684\u5206\u79bb\u5668\u901a\u5e38\u5de5\u4f5c\u5f97\u5f88\u597d\u662f \\n\\n###\\n\\n \u3002\u5206\u9694\u7b26\u4e0d\u5e94\u8be5\u51fa\u73b0\u5728\u4efb\u4f55\u63d0\u793a\u7b26\u7684\u5176\u4ed6\u5730\u65b9\u3002 \u7531\u4e8e\u6211\u4eec\u7684\u6807\u8bb0\u5316\uff0c\u6bcf\u4e2a\u8865\u5168\u90fd\u5e94\u8be5\u4ee5\u7a7a\u767d\u5f00\u59cb\uff0c\u6807\u8bb0\u5316\u5927\u591a\u6570\u5355\u8bcd\u4e4b\u524d\u90fd\u6709\u7a7a\u767d\u3002 \u6bcf\u4e2a\u8865\u5168\u5e94\u8be5\u4ee5\u56fa\u5b9a\u7684\u505c\u6b62\u5e8f\u5217\u7ed3\u675f\uff0c\u4ee5\u4fbf\u5728\u8865\u5168\u7ed3\u675f\u65f6\u901a\u77e5\u6a21\u578b\u3002\u505c\u6b62\u5e8f\u5217\u53ef\u4ee5\u662f \\n \u3001 ### \u6216\u4efb\u4f55\u4e0d\u51fa\u73b0\u5728\u4efb\u4f55\u8865\u5168\u4e2d\u7684\u5176\u4ed6\u6807\u8bb0\u3002 \u5bf9\u4e8e\u63a8\u65ad\uff0c\u60a8\u5e94\u8be5\u6309\u7167\u4e0e\u521b\u5efa\u8bad\u7ec3\u6570\u636e\u96c6\u65f6\u76f8\u540c\u7684\u65b9\u5f0f\u683c\u5f0f\u5316\u63d0\u793a\uff0c\u5305\u62ec\u76f8\u540c\u7684\u5206\u9694\u7b26\u3002\u8fd8\u8981\u6307\u5b9a\u76f8\u540c\u7684\u505c\u6b62\u5e8f\u5217\u4ee5\u6b63\u786e\u5730\u622a\u65ad\u8865\u5168\u3002 \u4e00\u822c\u6700\u4f73\u5b9e\u8df5 \u00b6 \u4f7f\u7528\u66f4\u591a\u9ad8\u8d28\u91cf\u7684\u793a\u4f8b\u8fdb\u884c\u5fae\u8c03\u6548\u679c\u66f4\u597d\u3002\u8981\u5bf9\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u4f7f\u5176\u6bd4\u4f7f\u7528\u6211\u4eec\u7684\u57fa\u672c\u6a21\u578b\u7684\u9ad8\u8d28\u91cf\u63d0\u793a\u6267\u884c\u5f97\u66f4\u597d\uff0c\u60a8\u5e94\u8be5\u81f3\u5c11\u63d0\u4f9b\u51e0\u767e\u4e2a\u9ad8\u8d28\u91cf\u793a\u4f8b\uff0c\u6700\u597d\u662f\u7531\u4eba\u7c7b\u4e13\u5bb6\u5ba1\u67e5\u3002\u4ece\u8fd9\u91cc\u5f00\u59cb\uff0c\u6027\u80fd\u4f1a\u968f\u7740\u793a\u4f8b\u6570\u91cf\u6bcf\u589e\u52a0\u4e00\u500d\u800c\u7ebf\u6027\u589e\u52a0\u3002\u589e\u52a0\u793a\u4f8b\u7684\u6570\u91cf\u901a\u5e38\u662f\u63d0\u9ad8\u6027\u80fd\u7684\u6700\u4f73\u548c\u6700\u53ef\u9760\u7684\u65b9\u6cd5\u3002 \u5206\u7c7b\u5668\u662f\u6700\u5bb9\u6613\u5165\u95e8\u7684\u6a21\u578b\u3002\u5bf9\u4e8e\u5206\u7c7b\u95ee\u9898\uff0c\u6211\u4eec\u5efa\u8bae\u4f7f\u7528 ada\uff0c\u5b83\u5728\u7ecf\u8fc7\u5fae\u8c03\u540e\u7684\u8868\u73b0\u901a\u5e38\u53ea\u6bd4\u66f4\u6709\u80fd\u529b\u7684\u6a21\u578b\u5dee\u4e00\u70b9\u70b9\uff0c\u540c\u65f6\u901f\u5ea6\u66f4\u5feb\uff0c\u6210\u672c\u66f4\u4f4e\u3002 \u5982\u679c\u60a8\u6b63\u5728\u5bf9\u5df2\u6709\u7684\u6570\u636e\u96c6\u8fdb\u884c\u5fae\u8c03\uff0c\u800c\u4e0d\u662f\u4ece\u5934\u5f00\u59cb\u7f16\u5199\u63d0\u793a\uff0c\u8bf7\u52a1\u5fc5\u5728\u53ef\u80fd\u7684\u60c5\u51b5\u4e0b\u624b\u52a8\u68c0\u67e5\u6570\u636e\u4e2d\u7684\u5192\u72af\u6027\u6216\u4e0d\u51c6\u786e\u5185\u5bb9\uff0c\u6216\u8005\u5728\u6570\u636e\u96c6\u5f88\u5927\u7684\u60c5\u51b5\u4e0b\u68c0\u67e5\u5c3d\u53ef\u80fd\u591a\u7684\u968f\u673a\u6837\u672c\u3002 \u5177\u4f53\u7684\u6307\u5bfc \u00b6 \u5fae\u8c03\u53ef\u4ee5\u89e3\u51b3\u5404\u79cd\u5404\u6837\u7684\u95ee\u9898\uff0c\u4f7f\u7528\u5b83\u7684\u6700\u4f73\u65b9\u5f0f\u53ef\u80fd\u53d6\u51b3\u4e8e\u60a8\u7684\u7279\u5b9a\u7528\u4f8b\u3002\u4e0b\u9762\uff0c\u6211\u4eec\u5217\u51fa\u4e86\u7528\u4e8e\u5fae\u8c03\u7684\u6700\u5e38\u89c1\u7528\u4f8b\u548c\u76f8\u5e94\u7684\u6307\u5bfc\u65b9\u9488\u3002 \u5206\u7c7b \u6a21\u7279\u662f\u5426\u505a\u4e86\u4e0d\u771f\u5b9e\u7684\u9648\u8ff0? \u60c5\u7eea\u5206\u6790 \u7535\u5b50\u90ae\u4ef6\u5206\u7c7b \u6709\u6761\u4ef6\u7684\u4ee3 \u6839\u636e\u7ef4\u57fa\u767e\u79d1\u4e0a\u7684\u6587\u7ae0\u5199\u4e00\u4e2a\u5438\u5f15\u4eba\u7684\u5e7f\u544a \u5b9e\u4f53\u63d0\u53d6 \u5ba2\u6237\u652f\u6301\u804a\u5929\u673a\u5668\u4eba \u57fa\u4e8e\u6280\u672f\u7279\u6027\u8868\u7684\u4ea7\u54c1\u63cf\u8ff0 \u5206\u7c7b \u00b6 \u5728\u5206\u7c7b\u95ee\u9898\u4e2d\uff0c\u63d0\u793a\u7b26\u4e2d\u7684\u6bcf\u4e2a\u8f93\u5165\u90fd\u5e94\u8be5\u88ab\u5206\u7c7b\u5230\u9884\u5b9a\u4e49\u7684\u7c7b\u4e2d\u3002\u5bf9\u4e8e\u8fd9\u7c7b\u95ee\u9898\uff0c\u6211\u4eec\u5efa\u8bae: \u5728\u63d0\u793a\u7b26\u7684\u672b\u5c3e\u4f7f\u7528\u5206\u9694\u7b26\uff0c\u4f8b\u5982 \\n\\n###\\n\\n \u3002\u8bb0\u4f4f\uff0c\u5f53\u4f60\u6700\u7ec8\u5411\u4f60\u7684\u6a21\u578b\u53d1\u51fa\u8bf7\u6c42\u65f6\uff0c\u4e5f\u8981\u9644\u52a0\u8fd9\u4e2a\u5206\u9694\u7b26\u3002 \u9009\u62e9\u6620\u5c04\u5230\u5355\u4e2a\u4ee4\u724c\u7684\u7c7b\u3002\u5728\u63a8\u7406\u65f6\uff0c\u6307\u5b9a max_tokens=1\uff0c\u56e0\u4e3a\u60a8\u53ea\u9700\u8981\u7b2c\u4e00\u4e2a\u6807\u8bb0\u8fdb\u884c\u5206\u7c7b\u3002 \u786e\u4fdd\u63d0\u793a\u7b26+\u8865\u5168\u4e0d\u8d85\u8fc7 2048 \u4e2a\u4ee4\u724c\uff0c\u5305\u62ec\u5206\u9694\u7b26 \u76ee\u6807\u662f\u6bcf\u7c7b\u81f3\u5c11 100 \u4e2a\u4f8b\u5b50 \u8981\u83b7\u5f97\u7c7b\u65e5\u5fd7\u6982\u7387\uff0c\u60a8\u53ef\u4ee5\u5728\u4f7f\u7528\u6a21\u578b\u65f6\u6307\u5b9a logprobs=5(\u7528\u4e8e 5 \u4e2a\u7c7b) \u786e\u4fdd\u7528\u4e8e\u5fae\u8c03\u7684\u6570\u636e\u96c6\u5728\u7ed3\u6784\u548c\u4efb\u52a1\u7c7b\u578b\u4e0a\u4e0e\u5c06\u7528\u4e8e\u6a21\u578b\u7684\u6570\u636e\u96c6\u975e\u5e38\u76f8\u4f3c \u6848\u4f8b\u7814\u7a76:\u6a21\u578b\u662f\u5426\u505a\u4e86\u4e0d\u771f\u5b9e\u7684\u9648\u8ff0? \u00b6 \u5047\u8bbe\u4f60\u60f3\u786e\u4fdd\u4f60\u7f51\u7ad9\u4e0a\u7684\u5e7f\u544a\u6587\u672c\u63d0\u5230\u4e86\u6b63\u786e\u7684\u4ea7\u54c1\u548c\u516c\u53f8\u3002\u6362\u53e5\u8bdd\u8bf4\uff0c\u60a8\u5e0c\u671b\u786e\u4fdd\u6a21\u578b\u6ca1\u6709\u7f16\u9020\u4efb\u4f55\u4e1c\u897f\u3002\u4f60\u53ef\u80fd\u60f3\u8981\u5fae\u8c03\u4e00\u4e2a\u5206\u7c7b\u5668\u6765\u8fc7\u6ee4\u6389\u4e0d\u6b63\u786e\u7684\u5e7f\u544a\u3002 \u6570\u636e\u96c6\u53ef\u80fd\u770b\u8d77\u6765\u50cf\u4e0b\u9762\u8fd9\u6837: 1 2 {\"prompt\":\"Company: BHFF insurance\\nProduct: allround insurance\\nAd:One stop shop for all your insurance needs!\\nSupported:\", \"completion\":\" yes\"} {\"prompt\":\"Company: Loft conversion specialists\\nProduct: -\\nAd:Straight teeth in weeks!\\nSupported:\", \"completion\":\" no\"} \u5728\u4e0a\u9762\u7684\u4f8b\u5b50\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528\u4e86\u4e00\u4e2a\u5305\u542b\u516c\u53f8\u540d\u79f0\u3001\u4ea7\u54c1\u548c\u76f8\u5173\u5e7f\u544a\u7684\u7ed3\u6784\u5316\u8f93\u5165\u3002\u6211\u4eec\u4f7f\u7528 \\nSupported: \u4f5c\u4e3a\u5206\u9694\u7b26\uff0c\u5b83\u6e05\u695a\u5730\u5c06\u63d0\u793a\u7b26\u4e0e\u5b8c\u6210\u7b26\u5206\u5f00\u3002\u5bf9\u4e8e\u8db3\u591f\u591a\u7684\u793a\u4f8b\uff0c\u53ea\u8981\u5206\u9694\u7b26\u4e0d\u51fa\u73b0\u5728\u63d0\u793a\u7b26\u6216\u5b8c\u6210\u7b26\u4e2d\uff0c\u5206\u9694\u7b26\u5c31\u4e0d\u4f1a\u4ea7\u751f\u592a\u5927\u7684\u5dee\u5f02(\u901a\u5e38\u5c0f\u4e8e 0.4%)\u3002 \u5bf9\u4e8e\u8fd9\u4e2a\u7528\u4f8b\uff0c\u6211\u4eec\u5bf9 ada \u6a21\u578b\u8fdb\u884c\u4e86\u5fae\u8c03\uff0c\u56e0\u4e3a\u5b83\u5c06\u66f4\u5feb\u3001\u66f4\u4fbf\u5b9c\uff0c\u5e76\u4e14\u6027\u80fd\u5c06\u4e0e\u5927\u578b\u6a21\u578b\u76f8\u5f53\uff0c\u56e0\u4e3a\u5b83\u662f\u4e00\u4e2a\u5206\u7c7b\u4efb\u52a1\u3002 \u73b0\u5728\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u5b8c\u6210\u8bf7\u6c42\u6765\u67e5\u8be2\u6211\u4eec\u7684\u6a21\u578b\u3002 1 2 3 4 5 6 7 8 curl https://api.openai.com/v1/completions \\ -H 'Content-Type: application/json' \\ -H 'Authorization: Bearer YOUR_API_KEY' \\ -d '{ \"prompt\": \"Company: Reliable accountants Ltd\\nProduct: Personal Tax help\\nAd:Best advice in town!\\nSupported:\", \"max_tokens\": 1, \"model\": \"YOUR_FINE_TUNED_MODEL_NAME\" }' \u5b83\u5c06\u8fd4\u56de yes \u6216 no \u3002 Case study: Sentiment analysis \u00b6 Let's say you'd like to get a degree to which a particular tweet is positive or negative. The dataset might look something like the following: 1 { \"prompt\" : \"Overjoyed with the new iPhone! ->\" , \"completion\" : \" positive\" } 1 2 3 4 { \"prompt\" : \"@lakers disappoint for a third straight night https://t.co/38EFe43 ->\" , \"completion\" : \" negative\" } Once the model is fine-tuned, you can get back the log probabilities for the first completion token by setting logprobs=2 on the completion request. The higher the probability for positive class, the higher the relative sentiment. Now we can query our model by making a Completion request. 1 2 3 4 5 6 7 8 curl https://api.openai.com/v1/completions \\ -H 'Content-Type: application/json' \\ -H 'Authorization: Bearer YOUR_API_KEY' \\ -d '{ \"prompt\": \"https://t.co/f93xEd2 Excited to share my latest blog post! ->\", \"max_tokens\": 1, \"model\": \"YOUR_FINE_TUNED_MODEL_NAME\" }' Which will return: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 { \"id\" : \"cmpl-COMPLETION_ID\" , \"object\" : \"text_completion\" , \"created\" : 1589498378 , \"model\" : \"YOUR_FINE_TUNED_MODEL_NAME\" , \"choices\" : [ { \"logprobs\" : { \"text_offset\" : [], \"token_logprobs\" : [ -0.03597255 ], \"tokens\" : [ \" positive\" ], \"top_logprobs\" : [ { \" negative\" : -4.9785037 , \" positive\" : -0.03597255 } ] }, \"text\" : \" positive\" , \"index\" : 0 , \"finish_reason\" : \"length\" } ] } Case study: Categorization for Email triage \u00b6 Let's say you'd like to categorize incoming email into one of a large number of predefined categories. For classification into a large number of categories, we recommend you convert those categories into numbers, which will work well up to ~500 categories. We've observed that adding a space before the number sometimes slightly helps the performance, due to tokenization. You may want to structure your training data as follows: 1 { \"prompt\" : \"Subject: <email_subject>\\nFrom:<customer_name>\\nDate:<date>\\nContent:<email_body>\\n\\n###\\n\\n\" , \"completion\" : \" <numerical_category>\" } For example: 1 { \"prompt\" : \"Subject: Update my address\\nFrom:Joe Doe\\nTo:support@ourcompany.com\\nDate:2021-06-03\\nContent:Hi,\\nI would like to update my billing address to match my delivery address.\\n\\nPlease let me know once done.\\n\\nThanks,\\nJoe\\n\\n###\\n\\n\" , \"completion\" : \" 4\" } In the example above we used an incoming email capped at 2043 tokens as input. (This allows for a 4 token separator and a one token completion, summing up to 2048.) As a separator we used \\n\\n###\\n\\n and we removed any occurrence of ### within the email. Conditional generation \u00b6 Conditional generation is a problem where the content needs to be generated given some kind of input. This includes paraphrasing, summarizing, entity extraction, product description writing given specifications, chatbots and many others. For this type of problem we recommend: Use a separator at the end of the prompt, e.g. \\n\\n###\\n\\n. Remember to also append this separator when you eventually make requests to your model. Use an ending token at the end of the completion, e.g. END Remember to add the ending token as a stop sequence during inference, e.g. stop=[\" END\"] Aim for at least ~500 examples Ensure that the prompt + completion doesn't exceed 2048 tokens, including the separator Ensure the examples are of high quality and follow the same desired format Ensure that the dataset used for finetuning is very similar in structure and type of task as what the model will be used for Using Lower learning rate and only 1-2 epochs tends to work better for these use cases Case study: Write an engaging ad based on a Wikipedia article \u00b6 This is a generative use case so you would want to ensure that the samples you provide are of the highest quality, as the fine-tuned model will try to imitate the style (and mistakes) of the given examples. A good starting point is around 500 examples. A sample dataset might look like this: 1 { \"prompt\" : \"<Product Name>\\n<Wikipedia description>\\n\\n###\\n\\n\" , \"completion\" : \" <engaging ad> END\" } For example: 1 2 3 4 { \"prompt\" : \"Samsung Galaxy Feel\\nThe Samsung Galaxy Feel is an Android smartphone developed by Samsung Electronics exclusively for the Japanese market. The phone was released in June 2017 and was sold by NTT Docomo. It runs on Android 7.0 (Nougat), has a 4.7 inch display, and a 3000 mAh battery.\\nSoftware\\nSamsung Galaxy Feel runs on Android 7.0 (Nougat), but can be later updated to Android 8.0 (Oreo).\\nHardware\\nSamsung Galaxy Feel has a 4.7 inch Super AMOLED HD display, 16 MP back facing and 5 MP front facing cameras. It has a 3000 mAh battery, a 1.6 GHz Octa-Core ARM Cortex-A53 CPU, and an ARM Mali-T830 MP1 700 MHz GPU. It comes with 32GB of internal storage, expandable to 256GB via microSD. Aside from its software and hardware specifications, Samsung also introduced a unique a hole in the phone's shell to accommodate the Japanese perceived penchant for personalizing their mobile phones. The Galaxy Feel's battery was also touted as a major selling point since the market favors handsets with longer battery life. The device is also waterproof and supports 1seg digital broadcasts using an antenna that is sold separately.\\n\\n###\\n\\n\" , \"completion\" : \"Looking for a smartphone that can do it all? Look no further than Samsung Galaxy Feel! With a slim and sleek design, our latest smartphone features high-quality picture and video capabilities, as well as an award winning battery life. END\" } Here we used a multi line separator, as Wikipedia articles contain multiple paragraphs and headings. We also used a simple end token, to ensure that the model knows when the completion should finish. Case study: Entity extraction \u00b6 This is similar to a language transformation task. To improve the performance, it is best to either sort different extracted entities alphabetically or in the same order as they appear in the original text. This will help the model to keep track of all the entities which need to be generated in order. The dataset could look as follows: 1 2 3 4 { \"prompt\" : \"<any text, for example news article>\\n\\n###\\n\\n\" , \"completion\" : \" <list of entities, separated by a newline> END\" } For example: 1 2 3 4 { \"prompt\" : \"Portugal will be removed from the UK's green travel list from Tuesday, amid rising coronavirus cases and concern over a \\\"Nepal mutation of the so-called Indian variant\\\". It will join the amber list, meaning holidaymakers should not visit and returnees must isolate for 10 days...\\n\\n###\\n\\n\" , \"completion\" : \" Portugal\\nUK\\nNepal mutation\\nIndian variant END\" } A multi-line separator works best, as the text will likely contain multiple lines. Ideally there will be a high diversity of the types of input prompts (news articles, Wikipedia pages, tweets, legal documents), which reflect the likely texts which will be encountered when extracting entities. Case study: Customer support chatbot \u00b6 A chatbot will normally contain relevant context about the conversation (order details), summary of the conversation so far as well as most recent messages. For this use case the same past conversation can generate multiple rows in the dataset, each time with a slightly different context, for every agent generation as a completion. This use case will require a few thousand examples, as it will likely deal with different types of requests, and customer issues. To ensure the performance is of high quality we recommend vetting the conversation samples to ensure the quality of agent messages. The summary can be generated with a separate text transformation fine tuned model. The dataset could look as follows: 1 2 3 4 { \"prompt\" : \"Summary: <summary of the interaction so far>\\n\\nSpecific information:<for example order details in natural language>\\n\\n###\\n\\nCustomer: <message1>\\nAgent: <response1>\\nCustomer: <message2>\\nAgent:\" , \"completion\" : \" <response2>\\n\" } 1 2 3 4 { \"prompt\" : \"Summary: <summary of the interaction so far>\\n\\nSpecific information:<for example order details in natural language>\\n\\n###\\n\\nCustomer: <message1>\\nAgent: <response1>\\nCustomer: <message2>\\nAgent: <response2>\\nCustomer: <message3>\\nAgent:\" , \"completion\" : \" <response3>\\n\" } Here we purposefully separated different types of input information, but maintained Customer Agent dialog in the same format between a prompt and a completion. All the completions should only be by the agent, and we can use \\n as a stop sequence when doing inference. Case study: Product description based on a technical list of properties Here it is important to convert the input data into a natural language, which will likely lead to superior performance. For example, the following format: 1 2 3 4 { \"prompt\" : \"Item=handbag, Color=army_green, price=$99, size=S->\" , \"completion\" : \" This stylish small green handbag will add a unique touch to your look, without costing you a fortune.\" } Won't work as well as: 1 2 3 4 { \"prompt\" : \"Item is a handbag. Colour is army green. Price is midrange. Size is small.->\" , \"completion\" : \" This stylish small green handbag will add a unique touch to your look, without costing you a fortune.\" } For high performance ensure that the completions were based on the description provided. If external content is often consulted, then adding such content in an automated way would improve the performance. If the description is based on images, it may help to use an algorithm to extract a textual description of the image. Since completions are only one sentence long, we can use . as the stop sequence during inference. Advanced usage \u00b6 Customize your model name \u00b6 You can add a suffix of up to 40 characters to your fine-tuned model name using the suffix parameter. OpenAI CLI: 1 openai api fine_tunes.create -t test.jsonl -m ada --suffix \"custom model name\" The resulting name would be: ada:ft-your-org:custom-model-name-2022-02-15-04-21-04 Analyzing your fine-tuned model \u00b6 We attach a result file to each job once it has been completed. This results file ID will be listed when you retrieve a fine-tune, and also when you look at the events on a fine-tune. You can download these files: OpenAI CLI: 1 openai api fine_tunes.results -i <YOUR_FINE_TUNE_JOB_ID> CURL: 1 2 curl https://api.openai.com/v1/files/ $RESULTS_FILE_ID /content \\ -H \"Authorization: Bearer $OPENAI_API_KEY \" > results.csv The _results.csv file contains a row for each training step, where a step refers to one forward and backward pass on a batch of data. In addition to the step number, each row contains the following fields corresponding to that step: elapsed_tokens : the number of tokens the model has seen so far (including repeats) elapsed_examples : the number of examples the model has seen so far (including repeats), where one example is one element in your batch. For example, if batch_size = 4, each step will increase elapsed_examples by 4. training_loss : loss on the training batch training_sequence_accuracy : the percentage of completions in the training batch for which the model's predicted tokens matched the true completion tokens exactly. For example, with a batch_size of 3, if your data contains the completions [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be \u2154 = 0.67 training_token_accuracy : the percentage of tokens in the training batch that were correctly predicted by the model. For example, with a batch_size of 3, if your data contains the completions [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be \u215a = 0.83 Classification specific metrics \u00b6 We also provide the option of generating additional classification-specific metrics in the results file, such as accuracy and weighted F1 score. These metrics are periodically calculated against the full validation set and at the end of fine-tuning. You will see them as additional columns in your results file. To enable this, set the parameter --compute_classification_metrics. Additionally, you must provide a validation file, and set either the classification_n_classes parameter, for multiclass classification, or classification_positive_class, for binary classification. OpenAI CLI: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # For multiclass classification openai api fine_tunes.create \\ -t <TRAIN_FILE_ID_OR_PATH> \\ -v <VALIDATION_FILE_OR_PATH> \\ -m <MODEL> \\ --compute_classification_metrics \\ --classification_n_classes <N_CLASSES> # For binary classification openai api fine_tunes.create \\ -t <TRAIN_FILE_ID_OR_PATH> \\ -v <VALIDATION_FILE_OR_PATH> \\ -m <MODEL> \\ --compute_classification_metrics \\ --classification_n_classes 2 \\ --classification_positive_class <POSITIVE_CLASS_FROM_DATASET> The following metrics will be displayed in your results file if you set --compute_classification_metrics: For multiclass classification \u00b6 classification/accuracy: accuracy classification/weighted_f1_score: weighted F-1 score For binary classification \u00b6 The following metrics are based on a classification threshold of 0.5 (i.e. when the probability is > 0.5, an example is classified as belonging to the positive class.) classification/accuracy classification/precision classification/recall classification/f{beta} classification/auroc - AUROC classification/auprc - AUPRC Note that these evaluations assume that you are using text labels for classes that tokenize down to a single token, as described above. If these conditions do not hold, the numbers you get will likely be wrong. Validation \u00b6 You can reserve some of your data for validation. A validation file has exactly the same format as a train file, and your train and validation data should be mutually exclusive. If you include a validation file when creating your fine-tune job, the generated results file will include evaluations on how well the fine-tuned model performs against your validation data at periodic intervals during training. OpenAI CLI: 1 2 3 openai api fine_tunes.create -t <TRAIN_FILE_ID_OR_PATH> \\ -v <VALIDATION_FILE_ID_OR_PATH> \\ -m <MODEL> If you provided a validation file, we periodically calculate metrics on batches of validation data during training time. You will see the following additional metrics in your results file: validation_loss: loss on the validation batch validation_sequence_accuracy: the percentage of completions in the validation batch for which the model's predicted tokens matched the true completion tokens exactly. For example, with a batch_size of 3, if your data contains the completion [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be \u2154 = 0.67 validation_token_accuracy: the percentage of tokens in the validation batch that were correctly predicted by the model. For example, with a batch_size of 3, if your data contains the completion [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be \u215a = 0.83 Hyperparameters \u00b6 We've picked default hyperparameters that work well across a range of use cases. The only required parameter is the training file. That said, tweaking the hyperparameters used for fine-tuning can often lead to a model that produces higher quality output. In particular, you may want to configure the following: model: The name of the base model to fine-tune. You can select one of \"ada\", \"babbage\", \"curie\", or \"davinci\". To learn more about these models, see the Models documentation. n_epochs - defaults to 4. The number of epochs to train the model for. An epoch refers to one full cycle through the training dataset. batch_size - defaults to ~0.2% of the number of examples in the training set, capped at 256. The batch size is the number of training examples used to train a single forward and backward pass. In general, we've found that larger batch sizes tend to work better for larger datasets. learning_rate_multiplier - defaults to 0.05, 0.1, or 0.2 depending on final batch_size. The fine-tuning learning rate is the original learning rate used for pretraining multiplied by this multiplier. We recommend experimenting with values in the range 0.02 to 0.2 to see what produces the best results. Empirically, we've found that larger learning rates often perform better with larger batch sizes. compute_classification_metrics - defaults to False. If True, for fine-tuning for classification tasks, computes classification-specific metrics (accuracy, F-1 score, etc) on the validation set at the end of every epoch. To configure these additional hyperparameters, pass them in via command line flags on the OpenAI CLI, for example: 1 2 3 4 openai api fine_tunes.create \\ -t file-JD89ePi5KMsB3Tayeli5ovfW \\ -m ada \\ --n_epochs 1 Continue fine-tuning from a fine-tuned model \u00b6 If you have already fine-tuned a model for your task and now have additional training data that you would like to incorporate, you can continue fine-tuning from the model. This creates a model that has learned from all of the training data without having to re-train from scratch. To do this, pass in the fine-tuned model name when creating a new fine-tuning job (e.g. -m curie:ft- - ). Other training parameters do not have to be changed, however if your new training data is much smaller than your previous training data, you may find it useful to reduce learning_rate_multiplier by a factor of 2 to 4. Weights & Biases You can sync your fine-tunes with Weights & Biases to track experiments, models, and datasets. To get started, you will need a Weights & Biases account and a paid OpenAI plan. To make sure you are using the lastest version of openai and wandb, run: 1 pip install --upgrade openai wandb To sync your fine-tunes with Weights & Biases, run: 1 openai wandb sync You can read the Weights & Biases documentation for more information on this integration. Example notebooks \u00b6 Classification \u00b6 finetuning-classification.ipynb This notebook will demonstrate how to fine-tune a model that can classify whether a piece of input text is related to Baseball or Hockey. We will perform this task in four steps in the notebook: Data exploration will give an overview of the data source and what an example looks like Data preparation will turn our data source into a jsonl file that can be used for fine-tuning Fine-tuning will kick off the fine-tuning job and explain the resulting model's performance Using the model will demonstrate making requests to the fine-tuned model to get predictions. Question answering \u00b6 olympics-1-collect-data.ipynbolympics-2-create-qa.ipynbolympics-3-train-qa.ipynb The idea of this project is to create a question answering model, based on a few paragraphs of provided text. Base GPT-3 models do a good job at answering questions when the answer is contained within the paragraph, however if the answer isn't contained, the base models tend to try their best to answer anyway, often leading to confabulated answers. To create a model which answers questions only if there is sufficient context for doing so, we first create a dataset of questions and answers based on paragraphs of text. In order to train the model to answer only when the answer is present, we also add adversarial examples, where the question doesn't match the context. In those cases, we ask the model to output \"No sufficient context for answering the question\". We will perform this task in three notebooks: The first notebook focuses on collecting recent data, which GPT-3 didn't see during it's pre-training. We picked the topic of Olympic Games 2020 (which actually took place in the summer of 2021), and downloaded 713 unique pages. We organized the dataset by individual sections, which will serve as context for asking and answering the questions. The second notebook will utilize Davinci-instruct to ask a few questions based on a Wikipedia section, as well as answer those questions, based on that section. The third notebook will utilize the dataset of context, question and answer pairs to additionally create adversarial questions and context pairs, where the question was not generated on that context. In those cases the model will be prompted to answer \"No sufficient context for answering the question\". We will also train a discriminator model, which predicts whether the question can be answered based on the context or not.","title":"\u5fae\u8c03"},{"location":"guides/fine-tuning/#_1","text":"\u4e86\u89e3\u5982\u4f55\u4e3a\u60a8\u7684\u5e94\u7528\u7a0b\u5e8f\u5b9a\u5236\u6a21\u578b\u3002","title":"\u5fae\u8c03"},{"location":"guides/fine-tuning/#_2","text":"\u5fae\u8c03\u53ef\u4ee5\u8ba9\u4f60\u4ece API \u63d0\u4f9b\u7684\u6a21\u578b\u4e2d\u83b7\u5f97\u66f4\u591a: \u9ad8\u8d28\u91cf\u7684\u7ed3\u679c\u6bd4\u53ca\u65f6\u7684\u8bbe\u8ba1 \u80fd\u591f\u8bad\u7ec3\u66f4\u591a\u7684\u4f8b\u5b50\uff0c\u8d85\u8fc7\u53ef\u4ee5\u5728\u63d0\u793a \u7531\u4e8e\u66f4\u77ed\u7684\u63d0\u793a\uff0c\u4ee4\u724c\u8282\u7701 \u4f4e\u5ef6\u8fdf\u8bf7\u6c42 GPT-3 \u5df2\u7ecf\u5728\u6765\u81ea\u5f00\u653e\u4e92\u8054\u7f51\u7684\u5927\u91cf\u6587\u672c\u4e0a\u8fdb\u884c\u4e86\u9884\u8bad\u7ec3\u3002 \u5f53\u7ed9\u51fa\u4e00\u4e2a\u53ea\u6709\u51e0\u4e2a\u4f8b\u5b50\u7684\u63d0\u793a\u65f6\uff0c\u5b83\u901a\u5e38\u53ef\u4ee5\u51ed\u76f4\u89c9\u5224\u65ad\u51fa\u4f60\u8981\u6267\u884c\u7684\u4efb\u52a1\uff0c\u5e76\u751f\u6210\u4e00\u4e2a\u5408\u7406\u7684\u5b8c\u6210\u7ed3\u679c\u3002 \u8fd9\u901a\u5e38\u88ab\u79f0\u4e3a\u201c\u51e0\u6b21\u5b66\u4e60\u201d\u3002 \u901a\u8fc7\u5bf9\u6bd4\u63d0\u793a\u4e2d\u9002\u5408\u7684\u66f4\u591a\u7684\u793a\u4f8b\u8fdb\u884c\u8bad\u7ec3\uff0c\u5fae\u8c03\u6539\u8fdb\u4e86\u5c11\u6570\u955c\u5934\u5b66\u4e60\uff0c\u8ba9\u60a8\u5728\u5927\u91cf\u4efb\u52a1\u4e2d\u83b7\u5f97\u66f4\u597d\u7684\u7ed3\u679c\u3002 \u4e00\u65e6\u5bf9\u6a21\u578b\u8fdb\u884c\u4e86\u5fae\u8c03\uff0c\u5c31\u4e0d\u518d\u9700\u8981\u5728\u63d0\u793a\u7b26\u4e2d\u63d0\u4f9b\u793a\u4f8b\u4e86\u3002 \u8fd9\u8282\u7701\u4e86\u6210\u672c\u5e76\u652f\u6301\u8f83\u4f4e\u7684\u5ef6\u8fdf\u8bf7\u6c42\u3002 \u5728\u9ad8\u5c42\u6b21\u4e0a\uff0c\u5fae\u8c03\u5305\u62ec\u4ee5\u4e0b\u6b65\u9aa4: \u51c6\u5907\u548c\u4e0a\u4f20\u57f9\u8bad\u6570\u636e \u8bad\u7ec3\u4e00\u4e2a\u65b0\u7684\u5fae\u8c03\u6a21\u578b \u4f7f\u7528\u60a8\u7684\u5fae\u8c03\u6a21\u578b \u8bbf\u95ee\u6211\u4eec\u7684\u5b9a\u4ef7\u9875\u9762\uff0c\u4e86\u89e3\u66f4\u591a\u5173\u4e8e\u5fae\u8c03\u6a21\u578b\u8bad\u7ec3\u548c\u4f7f\u7528\u662f\u5982\u4f55\u8ba1\u8d39\u7684\u3002","title":"\u7b80\u4ecb"},{"location":"guides/fine-tuning/#_3","text":"\u5fae\u8c03\u76ee\u524d\u53ea\u9002\u7528\u4e8e\u4ee5\u4e0b\u57fa\u672c\u6a21\u578b:davinci, curie, babbage, and ada. \u8fd9\u4e9b\u662f\u539f\u59cb\u6a21\u578b\uff0c\u5728\u8bad\u7ec3\u4e4b\u540e\u6ca1\u6709\u4efb\u4f55\u6307\u4ee4(\u4f8b\u5982 text-davinci-003)\u3002 \u60a8\u8fd8\u53ef\u4ee5\u7ee7\u7eed\u5bf9\u5df2\u8c03\u4f18\u7684\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u6dfb\u52a0\u989d\u5916\u7684\u6570\u636e\uff0c\u800c\u4e0d\u5fc5\u4ece\u5934\u5f00\u59cb\u3002","title":"\u54ea\u4e9b\u6a21\u578b\u53ef\u4ee5\u5fae\u8c03?"},{"location":"guides/fine-tuning/#_4","text":"\u6211\u4eec\u5efa\u8bae\u4f7f\u7528 OpenAI \u547d\u4ee4\u884c\u754c\u9762(CLI)\u3002\u8981\u5b89\u88c5\u6b64\u7a0b\u5e8f\uff0c\u8bf7\u8fd0\u884c 1 pip install --upgrade openai (\u4ee5\u4e0b\u8bf4\u660e\u9002\u7528\u4e8e 0.9.4 \u53ca\u66f4\u9ad8\u7248\u672c\u3002\u53e6\u5916\uff0cOpenAI CLI \u9700\u8981 python 3\u3002) \u901a\u8fc7\u5728\u4f60\u7684 shell \u521d\u59cb\u5316\u811a\u672c(\u4f8b\u5982 .bashrc, .zshrc \u7b49)\u4e2d\u6dfb\u52a0\u4ee5\u4e0b\u884c\u6765\u8bbe\u7f6e\u4f60\u7684 OPENAI_API_KEY \u73af\u5883\u53d8\u91cf\uff0c\u6216\u8005\u5728\u8c03\u4f18\u547d\u4ee4\u4e4b\u524d\u5728\u547d\u4ee4\u884c\u4e2d\u8fd0\u884c\u5b83: 1 export OPENAI_API_KEY = \"<OPENAI_API_KEY>\"","title":"\u5b89\u88c5"},{"location":"guides/fine-tuning/#_5","text":"\u8bad\u7ec3\u6570\u636e\u662f\u4f60\u5982\u4f55\u6559 GPT-3 \u4f60\u60f3\u8ba9\u5b83\u8bf4\u4ec0\u4e48\u3002 \u60a8\u7684\u6570\u636e\u5fc5\u987b\u662f JSONL \u6587\u6863\uff0c\u5176\u4e2d\u6bcf\u4e00\u884c\u90fd\u662f\u5bf9\u5e94\u4e8e\u4e00\u4e2a\u8bad\u7ec3\u793a\u4f8b\u7684\u63d0\u793a-\u8865\u5168\u5bf9\u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528\u6211\u4eec\u7684 CLI \u6570\u636e\u51c6\u5907\u5de5\u5177\u8f7b\u677e\u5730\u5c06\u6570\u636e\u8f6c\u6362\u4e3a\u8fd9\u79cd\u6587\u4ef6\u683c\u5f0f\u3002 1 2 3 4 { \"prompt\" : \"<prompt text>\" , \"completion\" : \"<ideal generated text>\" } { \"prompt\" : \"<prompt text>\" , \"completion\" : \"<ideal generated text>\" } { \"prompt\" : \"<prompt text>\" , \"completion\" : \"<ideal generated text>\" } ... \u8bbe\u8ba1\u7528\u4e8e\u5fae\u8c03\u7684\u63d0\u793a\u7b26\u548c\u8865\u5168\u4e0e\u8bbe\u8ba1\u7528\u4e8e\u57fa\u672c\u6a21\u578b(Davinci, Curie, Babbage, Ada)\u7684\u63d0\u793a\u7b26\u4e0d\u540c\u3002 \u7279\u522b\u662f\uff0c\u867d\u7136\u57fa\u672c\u6a21\u578b\u7684\u63d0\u793a\u901a\u5e38\u7531\u591a\u4e2a\u793a\u4f8b\u7ec4\u6210(\u201c\u5c11\u955c\u5934\u5b66\u4e60\u201d)\uff0c\u4f46\u4e3a\u4e86\u8fdb\u884c\u5fae\u8c03\uff0c\u6bcf\u4e2a\u8bad\u7ec3\u793a\u4f8b\u901a\u5e38\u7531\u5355\u4e2a\u8f93\u5165\u793a\u4f8b\u53ca\u5176\u76f8\u5173\u8f93\u51fa\u7ec4\u6210\uff0c\u800c\u4e0d\u9700\u8981\u7ed9\u51fa\u8be6\u7ec6\u7684\u8bf4\u660e\u6216\u5728\u540c\u4e00\u63d0\u793a\u4e2d\u5305\u542b\u591a\u4e2a\u793a\u4f8b\u3002 \u6709\u5173\u5982\u4f55\u4e3a\u5404\u79cd\u4efb\u52a1\u51c6\u5907\u8bad\u7ec3\u6570\u636e\u7684\u66f4\u8be6\u7ec6\u6307\u5bfc\uff0c\u8bf7\u53c2\u9605\u6211\u4eec\u7684\u51c6\u5907\u6570\u636e\u96c6\u6700\u4f73\u5b9e\u8df5\u3002 \u8bad\u7ec3\u7684\u4f8b\u5b50\u8d8a\u591a\u8d8a\u597d\u3002\u6211\u4eec\u5efa\u8bae\u81f3\u5c11\u6709\u51e0\u767e\u4e2a\u4f8b\u5b50\u3002\u4e00\u822c\u6765\u8bf4\uff0c\u6211\u4eec\u53d1\u73b0\u6570\u636e\u96c6\u5927\u5c0f\u6bcf\u589e\u52a0\u4e00\u500d\uff0c\u5c31\u4f1a\u5bfc\u81f4\u6a21\u578b\u8d28\u91cf\u7684\u7ebf\u6027\u589e\u957f\u3002","title":"\u51c6\u5907\u57f9\u8bad\u6570\u636e"},{"location":"guides/fine-tuning/#cli","text":"\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u5de5\u5177\uff0c\u53ef\u4ee5\u9a8c\u8bc1\u3001\u63d0\u4f9b\u5efa\u8bae\u5e76\u91cd\u65b0\u683c\u5f0f\u5316\u60a8\u7684\u6570\u636e: 1 openai tools fine_tunes.prepare_data -f <LOCAL_FILE> \u8be5\u5de5\u5177\u63a5\u53d7\u4e0d\u540c\u7684\u683c\u5f0f\uff0c\u552f\u4e00\u7684\u8981\u6c42\u662f\u5b83\u4eec\u5305\u542b\u63d0\u793a\u7b26\u548c\u5b8c\u6210\u5217/\u952e\u3002 \u60a8\u53ef\u4ee5\u4f20\u9012\u4e00\u4e2a CSV\u3001TSV\u3001XLSX\u3001JSON \u6216 JSONL \u6587\u4ef6\uff0c\u5728\u6307\u5bfc\u60a8\u5b8c\u6210\u5efa\u8bae\u7684\u66f4\u6539\u8fc7\u7a0b\u540e\uff0c\u5b83\u4f1a\u5c06\u8f93\u51fa\u4fdd\u5b58\u5230\u4e00\u4e2a JSONL \u6587\u4ef6\u4e2d\uff0c\u4ee5\u4fbf\u8fdb\u884c\u5fae\u8c03\u3002","title":"CLI \u6570\u636e\u51c6\u5907\u5de5\u5177"},{"location":"guides/fine-tuning/#_6","text":"\u4e0b\u9762\u5047\u8bbe\u4f60\u5df2\u7ecf\u6309\u7167\u4e0a\u9762\u7684\u8bf4\u660e\u51c6\u5907\u597d\u4e86\u8bad\u7ec3\u6570\u636e\u3002 \u4f7f\u7528 OpenAI \u547d\u4ee4\u884c\u5f00\u59cb\u4f60\u7684\u8c03\u4f18\u5de5\u4f5c: 1 openai api fine_tunes.create -t <TRAIN_FILE_ID_OR_PATH> -m <BASE_MODEL> \u5176\u4e2d BASE_MODEL \u662f\u60a8\u5f00\u59cb\u7684\u57fa\u672c\u6a21\u578b\u7684\u540d\u79f0(ada, babbage, curie \u6216 davinci)\u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528 suffix \u53c2\u6570\u81ea\u5b9a\u4e49\u7ecf\u8fc7\u5fae\u8c03\u7684\u6a21\u578b\u7684\u540d\u79f0\u3002 \u8fd0\u884c\u4e0a\u9762\u7684\u547d\u4ee4\u4f1a\u505a\u51e0\u4ef6\u4e8b: \u4f7f\u7528 files API \u4e0a\u4f20\u6587\u4ef6(\u6216\u4f7f\u7528\u5df2\u7ecf\u4e0a\u4f20\u7684\u6587\u4ef6) \u521b\u5efa\u5fae\u8c03\u4f5c\u4e1a \u6d41\u4e8b\u4ef6\u76f4\u5230\u4f5c\u4e1a\u5b8c\u6210(\u8fd9\u901a\u5e38\u9700\u8981\u51e0\u5206\u949f\uff0c\u4f46\u5982\u679c\u961f\u5217\u4e2d\u6709\u5f88\u591a\u4f5c\u4e1a\u6216\u6570\u636e\u96c6\u5f88\u5927\uff0c\u5219\u53ef\u80fd\u9700\u8981\u51e0\u4e2a\u5c0f\u65f6) \u6bcf\u4e2a\u5fae\u8c03\u5de5\u4f5c\u90fd\u4ece\u4e00\u4e2a\u57fa\u672c\u6a21\u578b\u5f00\u59cb\uff0c\u9ed8\u8ba4\u4e3a curie \u6a21\u578b\u3002 \u6a21\u578b\u7684\u9009\u62e9\u65e2\u5f71\u54cd\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e5f\u5f71\u54cd\u8fd0\u884c\u7ecf\u8fc7\u5fae\u8c03\u7684\u6a21\u578b\u7684\u6210\u672c\u3002 \u4f60\u7684\u6a21\u578b\u53ef\u4ee5\u662f ada, babbage, curie, or davinci\u3002 \u8bf7\u8bbf\u95ee\u6211\u4eec\u7684\u5b9a\u4ef7\u9875\u9762\u4e86\u89e3\u5fae\u8c03\u8d39\u7387\u7684\u8be6\u7ec6\u4fe1\u606f\u3002 \u5728\u5f00\u59cb\u4e00\u9879\u5fae\u8c03\u5de5\u4f5c\u4e4b\u540e\uff0c\u53ef\u80fd\u9700\u8981\u4e00\u4e9b\u65f6\u95f4\u624d\u80fd\u5b8c\u6210\u3002 \u4f60\u7684\u4f5c\u4e1a\u53ef\u80fd\u6392\u5728\u6211\u4eec\u7cfb\u7edf\u4e0a\u5176\u4ed6\u4f5c\u4e1a\u7684\u540e\u9762\uff0c\u8bad\u7ec3\u6211\u4eec\u7684\u6a21\u578b\u53ef\u80fd\u9700\u8981\u51e0\u5206\u949f\u6216\u51e0\u5c0f\u65f6\uff0c\u8fd9\u53d6\u51b3\u4e8e\u6a21\u578b\u548c\u6570\u636e\u96c6\u5927\u5c0f\u3002 \u5982\u679c\u4e8b\u4ef6\u6d41\u56e0\u4efb\u4f55\u539f\u56e0\u4e2d\u65ad\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6062\u590d\u5b83: 1 openai api fine_tunes.follow -i <YOUR_FINE_TUNE_JOB_ID> \u5f53\u5de5\u4f5c\u5b8c\u6210\u65f6\uff0c\u5b83\u5e94\u8be5\u663e\u793a\u7ecf\u8fc7\u5fae\u8c03\u7684\u6a21\u578b\u7684\u540d\u79f0\u3002 \u9664\u4e86\u521b\u5efa\u5fae\u8c03\u4f5c\u4e1a\u5916\uff0c\u60a8\u8fd8\u53ef\u4ee5\u5217\u51fa\u73b0\u6709\u4f5c\u4e1a\u3001\u68c0\u7d22\u4f5c\u4e1a\u7684\u72b6\u6001\u6216\u53d6\u6d88\u4f5c\u4e1a\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # List all created fine-tunes openai api fine_tunes.list # Retrieve the state of a fine-tune. The resulting object includes # job status (which can be one of pending, running, succeeded, or failed) # and other information openai api fine_tunes.get -i <YOUR_FINE_TUNE_JOB_ID> # Cancel a job openai api fine_tunes.cancel -i <YOUR_FINE_TUNE_JOB_ID>","title":"\u521b\u5efa\u4e00\u4e2a\u5fae\u8c03\u6a21\u578b"},{"location":"guides/fine-tuning/#_7","text":"\u5f53\u4f5c\u4e1a\u6210\u529f\u65f6\uff0cfine_tuned_model \u5b57\u6bb5\u5c06\u586b\u5145\u6a21\u578b\u7684\u540d\u79f0\u3002\u73b0\u5728\u4f60\u53ef\u4ee5\u5c06\u8fd9\u4e2a\u6a21\u578b\u6307\u5b9a\u4e3a Completions API \u7684\u53c2\u6570\uff0c\u5e76\u4f7f\u7528 Playground \u5411\u5b83\u53d1\u51fa\u8bf7\u6c42\u3002 \u5728\u60a8\u7684\u4efb\u52a1\u7b2c\u4e00\u6b21\u5b8c\u6210\u540e\uff0c\u60a8\u7684\u6a21\u578b\u53ef\u80fd\u9700\u8981\u51e0\u5206\u949f\u624d\u80fd\u51c6\u5907\u597d\u5904\u7406\u8bf7\u6c42\u3002\u5982\u679c\u5bf9\u6a21\u578b\u7684\u5b8c\u6210\u8bf7\u6c42\u8d85\u65f6\uff0c\u5f88\u53ef\u80fd\u662f\u56e0\u4e3a\u6a21\u578b\u4ecd\u5728\u52a0\u8f7d\u4e2d\u3002\u5982\u679c\u53d1\u751f\u8fd9\u79cd\u60c5\u51b5\uff0c\u51e0\u5206\u949f\u540e\u518d\u8bd5\u4e00\u6b21\u3002 \u4f60\u53ef\u4ee5\u901a\u8fc7\u4f20\u9012\u6a21\u578b\u540d\u4f5c\u4e3a\u5b8c\u6210\u8bf7\u6c42\u7684\u6a21\u578b\u53c2\u6570\u6765\u5f00\u59cb\u8bf7\u6c42: OpenAI CLI: 1 openai api completions.create -m <FINE_TUNED_MODEL> -p <YOUR_PROMPT> cURL: 1 2 3 4 curl https://api.openai.com/v1/completions \\ -H \"Authorization: Bearer $OPENAI_API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{\"prompt\": YOUR_PROMPT, \"model\": FINE_TUNED_MODEL}' Python: 1 2 3 4 import openai openai . Completion . create ( model = FINE_TUNED_MODEL , prompt = YOUR_PROMPT ) Node.js: 1 2 3 4 const response = await openai . createCompletion ({ model : FINE_TUNED_MODEL prompt : YOUR_PROMPT , }); \u4f60\u53ef\u4ee5\u7ee7\u7eed\u4f7f\u7528\u6240\u6709\u5176\u4ed6\u7684 Completions \u53c2\u6570\uff0c\u6bd4\u5982 temperature, frequency_penalty, presence_penalty \u7b49\uff0c\u5bf9\u8fd9\u4e9b\u8bf7\u6c42\u8fdb\u884c\u5fae\u8c03\u6a21\u578b\u3002","title":"\u4f7f\u7528\u7ecf\u8fc7\u5fae\u8c03\u7684\u6a21\u578b"},{"location":"guides/fine-tuning/#_8","text":"\u8981\u5220\u9664\u4e00\u4e2a\u7ecf\u8fc7\u5fae\u8c03\u7684\u6a21\u578b\uff0c\u60a8\u5fc5\u987b\u5728\u7ec4\u7ec7\u4e2d\u6307\u5b9a\u4e00\u4e2a\u201c\u6240\u6709\u8005\u201d\u3002 OpenAI CLI: 1 openai api models.delete -i <FINE_TUNED_MODEL> cURL: 1 2 curl -X \"DELETE\" https://api.openai.com/v1/models/<FINE_TUNED_MODEL> \\ -H \"Authorization: Bearer $OPENAI_API_KEY \" Python: 1 2 import openai openai . Model . delete ( FINE_TUNED_MODEL )","title":"\u5220\u9664\u4e00\u4e2a\u7ecf\u8fc7\u5fae\u8c03\u7684\u6a21\u578b"},{"location":"guides/fine-tuning/#_9","text":"\u5fae\u8c03\u662f\u521b\u5efa\u7279\u5b9a\u4e8e\u7528\u4f8b\u7684\u65b0\u6a21\u578b\u7684\u5f3a\u5927\u6280\u672f\u3002\u5728\u5bf9\u60a8\u7684\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u4e4b\u524d\uff0c\u6211\u4eec\u5f3a\u70c8\u5efa\u8bae\u60a8\u9605\u8bfb\u4ee5\u4e0b\u7528\u4f8b\u7684\u6700\u4f73\u5b9e\u8df5\u548c\u5177\u4f53\u6307\u5357\u3002","title":"\u51c6\u5907\u6570\u636e\u96c6"},{"location":"guides/fine-tuning/#_10","text":"\u8981\u5bf9\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u60a8\u9700\u8981\u4e00\u7ec4\u8bad\u7ec3\u793a\u4f8b\uff0c\u6bcf\u4e2a\u793a\u4f8b\u7531\u5355\u4e2a\u8f93\u5165(\u201c\u63d0\u793a\u201d)\u53ca\u5176\u76f8\u5173\u8f93\u51fa(\u201c\u5b8c\u6210\u201d)\u7ec4\u6210\u3002\u8fd9\u4e0e\u4f7f\u7528\u6211\u4eec\u7684\u57fa\u672c\u6a21\u578b\u660e\u663e\u4e0d\u540c\uff0c\u5728\u57fa\u672c\u6a21\u578b\u4e2d\uff0c\u60a8\u53ef\u4ee5\u5728\u4e00\u4e2a\u63d0\u793a\u7b26\u4e2d\u8f93\u5165\u8be6\u7ec6\u7684\u8bf4\u660e\u6216\u591a\u4e2a\u793a\u4f8b\u3002 \u6bcf\u4e2a\u63d0\u793a\u7b26\u5e94\u8be5\u4ee5\u4e00\u4e2a\u56fa\u5b9a\u7684\u5206\u9694\u7b26\u7ed3\u675f\uff0c\u4ee5\u901a\u77e5\u6a21\u578b\u63d0\u793a\u7b26\u4f55\u65f6\u7ed3\u675f\u548c\u5b8c\u6210\u5f00\u59cb\u3002\u4e00\u4e2a\u7b80\u5355\u7684\u5206\u79bb\u5668\u901a\u5e38\u5de5\u4f5c\u5f97\u5f88\u597d\u662f \\n\\n###\\n\\n \u3002\u5206\u9694\u7b26\u4e0d\u5e94\u8be5\u51fa\u73b0\u5728\u4efb\u4f55\u63d0\u793a\u7b26\u7684\u5176\u4ed6\u5730\u65b9\u3002 \u7531\u4e8e\u6211\u4eec\u7684\u6807\u8bb0\u5316\uff0c\u6bcf\u4e2a\u8865\u5168\u90fd\u5e94\u8be5\u4ee5\u7a7a\u767d\u5f00\u59cb\uff0c\u6807\u8bb0\u5316\u5927\u591a\u6570\u5355\u8bcd\u4e4b\u524d\u90fd\u6709\u7a7a\u767d\u3002 \u6bcf\u4e2a\u8865\u5168\u5e94\u8be5\u4ee5\u56fa\u5b9a\u7684\u505c\u6b62\u5e8f\u5217\u7ed3\u675f\uff0c\u4ee5\u4fbf\u5728\u8865\u5168\u7ed3\u675f\u65f6\u901a\u77e5\u6a21\u578b\u3002\u505c\u6b62\u5e8f\u5217\u53ef\u4ee5\u662f \\n \u3001 ### \u6216\u4efb\u4f55\u4e0d\u51fa\u73b0\u5728\u4efb\u4f55\u8865\u5168\u4e2d\u7684\u5176\u4ed6\u6807\u8bb0\u3002 \u5bf9\u4e8e\u63a8\u65ad\uff0c\u60a8\u5e94\u8be5\u6309\u7167\u4e0e\u521b\u5efa\u8bad\u7ec3\u6570\u636e\u96c6\u65f6\u76f8\u540c\u7684\u65b9\u5f0f\u683c\u5f0f\u5316\u63d0\u793a\uff0c\u5305\u62ec\u76f8\u540c\u7684\u5206\u9694\u7b26\u3002\u8fd8\u8981\u6307\u5b9a\u76f8\u540c\u7684\u505c\u6b62\u5e8f\u5217\u4ee5\u6b63\u786e\u5730\u622a\u65ad\u8865\u5168\u3002","title":"\u6570\u636e\u683c\u5f0f"},{"location":"guides/fine-tuning/#_11","text":"\u4f7f\u7528\u66f4\u591a\u9ad8\u8d28\u91cf\u7684\u793a\u4f8b\u8fdb\u884c\u5fae\u8c03\u6548\u679c\u66f4\u597d\u3002\u8981\u5bf9\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u4f7f\u5176\u6bd4\u4f7f\u7528\u6211\u4eec\u7684\u57fa\u672c\u6a21\u578b\u7684\u9ad8\u8d28\u91cf\u63d0\u793a\u6267\u884c\u5f97\u66f4\u597d\uff0c\u60a8\u5e94\u8be5\u81f3\u5c11\u63d0\u4f9b\u51e0\u767e\u4e2a\u9ad8\u8d28\u91cf\u793a\u4f8b\uff0c\u6700\u597d\u662f\u7531\u4eba\u7c7b\u4e13\u5bb6\u5ba1\u67e5\u3002\u4ece\u8fd9\u91cc\u5f00\u59cb\uff0c\u6027\u80fd\u4f1a\u968f\u7740\u793a\u4f8b\u6570\u91cf\u6bcf\u589e\u52a0\u4e00\u500d\u800c\u7ebf\u6027\u589e\u52a0\u3002\u589e\u52a0\u793a\u4f8b\u7684\u6570\u91cf\u901a\u5e38\u662f\u63d0\u9ad8\u6027\u80fd\u7684\u6700\u4f73\u548c\u6700\u53ef\u9760\u7684\u65b9\u6cd5\u3002 \u5206\u7c7b\u5668\u662f\u6700\u5bb9\u6613\u5165\u95e8\u7684\u6a21\u578b\u3002\u5bf9\u4e8e\u5206\u7c7b\u95ee\u9898\uff0c\u6211\u4eec\u5efa\u8bae\u4f7f\u7528 ada\uff0c\u5b83\u5728\u7ecf\u8fc7\u5fae\u8c03\u540e\u7684\u8868\u73b0\u901a\u5e38\u53ea\u6bd4\u66f4\u6709\u80fd\u529b\u7684\u6a21\u578b\u5dee\u4e00\u70b9\u70b9\uff0c\u540c\u65f6\u901f\u5ea6\u66f4\u5feb\uff0c\u6210\u672c\u66f4\u4f4e\u3002 \u5982\u679c\u60a8\u6b63\u5728\u5bf9\u5df2\u6709\u7684\u6570\u636e\u96c6\u8fdb\u884c\u5fae\u8c03\uff0c\u800c\u4e0d\u662f\u4ece\u5934\u5f00\u59cb\u7f16\u5199\u63d0\u793a\uff0c\u8bf7\u52a1\u5fc5\u5728\u53ef\u80fd\u7684\u60c5\u51b5\u4e0b\u624b\u52a8\u68c0\u67e5\u6570\u636e\u4e2d\u7684\u5192\u72af\u6027\u6216\u4e0d\u51c6\u786e\u5185\u5bb9\uff0c\u6216\u8005\u5728\u6570\u636e\u96c6\u5f88\u5927\u7684\u60c5\u51b5\u4e0b\u68c0\u67e5\u5c3d\u53ef\u80fd\u591a\u7684\u968f\u673a\u6837\u672c\u3002","title":"\u4e00\u822c\u6700\u4f73\u5b9e\u8df5"},{"location":"guides/fine-tuning/#_12","text":"\u5fae\u8c03\u53ef\u4ee5\u89e3\u51b3\u5404\u79cd\u5404\u6837\u7684\u95ee\u9898\uff0c\u4f7f\u7528\u5b83\u7684\u6700\u4f73\u65b9\u5f0f\u53ef\u80fd\u53d6\u51b3\u4e8e\u60a8\u7684\u7279\u5b9a\u7528\u4f8b\u3002\u4e0b\u9762\uff0c\u6211\u4eec\u5217\u51fa\u4e86\u7528\u4e8e\u5fae\u8c03\u7684\u6700\u5e38\u89c1\u7528\u4f8b\u548c\u76f8\u5e94\u7684\u6307\u5bfc\u65b9\u9488\u3002 \u5206\u7c7b \u6a21\u7279\u662f\u5426\u505a\u4e86\u4e0d\u771f\u5b9e\u7684\u9648\u8ff0? \u60c5\u7eea\u5206\u6790 \u7535\u5b50\u90ae\u4ef6\u5206\u7c7b \u6709\u6761\u4ef6\u7684\u4ee3 \u6839\u636e\u7ef4\u57fa\u767e\u79d1\u4e0a\u7684\u6587\u7ae0\u5199\u4e00\u4e2a\u5438\u5f15\u4eba\u7684\u5e7f\u544a \u5b9e\u4f53\u63d0\u53d6 \u5ba2\u6237\u652f\u6301\u804a\u5929\u673a\u5668\u4eba \u57fa\u4e8e\u6280\u672f\u7279\u6027\u8868\u7684\u4ea7\u54c1\u63cf\u8ff0","title":"\u5177\u4f53\u7684\u6307\u5bfc"},{"location":"guides/fine-tuning/#_13","text":"\u5728\u5206\u7c7b\u95ee\u9898\u4e2d\uff0c\u63d0\u793a\u7b26\u4e2d\u7684\u6bcf\u4e2a\u8f93\u5165\u90fd\u5e94\u8be5\u88ab\u5206\u7c7b\u5230\u9884\u5b9a\u4e49\u7684\u7c7b\u4e2d\u3002\u5bf9\u4e8e\u8fd9\u7c7b\u95ee\u9898\uff0c\u6211\u4eec\u5efa\u8bae: \u5728\u63d0\u793a\u7b26\u7684\u672b\u5c3e\u4f7f\u7528\u5206\u9694\u7b26\uff0c\u4f8b\u5982 \\n\\n###\\n\\n \u3002\u8bb0\u4f4f\uff0c\u5f53\u4f60\u6700\u7ec8\u5411\u4f60\u7684\u6a21\u578b\u53d1\u51fa\u8bf7\u6c42\u65f6\uff0c\u4e5f\u8981\u9644\u52a0\u8fd9\u4e2a\u5206\u9694\u7b26\u3002 \u9009\u62e9\u6620\u5c04\u5230\u5355\u4e2a\u4ee4\u724c\u7684\u7c7b\u3002\u5728\u63a8\u7406\u65f6\uff0c\u6307\u5b9a max_tokens=1\uff0c\u56e0\u4e3a\u60a8\u53ea\u9700\u8981\u7b2c\u4e00\u4e2a\u6807\u8bb0\u8fdb\u884c\u5206\u7c7b\u3002 \u786e\u4fdd\u63d0\u793a\u7b26+\u8865\u5168\u4e0d\u8d85\u8fc7 2048 \u4e2a\u4ee4\u724c\uff0c\u5305\u62ec\u5206\u9694\u7b26 \u76ee\u6807\u662f\u6bcf\u7c7b\u81f3\u5c11 100 \u4e2a\u4f8b\u5b50 \u8981\u83b7\u5f97\u7c7b\u65e5\u5fd7\u6982\u7387\uff0c\u60a8\u53ef\u4ee5\u5728\u4f7f\u7528\u6a21\u578b\u65f6\u6307\u5b9a logprobs=5(\u7528\u4e8e 5 \u4e2a\u7c7b) \u786e\u4fdd\u7528\u4e8e\u5fae\u8c03\u7684\u6570\u636e\u96c6\u5728\u7ed3\u6784\u548c\u4efb\u52a1\u7c7b\u578b\u4e0a\u4e0e\u5c06\u7528\u4e8e\u6a21\u578b\u7684\u6570\u636e\u96c6\u975e\u5e38\u76f8\u4f3c","title":"\u5206\u7c7b"},{"location":"guides/fine-tuning/#_14","text":"\u5047\u8bbe\u4f60\u60f3\u786e\u4fdd\u4f60\u7f51\u7ad9\u4e0a\u7684\u5e7f\u544a\u6587\u672c\u63d0\u5230\u4e86\u6b63\u786e\u7684\u4ea7\u54c1\u548c\u516c\u53f8\u3002\u6362\u53e5\u8bdd\u8bf4\uff0c\u60a8\u5e0c\u671b\u786e\u4fdd\u6a21\u578b\u6ca1\u6709\u7f16\u9020\u4efb\u4f55\u4e1c\u897f\u3002\u4f60\u53ef\u80fd\u60f3\u8981\u5fae\u8c03\u4e00\u4e2a\u5206\u7c7b\u5668\u6765\u8fc7\u6ee4\u6389\u4e0d\u6b63\u786e\u7684\u5e7f\u544a\u3002 \u6570\u636e\u96c6\u53ef\u80fd\u770b\u8d77\u6765\u50cf\u4e0b\u9762\u8fd9\u6837: 1 2 {\"prompt\":\"Company: BHFF insurance\\nProduct: allround insurance\\nAd:One stop shop for all your insurance needs!\\nSupported:\", \"completion\":\" yes\"} {\"prompt\":\"Company: Loft conversion specialists\\nProduct: -\\nAd:Straight teeth in weeks!\\nSupported:\", \"completion\":\" no\"} \u5728\u4e0a\u9762\u7684\u4f8b\u5b50\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528\u4e86\u4e00\u4e2a\u5305\u542b\u516c\u53f8\u540d\u79f0\u3001\u4ea7\u54c1\u548c\u76f8\u5173\u5e7f\u544a\u7684\u7ed3\u6784\u5316\u8f93\u5165\u3002\u6211\u4eec\u4f7f\u7528 \\nSupported: \u4f5c\u4e3a\u5206\u9694\u7b26\uff0c\u5b83\u6e05\u695a\u5730\u5c06\u63d0\u793a\u7b26\u4e0e\u5b8c\u6210\u7b26\u5206\u5f00\u3002\u5bf9\u4e8e\u8db3\u591f\u591a\u7684\u793a\u4f8b\uff0c\u53ea\u8981\u5206\u9694\u7b26\u4e0d\u51fa\u73b0\u5728\u63d0\u793a\u7b26\u6216\u5b8c\u6210\u7b26\u4e2d\uff0c\u5206\u9694\u7b26\u5c31\u4e0d\u4f1a\u4ea7\u751f\u592a\u5927\u7684\u5dee\u5f02(\u901a\u5e38\u5c0f\u4e8e 0.4%)\u3002 \u5bf9\u4e8e\u8fd9\u4e2a\u7528\u4f8b\uff0c\u6211\u4eec\u5bf9 ada \u6a21\u578b\u8fdb\u884c\u4e86\u5fae\u8c03\uff0c\u56e0\u4e3a\u5b83\u5c06\u66f4\u5feb\u3001\u66f4\u4fbf\u5b9c\uff0c\u5e76\u4e14\u6027\u80fd\u5c06\u4e0e\u5927\u578b\u6a21\u578b\u76f8\u5f53\uff0c\u56e0\u4e3a\u5b83\u662f\u4e00\u4e2a\u5206\u7c7b\u4efb\u52a1\u3002 \u73b0\u5728\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u5b8c\u6210\u8bf7\u6c42\u6765\u67e5\u8be2\u6211\u4eec\u7684\u6a21\u578b\u3002 1 2 3 4 5 6 7 8 curl https://api.openai.com/v1/completions \\ -H 'Content-Type: application/json' \\ -H 'Authorization: Bearer YOUR_API_KEY' \\ -d '{ \"prompt\": \"Company: Reliable accountants Ltd\\nProduct: Personal Tax help\\nAd:Best advice in town!\\nSupported:\", \"max_tokens\": 1, \"model\": \"YOUR_FINE_TUNED_MODEL_NAME\" }' \u5b83\u5c06\u8fd4\u56de yes \u6216 no \u3002","title":"\u6848\u4f8b\u7814\u7a76:\u6a21\u578b\u662f\u5426\u505a\u4e86\u4e0d\u771f\u5b9e\u7684\u9648\u8ff0?"},{"location":"guides/fine-tuning/#case-study-sentiment-analysis","text":"Let's say you'd like to get a degree to which a particular tweet is positive or negative. The dataset might look something like the following: 1 { \"prompt\" : \"Overjoyed with the new iPhone! ->\" , \"completion\" : \" positive\" } 1 2 3 4 { \"prompt\" : \"@lakers disappoint for a third straight night https://t.co/38EFe43 ->\" , \"completion\" : \" negative\" } Once the model is fine-tuned, you can get back the log probabilities for the first completion token by setting logprobs=2 on the completion request. The higher the probability for positive class, the higher the relative sentiment. Now we can query our model by making a Completion request. 1 2 3 4 5 6 7 8 curl https://api.openai.com/v1/completions \\ -H 'Content-Type: application/json' \\ -H 'Authorization: Bearer YOUR_API_KEY' \\ -d '{ \"prompt\": \"https://t.co/f93xEd2 Excited to share my latest blog post! ->\", \"max_tokens\": 1, \"model\": \"YOUR_FINE_TUNED_MODEL_NAME\" }' Which will return: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 { \"id\" : \"cmpl-COMPLETION_ID\" , \"object\" : \"text_completion\" , \"created\" : 1589498378 , \"model\" : \"YOUR_FINE_TUNED_MODEL_NAME\" , \"choices\" : [ { \"logprobs\" : { \"text_offset\" : [], \"token_logprobs\" : [ -0.03597255 ], \"tokens\" : [ \" positive\" ], \"top_logprobs\" : [ { \" negative\" : -4.9785037 , \" positive\" : -0.03597255 } ] }, \"text\" : \" positive\" , \"index\" : 0 , \"finish_reason\" : \"length\" } ] }","title":"Case study: Sentiment analysis"},{"location":"guides/fine-tuning/#case-study-categorization-for-email-triage","text":"Let's say you'd like to categorize incoming email into one of a large number of predefined categories. For classification into a large number of categories, we recommend you convert those categories into numbers, which will work well up to ~500 categories. We've observed that adding a space before the number sometimes slightly helps the performance, due to tokenization. You may want to structure your training data as follows: 1 { \"prompt\" : \"Subject: <email_subject>\\nFrom:<customer_name>\\nDate:<date>\\nContent:<email_body>\\n\\n###\\n\\n\" , \"completion\" : \" <numerical_category>\" } For example: 1 { \"prompt\" : \"Subject: Update my address\\nFrom:Joe Doe\\nTo:support@ourcompany.com\\nDate:2021-06-03\\nContent:Hi,\\nI would like to update my billing address to match my delivery address.\\n\\nPlease let me know once done.\\n\\nThanks,\\nJoe\\n\\n###\\n\\n\" , \"completion\" : \" 4\" } In the example above we used an incoming email capped at 2043 tokens as input. (This allows for a 4 token separator and a one token completion, summing up to 2048.) As a separator we used \\n\\n###\\n\\n and we removed any occurrence of ### within the email.","title":"Case study: Categorization for Email triage"},{"location":"guides/fine-tuning/#conditional-generation","text":"Conditional generation is a problem where the content needs to be generated given some kind of input. This includes paraphrasing, summarizing, entity extraction, product description writing given specifications, chatbots and many others. For this type of problem we recommend: Use a separator at the end of the prompt, e.g. \\n\\n###\\n\\n. Remember to also append this separator when you eventually make requests to your model. Use an ending token at the end of the completion, e.g. END Remember to add the ending token as a stop sequence during inference, e.g. stop=[\" END\"] Aim for at least ~500 examples Ensure that the prompt + completion doesn't exceed 2048 tokens, including the separator Ensure the examples are of high quality and follow the same desired format Ensure that the dataset used for finetuning is very similar in structure and type of task as what the model will be used for Using Lower learning rate and only 1-2 epochs tends to work better for these use cases","title":"Conditional generation"},{"location":"guides/fine-tuning/#case-study-write-an-engaging-ad-based-on-a-wikipedia-article","text":"This is a generative use case so you would want to ensure that the samples you provide are of the highest quality, as the fine-tuned model will try to imitate the style (and mistakes) of the given examples. A good starting point is around 500 examples. A sample dataset might look like this: 1 { \"prompt\" : \"<Product Name>\\n<Wikipedia description>\\n\\n###\\n\\n\" , \"completion\" : \" <engaging ad> END\" } For example: 1 2 3 4 { \"prompt\" : \"Samsung Galaxy Feel\\nThe Samsung Galaxy Feel is an Android smartphone developed by Samsung Electronics exclusively for the Japanese market. The phone was released in June 2017 and was sold by NTT Docomo. It runs on Android 7.0 (Nougat), has a 4.7 inch display, and a 3000 mAh battery.\\nSoftware\\nSamsung Galaxy Feel runs on Android 7.0 (Nougat), but can be later updated to Android 8.0 (Oreo).\\nHardware\\nSamsung Galaxy Feel has a 4.7 inch Super AMOLED HD display, 16 MP back facing and 5 MP front facing cameras. It has a 3000 mAh battery, a 1.6 GHz Octa-Core ARM Cortex-A53 CPU, and an ARM Mali-T830 MP1 700 MHz GPU. It comes with 32GB of internal storage, expandable to 256GB via microSD. Aside from its software and hardware specifications, Samsung also introduced a unique a hole in the phone's shell to accommodate the Japanese perceived penchant for personalizing their mobile phones. The Galaxy Feel's battery was also touted as a major selling point since the market favors handsets with longer battery life. The device is also waterproof and supports 1seg digital broadcasts using an antenna that is sold separately.\\n\\n###\\n\\n\" , \"completion\" : \"Looking for a smartphone that can do it all? Look no further than Samsung Galaxy Feel! With a slim and sleek design, our latest smartphone features high-quality picture and video capabilities, as well as an award winning battery life. END\" } Here we used a multi line separator, as Wikipedia articles contain multiple paragraphs and headings. We also used a simple end token, to ensure that the model knows when the completion should finish.","title":"Case study: Write an engaging ad based on a Wikipedia article"},{"location":"guides/fine-tuning/#case-study-entity-extraction","text":"This is similar to a language transformation task. To improve the performance, it is best to either sort different extracted entities alphabetically or in the same order as they appear in the original text. This will help the model to keep track of all the entities which need to be generated in order. The dataset could look as follows: 1 2 3 4 { \"prompt\" : \"<any text, for example news article>\\n\\n###\\n\\n\" , \"completion\" : \" <list of entities, separated by a newline> END\" } For example: 1 2 3 4 { \"prompt\" : \"Portugal will be removed from the UK's green travel list from Tuesday, amid rising coronavirus cases and concern over a \\\"Nepal mutation of the so-called Indian variant\\\". It will join the amber list, meaning holidaymakers should not visit and returnees must isolate for 10 days...\\n\\n###\\n\\n\" , \"completion\" : \" Portugal\\nUK\\nNepal mutation\\nIndian variant END\" } A multi-line separator works best, as the text will likely contain multiple lines. Ideally there will be a high diversity of the types of input prompts (news articles, Wikipedia pages, tweets, legal documents), which reflect the likely texts which will be encountered when extracting entities.","title":"Case study: Entity extraction"},{"location":"guides/fine-tuning/#case-study-customer-support-chatbot","text":"A chatbot will normally contain relevant context about the conversation (order details), summary of the conversation so far as well as most recent messages. For this use case the same past conversation can generate multiple rows in the dataset, each time with a slightly different context, for every agent generation as a completion. This use case will require a few thousand examples, as it will likely deal with different types of requests, and customer issues. To ensure the performance is of high quality we recommend vetting the conversation samples to ensure the quality of agent messages. The summary can be generated with a separate text transformation fine tuned model. The dataset could look as follows: 1 2 3 4 { \"prompt\" : \"Summary: <summary of the interaction so far>\\n\\nSpecific information:<for example order details in natural language>\\n\\n###\\n\\nCustomer: <message1>\\nAgent: <response1>\\nCustomer: <message2>\\nAgent:\" , \"completion\" : \" <response2>\\n\" } 1 2 3 4 { \"prompt\" : \"Summary: <summary of the interaction so far>\\n\\nSpecific information:<for example order details in natural language>\\n\\n###\\n\\nCustomer: <message1>\\nAgent: <response1>\\nCustomer: <message2>\\nAgent: <response2>\\nCustomer: <message3>\\nAgent:\" , \"completion\" : \" <response3>\\n\" } Here we purposefully separated different types of input information, but maintained Customer Agent dialog in the same format between a prompt and a completion. All the completions should only be by the agent, and we can use \\n as a stop sequence when doing inference. Case study: Product description based on a technical list of properties Here it is important to convert the input data into a natural language, which will likely lead to superior performance. For example, the following format: 1 2 3 4 { \"prompt\" : \"Item=handbag, Color=army_green, price=$99, size=S->\" , \"completion\" : \" This stylish small green handbag will add a unique touch to your look, without costing you a fortune.\" } Won't work as well as: 1 2 3 4 { \"prompt\" : \"Item is a handbag. Colour is army green. Price is midrange. Size is small.->\" , \"completion\" : \" This stylish small green handbag will add a unique touch to your look, without costing you a fortune.\" } For high performance ensure that the completions were based on the description provided. If external content is often consulted, then adding such content in an automated way would improve the performance. If the description is based on images, it may help to use an algorithm to extract a textual description of the image. Since completions are only one sentence long, we can use . as the stop sequence during inference.","title":"Case study: Customer support chatbot"},{"location":"guides/fine-tuning/#advanced-usage","text":"","title":"Advanced usage"},{"location":"guides/fine-tuning/#customize-your-model-name","text":"You can add a suffix of up to 40 characters to your fine-tuned model name using the suffix parameter. OpenAI CLI: 1 openai api fine_tunes.create -t test.jsonl -m ada --suffix \"custom model name\" The resulting name would be: ada:ft-your-org:custom-model-name-2022-02-15-04-21-04","title":"Customize your model name"},{"location":"guides/fine-tuning/#analyzing-your-fine-tuned-model","text":"We attach a result file to each job once it has been completed. This results file ID will be listed when you retrieve a fine-tune, and also when you look at the events on a fine-tune. You can download these files: OpenAI CLI: 1 openai api fine_tunes.results -i <YOUR_FINE_TUNE_JOB_ID> CURL: 1 2 curl https://api.openai.com/v1/files/ $RESULTS_FILE_ID /content \\ -H \"Authorization: Bearer $OPENAI_API_KEY \" > results.csv The _results.csv file contains a row for each training step, where a step refers to one forward and backward pass on a batch of data. In addition to the step number, each row contains the following fields corresponding to that step: elapsed_tokens : the number of tokens the model has seen so far (including repeats) elapsed_examples : the number of examples the model has seen so far (including repeats), where one example is one element in your batch. For example, if batch_size = 4, each step will increase elapsed_examples by 4. training_loss : loss on the training batch training_sequence_accuracy : the percentage of completions in the training batch for which the model's predicted tokens matched the true completion tokens exactly. For example, with a batch_size of 3, if your data contains the completions [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be \u2154 = 0.67 training_token_accuracy : the percentage of tokens in the training batch that were correctly predicted by the model. For example, with a batch_size of 3, if your data contains the completions [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be \u215a = 0.83","title":"Analyzing your fine-tuned model"},{"location":"guides/fine-tuning/#classification-specific-metrics","text":"We also provide the option of generating additional classification-specific metrics in the results file, such as accuracy and weighted F1 score. These metrics are periodically calculated against the full validation set and at the end of fine-tuning. You will see them as additional columns in your results file. To enable this, set the parameter --compute_classification_metrics. Additionally, you must provide a validation file, and set either the classification_n_classes parameter, for multiclass classification, or classification_positive_class, for binary classification. OpenAI CLI: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # For multiclass classification openai api fine_tunes.create \\ -t <TRAIN_FILE_ID_OR_PATH> \\ -v <VALIDATION_FILE_OR_PATH> \\ -m <MODEL> \\ --compute_classification_metrics \\ --classification_n_classes <N_CLASSES> # For binary classification openai api fine_tunes.create \\ -t <TRAIN_FILE_ID_OR_PATH> \\ -v <VALIDATION_FILE_OR_PATH> \\ -m <MODEL> \\ --compute_classification_metrics \\ --classification_n_classes 2 \\ --classification_positive_class <POSITIVE_CLASS_FROM_DATASET> The following metrics will be displayed in your results file if you set --compute_classification_metrics:","title":"Classification specific metrics"},{"location":"guides/fine-tuning/#for-multiclass-classification","text":"classification/accuracy: accuracy classification/weighted_f1_score: weighted F-1 score","title":"For multiclass classification"},{"location":"guides/fine-tuning/#for-binary-classification","text":"The following metrics are based on a classification threshold of 0.5 (i.e. when the probability is > 0.5, an example is classified as belonging to the positive class.) classification/accuracy classification/precision classification/recall classification/f{beta} classification/auroc - AUROC classification/auprc - AUPRC Note that these evaluations assume that you are using text labels for classes that tokenize down to a single token, as described above. If these conditions do not hold, the numbers you get will likely be wrong.","title":"For binary classification"},{"location":"guides/fine-tuning/#validation","text":"You can reserve some of your data for validation. A validation file has exactly the same format as a train file, and your train and validation data should be mutually exclusive. If you include a validation file when creating your fine-tune job, the generated results file will include evaluations on how well the fine-tuned model performs against your validation data at periodic intervals during training. OpenAI CLI: 1 2 3 openai api fine_tunes.create -t <TRAIN_FILE_ID_OR_PATH> \\ -v <VALIDATION_FILE_ID_OR_PATH> \\ -m <MODEL> If you provided a validation file, we periodically calculate metrics on batches of validation data during training time. You will see the following additional metrics in your results file: validation_loss: loss on the validation batch validation_sequence_accuracy: the percentage of completions in the validation batch for which the model's predicted tokens matched the true completion tokens exactly. For example, with a batch_size of 3, if your data contains the completion [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be \u2154 = 0.67 validation_token_accuracy: the percentage of tokens in the validation batch that were correctly predicted by the model. For example, with a batch_size of 3, if your data contains the completion [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be \u215a = 0.83","title":"Validation"},{"location":"guides/fine-tuning/#hyperparameters","text":"We've picked default hyperparameters that work well across a range of use cases. The only required parameter is the training file. That said, tweaking the hyperparameters used for fine-tuning can often lead to a model that produces higher quality output. In particular, you may want to configure the following: model: The name of the base model to fine-tune. You can select one of \"ada\", \"babbage\", \"curie\", or \"davinci\". To learn more about these models, see the Models documentation. n_epochs - defaults to 4. The number of epochs to train the model for. An epoch refers to one full cycle through the training dataset. batch_size - defaults to ~0.2% of the number of examples in the training set, capped at 256. The batch size is the number of training examples used to train a single forward and backward pass. In general, we've found that larger batch sizes tend to work better for larger datasets. learning_rate_multiplier - defaults to 0.05, 0.1, or 0.2 depending on final batch_size. The fine-tuning learning rate is the original learning rate used for pretraining multiplied by this multiplier. We recommend experimenting with values in the range 0.02 to 0.2 to see what produces the best results. Empirically, we've found that larger learning rates often perform better with larger batch sizes. compute_classification_metrics - defaults to False. If True, for fine-tuning for classification tasks, computes classification-specific metrics (accuracy, F-1 score, etc) on the validation set at the end of every epoch. To configure these additional hyperparameters, pass them in via command line flags on the OpenAI CLI, for example: 1 2 3 4 openai api fine_tunes.create \\ -t file-JD89ePi5KMsB3Tayeli5ovfW \\ -m ada \\ --n_epochs 1","title":"Hyperparameters"},{"location":"guides/fine-tuning/#continue-fine-tuning-from-a-fine-tuned-model","text":"If you have already fine-tuned a model for your task and now have additional training data that you would like to incorporate, you can continue fine-tuning from the model. This creates a model that has learned from all of the training data without having to re-train from scratch. To do this, pass in the fine-tuned model name when creating a new fine-tuning job (e.g. -m curie:ft- - ). Other training parameters do not have to be changed, however if your new training data is much smaller than your previous training data, you may find it useful to reduce learning_rate_multiplier by a factor of 2 to 4. Weights & Biases You can sync your fine-tunes with Weights & Biases to track experiments, models, and datasets. To get started, you will need a Weights & Biases account and a paid OpenAI plan. To make sure you are using the lastest version of openai and wandb, run: 1 pip install --upgrade openai wandb To sync your fine-tunes with Weights & Biases, run: 1 openai wandb sync You can read the Weights & Biases documentation for more information on this integration.","title":"Continue fine-tuning from a fine-tuned model"},{"location":"guides/fine-tuning/#example-notebooks","text":"","title":"Example notebooks"},{"location":"guides/fine-tuning/#classification","text":"finetuning-classification.ipynb This notebook will demonstrate how to fine-tune a model that can classify whether a piece of input text is related to Baseball or Hockey. We will perform this task in four steps in the notebook: Data exploration will give an overview of the data source and what an example looks like Data preparation will turn our data source into a jsonl file that can be used for fine-tuning Fine-tuning will kick off the fine-tuning job and explain the resulting model's performance Using the model will demonstrate making requests to the fine-tuned model to get predictions.","title":"Classification"},{"location":"guides/fine-tuning/#question-answering","text":"olympics-1-collect-data.ipynbolympics-2-create-qa.ipynbolympics-3-train-qa.ipynb The idea of this project is to create a question answering model, based on a few paragraphs of provided text. Base GPT-3 models do a good job at answering questions when the answer is contained within the paragraph, however if the answer isn't contained, the base models tend to try their best to answer anyway, often leading to confabulated answers. To create a model which answers questions only if there is sufficient context for doing so, we first create a dataset of questions and answers based on paragraphs of text. In order to train the model to answer only when the answer is present, we also add adversarial examples, where the question doesn't match the context. In those cases, we ask the model to output \"No sufficient context for answering the question\". We will perform this task in three notebooks: The first notebook focuses on collecting recent data, which GPT-3 didn't see during it's pre-training. We picked the topic of Olympic Games 2020 (which actually took place in the summer of 2021), and downloaded 713 unique pages. We organized the dataset by individual sections, which will serve as context for asking and answering the questions. The second notebook will utilize Davinci-instruct to ask a few questions based on a Wikipedia section, as well as answer those questions, based on that section. The third notebook will utilize the dataset of context, question and answer pairs to additionally create adversarial questions and context pairs, where the question was not generated on that context. In those cases the model will be prompted to answer \"No sufficient context for answering the question\". We will also train a discriminator model, which predicts whether the question can be answered based on the context or not.","title":"Question answering"},{"location":"guides/images/","text":"\u56fe\u50cf\u751f\u6210 Beta \u00b6 Learn how to generate or manipulate images with our DALL\u00b7E models Introduction \u00b6 The Images API provides three methods for interacting with images: Creating images from scratch based on a text prompt Creating edits of an existing image based on a new text prompt Creating variations of an existing image This guide covers the basics of using these three API endpoints with useful code samples. To see them in action, check out our DALL\u00b7E preview app. The Images API is in beta. During this time the API and models will evolve based on your feedback. To ensure all users can prototype comfortably, the default rate limit is 50 images per minute. If you would like to increase your rate limit, please review this help center article. We will increase the default rate limit as we learn more about usage and capacity requirements. Usage Generations The image generations endpoint allows you to create an original image given a text prompt. Generated images can have a size of 256x256, 512x512, or 1024x1024 pixels. Smaller sizes are faster to generate. You can request 1-10 images at a time using the n parameter. Generate an image node.js node.js 1 2 3 4 5 6 const response = await openai . createImage ({ prompt : \"a white siamese cat\" , n : 1 , size : \"1024x1024\" , }); image_url = response . data . data [ 0 ]. url ; The more detailed the description, the more likely you are to get the result that you or your end user want. You can explore the examples in the DALL\u00b7E preview app for more prompting inspiration. Here's a quick example: PROMPT GENERATION a white siamese cat a close up, studio photographic portrait of a white siamese cat that looks curious, backlit ears Each image can be returned as either a URL or Base64 data, using the response_format parameter. URLs will expire after an hour. Edits The image edits endpoint allows you to edit and extend an image by uploading a mask. The transparent areas of the mask indicate where the image should be edited, and the prompt should describe the full new image, not just the erased area. This endpoint can enable experiences like the editor in our DALL\u00b7E preview app. Edit an image node.js node.js 1 2 3 4 5 6 7 8 const response = await openai . createImageEdit ( fs . createReadStream ( \"sunlit_lounge.png\" ), fs . createReadStream ( \"mask.png\" ), \"A sunlit indoor lounge area with a pool containing a flamingo\" , 1 , \"1024x1024\" ); image_url = response . data . data [ 0 ]. url ; IMAGE MASK OUTPUT Prompt: a sunlit indoor lounge area with a pool containing a flamingo The uploaded image and mask must both be square PNG images less than 4MB in size, and also must have the same dimensions as each other. The non-transparent areas of the mask are not used when generating the output, so they don\u2019t necessarily need to match the original image like the example above. Variations The image variations endpoint allows you to generate a variation of a given image. Generate an image variation node.js node.js 1 2 3 4 5 6 const response = await openai . createImageVariation ( fs . createReadStream ( \"corgi_and_cat_paw.png\" ), 1 , \"1024x1024\" ); image_url = response . data . data [ 0 ]. url ; IMAGE OUTPUT Similar to the edits endpoint, the input image must be a square PNG image less than 4MB in size. Content moderation Prompts and images are filtered based on our content policy, returning an error when a prompt or image is flagged. If you have any feedback on false positives or related issues, please contact us through our help center. Language-specific tips NODE.JS PYTHON Using in-memory image data The Node.js examples in the guide above use the fs module to read image data from disk. In some cases, you may have your image data in memory instead. Here's an example API call that uses image data stored in a Node.js Buffer object: 1 2 3 4 5 6 7 8 9 // This is the Buffer object that contains your image data const buffer = [ your image data ]; // Set a `name` that ends with .png so that the API knows it's a PNG image buffer . name = \"image.png\" ; const response = await openai . createImageVariation ( buffer , 1 , \"1024x1024\" ); Working with TypeScript \u00b6 If you're using TypeScript, you may encounter some quirks with image file arguments. Here's an example of working around the type mismatch by explicitly casting the argument: 1 2 3 4 5 6 // Cast the ReadStream to `any` to appease the TypeScript compiler const response = await openai . createImageVariation ( fs . createReadStream ( \"image.png\" ) as any , 1 , \"1024x1024\" ); And here's a similar example for in-memory image data: 1 2 3 4 5 6 7 8 9 10 11 // This is the Buffer object that contains your image data const buffer : Buffer = [ your image data ]; // Cast the buffer to `any` so that we can set the `name` property const file : any = buffer ; // Set a `name` that ends with .png so that the API knows it's a PNG image file . name = \"image.png\" ; const response = await openai . createImageVariation ( file , 1 , \"1024x1024\" ); Error handling \u00b6 API requests can potentially return errors due to invalid inputs, rate limits, or other issues. These errors can be handled with a try...catch statement, and the error details can be found in either error.response or error.message: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 try { const response = await openai . createImageVariation ( fs . createReadStream ( \"image.png\" ), 1 , \"1024x1024\" ); console . log ( response . data . data [ 0 ]. url ); } catch ( error ) { if ( error . response ) { console . log ( error . response . status ); console . log ( error . response . data ); } else { console . log ( error . message ); } }","title":"\u56fe\u50cf\u751f\u6210"},{"location":"guides/images/#beta","text":"Learn how to generate or manipulate images with our DALL\u00b7E models","title":"\u56fe\u50cf\u751f\u6210 Beta"},{"location":"guides/images/#introduction","text":"The Images API provides three methods for interacting with images: Creating images from scratch based on a text prompt Creating edits of an existing image based on a new text prompt Creating variations of an existing image This guide covers the basics of using these three API endpoints with useful code samples. To see them in action, check out our DALL\u00b7E preview app. The Images API is in beta. During this time the API and models will evolve based on your feedback. To ensure all users can prototype comfortably, the default rate limit is 50 images per minute. If you would like to increase your rate limit, please review this help center article. We will increase the default rate limit as we learn more about usage and capacity requirements. Usage Generations The image generations endpoint allows you to create an original image given a text prompt. Generated images can have a size of 256x256, 512x512, or 1024x1024 pixels. Smaller sizes are faster to generate. You can request 1-10 images at a time using the n parameter. Generate an image node.js node.js 1 2 3 4 5 6 const response = await openai . createImage ({ prompt : \"a white siamese cat\" , n : 1 , size : \"1024x1024\" , }); image_url = response . data . data [ 0 ]. url ; The more detailed the description, the more likely you are to get the result that you or your end user want. You can explore the examples in the DALL\u00b7E preview app for more prompting inspiration. Here's a quick example: PROMPT GENERATION a white siamese cat a close up, studio photographic portrait of a white siamese cat that looks curious, backlit ears Each image can be returned as either a URL or Base64 data, using the response_format parameter. URLs will expire after an hour. Edits The image edits endpoint allows you to edit and extend an image by uploading a mask. The transparent areas of the mask indicate where the image should be edited, and the prompt should describe the full new image, not just the erased area. This endpoint can enable experiences like the editor in our DALL\u00b7E preview app. Edit an image node.js node.js 1 2 3 4 5 6 7 8 const response = await openai . createImageEdit ( fs . createReadStream ( \"sunlit_lounge.png\" ), fs . createReadStream ( \"mask.png\" ), \"A sunlit indoor lounge area with a pool containing a flamingo\" , 1 , \"1024x1024\" ); image_url = response . data . data [ 0 ]. url ; IMAGE MASK OUTPUT Prompt: a sunlit indoor lounge area with a pool containing a flamingo The uploaded image and mask must both be square PNG images less than 4MB in size, and also must have the same dimensions as each other. The non-transparent areas of the mask are not used when generating the output, so they don\u2019t necessarily need to match the original image like the example above. Variations The image variations endpoint allows you to generate a variation of a given image. Generate an image variation node.js node.js 1 2 3 4 5 6 const response = await openai . createImageVariation ( fs . createReadStream ( \"corgi_and_cat_paw.png\" ), 1 , \"1024x1024\" ); image_url = response . data . data [ 0 ]. url ; IMAGE OUTPUT Similar to the edits endpoint, the input image must be a square PNG image less than 4MB in size. Content moderation Prompts and images are filtered based on our content policy, returning an error when a prompt or image is flagged. If you have any feedback on false positives or related issues, please contact us through our help center. Language-specific tips NODE.JS PYTHON Using in-memory image data The Node.js examples in the guide above use the fs module to read image data from disk. In some cases, you may have your image data in memory instead. Here's an example API call that uses image data stored in a Node.js Buffer object: 1 2 3 4 5 6 7 8 9 // This is the Buffer object that contains your image data const buffer = [ your image data ]; // Set a `name` that ends with .png so that the API knows it's a PNG image buffer . name = \"image.png\" ; const response = await openai . createImageVariation ( buffer , 1 , \"1024x1024\" );","title":"Introduction"},{"location":"guides/images/#working-with-typescript","text":"If you're using TypeScript, you may encounter some quirks with image file arguments. Here's an example of working around the type mismatch by explicitly casting the argument: 1 2 3 4 5 6 // Cast the ReadStream to `any` to appease the TypeScript compiler const response = await openai . createImageVariation ( fs . createReadStream ( \"image.png\" ) as any , 1 , \"1024x1024\" ); And here's a similar example for in-memory image data: 1 2 3 4 5 6 7 8 9 10 11 // This is the Buffer object that contains your image data const buffer : Buffer = [ your image data ]; // Cast the buffer to `any` so that we can set the `name` property const file : any = buffer ; // Set a `name` that ends with .png so that the API knows it's a PNG image file . name = \"image.png\" ; const response = await openai . createImageVariation ( file , 1 , \"1024x1024\" );","title":"Working with TypeScript"},{"location":"guides/images/#error-handling","text":"API requests can potentially return errors due to invalid inputs, rate limits, or other issues. These errors can be handled with a try...catch statement, and the error details can be found in either error.response or error.message: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 try { const response = await openai . createImageVariation ( fs . createReadStream ( \"image.png\" ), 1 , \"1024x1024\" ); console . log ( response . data . data [ 0 ]. url ); } catch ( error ) { if ( error . response ) { console . log ( error . response . status ); console . log ( error . response . data ); } else { console . log ( error . message ); } }","title":"Error handling"},{"location":"guides/moderation/","text":"\u9002\u5ea6 \u00b6 Overview The moderation endpoint is a tool you can use to check whether content complies with OpenAI's usage policies. Developers can thus identify content that our usage policies prohibits and take action, for instance by filtering it. The models classifies the following categories: CATEGORY DESCRIPTION hate Content that expresses, incites, or promotes hate based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status, or caste. hate/threatening Hateful content that also includes violence or serious harm towards the targeted group. self-harm Content that promotes, encourages, or depicts acts of self-harm, such as suicide, cutting, and eating disorders. sexual Content meant to arouse sexual excitement, such as the description of sexual activity, or that promotes sexual services (excluding sex education and wellness). sexual/minors Sexual content that includes an individual who is under 18 years old. violence Content that promotes or glorifies violence or celebrates the suffering or humiliation of others. violence/graphic Violent content that depicts death, violence, or serious physical injury in extreme graphic detail. The moderation endpoint is free to use when monitoring the inputs and outputs of OpenAI APIs. We currently do not support monitoring of third-party traffic. We are continuously working to improve the accuracy of our classifier and are especially working to improve the classifications of hate, self-harm, and violence/graphic content. Our support for non-English languages is currently limited. Quickstart \u00b6 To obtain a classification for a piece of text, make a request to the moderation endpoint as demonstrated in the following code snippets: Example: Getting moderations curl curl 1 2 3 4 5 curl https://api.openai.com/v1/moderations \\ -X POST \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer $OPENAI_API_KEY \" \\ -d '{\"input\": \"Sample text goes here\"}' Below is an example output of the endpoint. It returns the following fields: flagged: Set to true if the model classifies the content as violating OpenAI's usage policies, false otherwise. categories: Contains a dictionary of per-category binary usage policies violation flags. For each category, the value is true if the model flags the corresponding category as violated, false otherwise. category_scores: Contains a dictionary of per-category raw scores output by the model, denoting the model's confidence that the input violates the OpenAI's policy for the category. The value is between 0 and 1, where higher values denote higher confidence. The scores should not be interpreted as probabilities. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 { \"id\" : \"modr-XXXXX\" , \"model\" : \"text-moderation-001\" , \"results\" : [ { \"categories\" : { \"hate\" : false , \"hate/threatening\" : false , \"self-harm\" : false , \"sexual\" : false , \"sexual/minors\" : false , \"violence\" : false , \"violence/graphic\" : false }, \"category_scores\" : { \"hate\" : 0.18805529177188873 , \"hate/threatening\" : 0.0001250059431185946 , \"self-harm\" : 0.0003706029092427343 , \"sexual\" : 0.0008735615410842001 , \"sexual/minors\" : 0.0007470346172340214 , \"violence\" : 0.0041268812492489815 , \"violence/graphic\" : 0.00023186142789199948 }, \"flagged\" : false } ] } OpenAI will continuously upgrade the moderation endpoint's underlying model. Therefore, custom policies that rely on category_scores may need recalibration over time.","title":"\u9002\u5ea6"},{"location":"guides/moderation/#_1","text":"Overview The moderation endpoint is a tool you can use to check whether content complies with OpenAI's usage policies. Developers can thus identify content that our usage policies prohibits and take action, for instance by filtering it. The models classifies the following categories: CATEGORY DESCRIPTION hate Content that expresses, incites, or promotes hate based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status, or caste. hate/threatening Hateful content that also includes violence or serious harm towards the targeted group. self-harm Content that promotes, encourages, or depicts acts of self-harm, such as suicide, cutting, and eating disorders. sexual Content meant to arouse sexual excitement, such as the description of sexual activity, or that promotes sexual services (excluding sex education and wellness). sexual/minors Sexual content that includes an individual who is under 18 years old. violence Content that promotes or glorifies violence or celebrates the suffering or humiliation of others. violence/graphic Violent content that depicts death, violence, or serious physical injury in extreme graphic detail. The moderation endpoint is free to use when monitoring the inputs and outputs of OpenAI APIs. We currently do not support monitoring of third-party traffic. We are continuously working to improve the accuracy of our classifier and are especially working to improve the classifications of hate, self-harm, and violence/graphic content. Our support for non-English languages is currently limited.","title":"\u9002\u5ea6"},{"location":"guides/moderation/#quickstart","text":"To obtain a classification for a piece of text, make a request to the moderation endpoint as demonstrated in the following code snippets: Example: Getting moderations curl curl 1 2 3 4 5 curl https://api.openai.com/v1/moderations \\ -X POST \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer $OPENAI_API_KEY \" \\ -d '{\"input\": \"Sample text goes here\"}' Below is an example output of the endpoint. It returns the following fields: flagged: Set to true if the model classifies the content as violating OpenAI's usage policies, false otherwise. categories: Contains a dictionary of per-category binary usage policies violation flags. For each category, the value is true if the model flags the corresponding category as violated, false otherwise. category_scores: Contains a dictionary of per-category raw scores output by the model, denoting the model's confidence that the input violates the OpenAI's policy for the category. The value is between 0 and 1, where higher values denote higher confidence. The scores should not be interpreted as probabilities. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 { \"id\" : \"modr-XXXXX\" , \"model\" : \"text-moderation-001\" , \"results\" : [ { \"categories\" : { \"hate\" : false , \"hate/threatening\" : false , \"self-harm\" : false , \"sexual\" : false , \"sexual/minors\" : false , \"violence\" : false , \"violence/graphic\" : false }, \"category_scores\" : { \"hate\" : 0.18805529177188873 , \"hate/threatening\" : 0.0001250059431185946 , \"self-harm\" : 0.0003706029092427343 , \"sexual\" : 0.0008735615410842001 , \"sexual/minors\" : 0.0007470346172340214 , \"violence\" : 0.0041268812492489815 , \"violence/graphic\" : 0.00023186142789199948 }, \"flagged\" : false } ] } OpenAI will continuously upgrade the moderation endpoint's underlying model. Therefore, custom policies that rely on category_scores may need recalibration over time.","title":"Quickstart"},{"location":"guides/production-best-practices/","text":"\u751f\u4ea7\u6700\u4f73\u5b9e\u8df5 \u00b6 This guide provides a comprehensive set of best practices to help you transition from prototype to production. Whether you are a seasoned machine learning engineer or a recent enthusiast, this guide should provide you with the tools you need to successfully put the platform to work in a production setting: from securing access to our API to designing a robust architecture that can handle high traffic volumes. Use this guide to help develop a plan for deploying your application as smoothly and effectively as possible. Setting up your organization \u00b6 Once you log in to your OpenAI account, you can find your organization name and ID in your organization settings. The organization name is the label for your organization, shown in user interfaces. The organization ID is the unique identifier for your organization which can be used in API requests. Users who belong to multiple organizations can pass a header to specify which organization is used for an API request. Usage from these API requests will count against the specified organization's quota. If no header is provided, the default organization will be billed. You can change your default organization in your user settings. You can invite new members to your organization from the members settings page. Members can be readers or owners. Readers can make API requests and view basic organization information, while owners can modify billing information and manage members within an organization. Managing billing limits \u00b6 New free trial users receive an initial credit of $5 that expires after three months. Once the credit has been used or expires, you can choose to enter billing information to continue your use of the API. If no billing information is entered, you will still have login access but will be unable to make any further API requests. Once you\u2019ve entered your billing information, you will have an approved usage limit of $120 per month, which is set by OpenAI. To increase your quota beyond the $120 monthly billing limit, please submit a quota increase request. If you\u2019d like to be notified when your usage exceeds a certain amount, you can set a soft limit through the usage limits page. When the soft limit is reached, the owners of the organization will receive an email notification. You can also set a hard limit so that, once the hard limit is reached, any subsequent API requests will be rejected. Note that these limits are best effort, and there may be 5 to 10 minutes of delay between the usage and the limits being enforced. API keys \u00b6 The OpenAI API uses API keys for authentication. Visit your API keys page to retrieve the API key you'll use in your requests. This is a relatively straightforward way to control access, but you must be vigilant about securing these keys. Avoid exposing the API keys in your code or in public repositories; instead, store them in a secure location. You should expose your keys to your application using environment variables or secret management service, so that you don't need to hard-code them in your codebase. Read more in our Best practices for API key safety. Staging accounts \u00b6 As you scale, you may want to create separate organizations for your staging and production environments. Please note that you can sign up using two separate email addresses like bob+prod@widgetcorp.com and bob+dev@widgetcorp.com to create two organizations. This will allow you to isolate your development and testing work so you don't accidentally disrupt your live application. You can also limit access to your production organization this way. Building your prototype \u00b6 If you haven\u2019t gone through the quickstart guide, we recommend you start there before diving into the rest of this guide. For those new to the OpenAI API, our playground can be a great resource for exploring its capabilities. Doing so will help you learn what's possible and where you may want to focus your efforts. You can also explore our example prompts. While the playground is a great place to prototype, it can also be used as an incubation area for larger projects. The playground also makes it easy to export code snippets for API requests and share prompts with collaborators, making it an integral part of your development process. Additional tips \u00b6 Start by determining the core functionalities you want your application to have. Consider the types of data inputs, outputs, and processes you will need. Aim to keep the prototype as focused as possible, so that you can iterate quickly and efficiently. Choose the programming language and framework that you feel most comfortable with and that best aligns with your goals for the project. Some popular options include Python, Java, and Node.js. See library support page to learn more about the library bindings maintained both by our team and by the broader developer community. Development environment and support: Set up your development environment with the right tools and libraries and ensure you have the resources you need to train your model. Leverage our documentation, community forum and our help center to get help with troubleshooting. If you are developing using Python, take a look at this structuring your project guide (repository structure is a crucial part of your project\u2019s architecture). In order to connect with our support engineers, simply log in to your account and use the \"Help\" button to start a conversation. Techniques for improving reliability around prompts Even with careful planning, it's important to be prepared for unexpected issues when using GPT-3 in your application. In some cases, the model may fail on a task, so it's helpful to consider what you can do to improve the reliability of your application. If your task involves logical reasoning or complexity, you may need to take additional steps to build more reliable prompts. For some helpful suggestions, consult our Techniques to improve reliability guide. Overall the recommendations revolve around: Decomposing unreliable operations into smaller, more reliable operations (e.g., selection-inference prompting) Using multiple steps or multiple relationships to make the system's reliability greater than any individual component (e.g., maieutic prompting) Evaluation and iteration One of the most important aspects of developing a system for production is regular evaluation and iterative experimentation. This process allows you to measure performance, troubleshoot issues, and fine-tune your models to improve accuracy and efficiency. A key part of this process is creating an evaluation dataset for your functionality. Here are a few things to keep in mind: Make sure your evaluation set is representative of the data your model will be used on in the real world. This will allow you to assess your model's performance on data it hasn't seen before and help you understand how well it generalizes to new situations. Regularly update your evaluation set to ensure that it stays relevant as your model evolves and as new data becomes available. Use a variety of metrics to evaluate your model's performance. Depending on your application and business outcomes, this could include accuracy, precision, recall, F1 score, or mean average precision (MAP). Additionally, you can sync your fine-tunes with Weights & Biases to track experiments, models, and datasets. Compare your model's performance against baseline. This will give you a better understanding of your model's strengths and weaknesses and can help guide your future development efforts. By conducting regular evaluation and iterative experimentation, you can ensure that your GPT-powered application or prototype continues to improve over time. Evaluating language models \u00b6 Language models can be difficult to evaluate because evaluating the quality of generated language is often subjective, and there are many different ways to communicate the same message correctly in language. For example, when evaluating a model on the ability to summarize a long passage of text, there are many correct summaries. That being said, designing good evaluations is critical to making progress in machine learning. An eval suite needs to be comprehensive, easy to run, and reasonably fast (depending on model size). It also needs to be easy to continue to add to the suite as what is comprehensive one month will likely be out of date in another month. We should prioritize having a diversity of tasks and tasks that identify weaknesses in the models or capabilities that are not improving with scaling. The simplest way to evaluate your system is to manually inspect its outputs. Is it doing what you want? Are the outputs high quality? Are they consistent? Automated evaluations \u00b6 The best way to test faster is to develop automated evaluations. However, this may not be possible in more subjective applications like summarization tasks. Automated evaluations work best when it\u2019s easy to grade a final output as correct or incorrect. For example, if you\u2019re fine-tuning a classifier to classify text strings as class A or class B, it\u2019s fairly simple: create a test set with example input and output pairs, run your system on the inputs, and then grade the system outputs versus the correct outputs (looking at metrics like accuracy, F1 score, cross-entropy, etc.). If your outputs are semi open-ended, as they might be for a meeting notes summarizer, it can be trickier to define success: for example, what makes one summary better than another? Here, possible techniques include: Writing a test with \u2018gold standard\u2019 answers and then measuring some sort of similarity score between each gold standard answer and the system output (we\u2019ve seen embeddings work decently well for this) Building a discriminator system to judge / rank outputs, and then giving that discriminator a set of outputs where one is generated by the system under test (this can even be GPT model that is asked whether the question is answered correctly by a given output) Building an evaluation model that checks for the truth of components of the answer; e.g., detecting whether a quote actually appears in the piece of given text For very open-ended tasks, such as a creative story writer, automated evaluation is more difficult. Although it might be possible to develop quality metrics that look at spelling errors, word diversity, and readability scores, these metrics don\u2019t really capture the creative quality of a piece of writing. In cases where no good automated metric can be found, human evaluations remain the best method. Example procedure for evaluating a GPT-3-based system As an example, let\u2019s consider the case of building a retrieval-based Q&A system. A retrieval-based Q&A system has two steps. First, a user\u2019s query is used to rank potentially relevant documents in a knowledge base. Second, GPT-3 is given the top-ranking documents and asked to generate an answer to the query. Evaluations can be made to measure the performance of each step. For the search step, one could: First, generate a test set with ~100 questions and a set of correct documents for each The questions can be sourced from user data if you have any; otherwise, you can invent a set of questions with diverse styles and difficulty. For each question, have a person manually search through the knowledge base and record the set of documents that contain the answer. Second, use the test set to grade the system\u2019s performance For each question, use the system to rank the candidate documents (e.g., by cosine similarity of the document embeddings with the query embedding). You can score the results with a binary accuracy score of 1 if the candidate documents contain at least 1 relevant document from the answer key and 0 otherwise You can also use a continuous metric like Mean Reciprocal Rank which can help distinguish between answers that were close to being right or far from being right (e.g., a score of 1 if the correct document is rank 1, a score of \u00bd if rank 2, a score of \u2153 if rank 3, etc.) For the question answering step, one could: First, generate a test set with ~100 sets of {question, relevant text, correct answer} For the questions and relevant texts, use the above data For the correct answers, have a person write down ~100 examples of what a great answer looks like. Second, use the test set to grade the system\u2019s performance For each question & text pair, combine them into a prompt and submit the prompt to GPT-3 Next, compare GPT-3\u2019s answers to the gold-standard answer written by a human This comparison can be manual, where humans look at them side by side and grade whether the GPT-3 answer is correct/high quality This comparison can also be automated, by using embedding similarity scores or another method (automated methods will likely be noisy, but noise is ok as long as it\u2019s unbiased and equally noisy across different types of models that you\u2019re testing against one another) Of course, N=100 is just an example, and in early stages, you might start with a smaller set that\u2019s easier to generate, and in later stages, you might invest in a larger set that\u2019s more costly but more statistically reliable. Scaling your solution architecture When designing your application or service for production that uses our API, it's important to consider how you will scale to meet traffic demands. There are a few key areas you will need to consider regardless of the cloud service provider of your choice: Horizontal scaling: You may want to scale your application out horizontally to accommodate requests to your application that come from multiple sources. This could involve deploying additional servers or containers to distribute the load. If you opt for this type of scaling, make sure that your architecture is designed to handle multiple nodes and that you have mechanisms in place to balance the load between them. Vertical scaling: Another option is to scale your application up vertically, meaning you can beef up the resources available to a single node. This would involve upgrading your server's capabilities to handle the additional load. If you opt for this type of scaling, make sure your application is designed to take advantage of these additional resources. Caching: By storing frequently accessed data, you can improve response times without needing to make repeated calls to our API. Your application will need to be designed to use cached data whenever possible and invalidate the cache when new information is added. There are a few different ways you could do this. For example, you could store data in a database, filesystem, or in-memory cache, depending on what makes the most sense for your application. Load balancing: Finally, consider load-balancing techniques to ensure requests are distributed evenly across your available servers. This could involve using a load balancer in front of your servers or using DNS round-robin. Balancing the load will help improve performance and reduce bottlenecks. Managing rate limits When using our API, it's important to understand and plan for rate limits. Improving latencies Latency is the time it takes for a request to be processed and a response to be returned. In this section, we will discuss some factors that influence the latency of our text generation models and provide suggestions on how to reduce it. The latency of a completion request is mostly influenced by two factors: the model and the number of tokens generated. The life cycle of a completion request looks like this: Network End user to API latency Server Time to process prompt tokens Server Time to sample/generate tokens Network API to end user latency The bulk of the latency typically arises from the token generation step. Intuition: Prompt tokens add very little latency to completion calls. Time to generate completion tokens is much longer, as tokens are generated one at a time. Longer generation lengths will accumulate latency due to generation required for each token. Common factors affecting latency and possible mitigation techniques Now that we have looked at the basics of latency, let\u2019s take a look at various factors that can affect latency, broadly ordered from most impactful to least impactful. Model Our API offers different models with varying levels of complexity and generality. The most capable models, such as text-davinci-003, can generate more complex and diverse completions, but they also take longer to process your query. Models such as text-curie-001, can generate faster completions, but they may generate results that are less accurate or relevant for your query. You can choose the model that best suits your use case and the trade-off between speed and quality. Number of completion tokens Requesting a large amount of generated tokens completions can lead to increased latencies: Lower max tokens: for requests with a similar token generation count, those that have a lower max_tokens parameter incur less latency. Include stop sequences: to prevent generating unneeded tokens, add a stop sequence. For example, you can use stop sequences to generate a list with a specific number of items. In this case, by using 11. as a stop sequence, you can generate a list with only 10 items, since the completion will stop when 11. is reached. Read our help article on stop sequences for more context on how you can do this. Generate fewer completions: lower the values of n and best_of when possible where n refers to how many completions to generate for each prompt and best_of is used to represent the result with the highest log probability per token. If n and best_of both equal 1 (which is the default), the number of generated tokens will be at most, equal to max_tokens. If n (the number of completions returned) or best_of (the number of completions generated for consideration) are set to > 1, each request will create multiple outputs. Here, you can consider the number of generated tokens as [ max_tokens * max (n, best_of) ] Streaming Setting stream: true in a request makes the model start returning tokens as soon as they are available, instead of waiting for the full sequence of tokens to be generated. It does not change the time to get all the tokens, but it reduces the time for first token for an application where we want to show partial progress or are going to stop generations. This can be a better user experience and a UX improvement so it\u2019s worth experimenting with streaming. Infrastructure Our servers are currently located in the US. While we hope to have global redundancy in the future, in the meantime you could consider locating the relevant parts of your infrastructure in the US to minimize the roundtrip time between your servers and the OpenAI servers. Batching Depending on your use case, batching may help. If you are sending multiple requests to the same endpoint, you can batch the prompts to be sent in the same request. This will reduce the number of requests you need to make. The prompt parameter can hold up to 20 unique prompts. We advise you to test out this method and see if it helps. In some cases, you may end up increasing the number of generated tokens which will slow the response time. Managing costs To monitor your costs, you can set a soft limit in your account to receive an email alert once you pass a certain usage threshold. You can also set a hard limit. Please be mindful of the potential for a hard limit to cause disruptions to your application/users. Use the usage tracking dashboard to monitor your token usage during the current and past billing cycles. Text generation One of the challenges of moving your prototype into production is budgeting for the costs associated with running your application. OpenAI offers a pay-as-you-go pricing model, with prices per 1,000 tokens (roughly equal to 750 words). To estimate your costs, you will need to project the token utilization. Consider factors such as traffic levels, the frequency with which users will interact with your application, and the amount of data you will be processing. One useful framework for thinking about reducing costs is to consider costs as a function of the number of tokens and the cost per token. There are two potential avenues for reducing costs using this framework. First, you could work to reduce the cost per token by switching to smaller models for some tasks in order to reduce costs. Alternatively, you could try to reduce the number of tokens required. There are a few ways you could do this, such as by using shorter prompts, fine-tuning models, or caching common user queries so that they don't need to be processed repeatedly. You can experiment with our interactive tokenizer tool to help you estimate costs. The API and playground also returns token counts as part of the response. Once you\u2019ve got things working with our most capable model, you can see if the other models can produce the same results with lower latency and costs. Learn more in our token usage help article. MLOps strategy As you move your prototype into production, you may want to consider developing an MLOps strategy. MLOps (machine learning operations) refers to the process of managing the end-to-end life cycle of your machine learning models, including any models you may be fine-tuning using our API. There are a number of areas to consider when designing your MLOps strategy. These include Data and model management: managing the data used to train or fine-tune your model and tracking versions and changes. Model monitoring: tracking your model's performance over time and detecting any potential issues or degradation. Model retraining: ensuring your model stays up to date with changes in data or evolving requirements and retraining or fine-tuning it as needed. Model deployment: automating the process of deploying your model and related artifacts into production. Thinking through these aspects of your application will help ensure your model stays relevant and performs well over time. Security and compliance As you move your prototype into production, you will need to assess and address any security and compliance requirements that may apply to your application. This will involve examining the data you are handling, understanding how our API processes data, and determining what regulations you must adhere to. For reference, here is our Privacy Policy and Terms of Use. Some common areas you'll need to consider include data storage, data transmission, and data retention. You might also need to implement data privacy protections, such as encryption or anonymization where possible. In addition, you should follow best practices for secure coding, such as input sanitization and proper error handling. Safety best practices When creating your application with our API, consider our safety best practices to ensure your application is safe and successful. These recommendations highlight the importance of testing the product extensively, being proactive about addressing potential issues, and limiting opportunities for misuse.","title":"\u751f\u4ea7\u6700\u4f73\u5b9e\u8df5"},{"location":"guides/production-best-practices/#_1","text":"This guide provides a comprehensive set of best practices to help you transition from prototype to production. Whether you are a seasoned machine learning engineer or a recent enthusiast, this guide should provide you with the tools you need to successfully put the platform to work in a production setting: from securing access to our API to designing a robust architecture that can handle high traffic volumes. Use this guide to help develop a plan for deploying your application as smoothly and effectively as possible.","title":"\u751f\u4ea7\u6700\u4f73\u5b9e\u8df5"},{"location":"guides/production-best-practices/#setting-up-your-organization","text":"Once you log in to your OpenAI account, you can find your organization name and ID in your organization settings. The organization name is the label for your organization, shown in user interfaces. The organization ID is the unique identifier for your organization which can be used in API requests. Users who belong to multiple organizations can pass a header to specify which organization is used for an API request. Usage from these API requests will count against the specified organization's quota. If no header is provided, the default organization will be billed. You can change your default organization in your user settings. You can invite new members to your organization from the members settings page. Members can be readers or owners. Readers can make API requests and view basic organization information, while owners can modify billing information and manage members within an organization.","title":"Setting up your organization"},{"location":"guides/production-best-practices/#managing-billing-limits","text":"New free trial users receive an initial credit of $5 that expires after three months. Once the credit has been used or expires, you can choose to enter billing information to continue your use of the API. If no billing information is entered, you will still have login access but will be unable to make any further API requests. Once you\u2019ve entered your billing information, you will have an approved usage limit of $120 per month, which is set by OpenAI. To increase your quota beyond the $120 monthly billing limit, please submit a quota increase request. If you\u2019d like to be notified when your usage exceeds a certain amount, you can set a soft limit through the usage limits page. When the soft limit is reached, the owners of the organization will receive an email notification. You can also set a hard limit so that, once the hard limit is reached, any subsequent API requests will be rejected. Note that these limits are best effort, and there may be 5 to 10 minutes of delay between the usage and the limits being enforced.","title":"Managing billing limits"},{"location":"guides/production-best-practices/#api-keys","text":"The OpenAI API uses API keys for authentication. Visit your API keys page to retrieve the API key you'll use in your requests. This is a relatively straightforward way to control access, but you must be vigilant about securing these keys. Avoid exposing the API keys in your code or in public repositories; instead, store them in a secure location. You should expose your keys to your application using environment variables or secret management service, so that you don't need to hard-code them in your codebase. Read more in our Best practices for API key safety.","title":"API keys"},{"location":"guides/production-best-practices/#staging-accounts","text":"As you scale, you may want to create separate organizations for your staging and production environments. Please note that you can sign up using two separate email addresses like bob+prod@widgetcorp.com and bob+dev@widgetcorp.com to create two organizations. This will allow you to isolate your development and testing work so you don't accidentally disrupt your live application. You can also limit access to your production organization this way.","title":"Staging accounts"},{"location":"guides/production-best-practices/#building-your-prototype","text":"If you haven\u2019t gone through the quickstart guide, we recommend you start there before diving into the rest of this guide. For those new to the OpenAI API, our playground can be a great resource for exploring its capabilities. Doing so will help you learn what's possible and where you may want to focus your efforts. You can also explore our example prompts. While the playground is a great place to prototype, it can also be used as an incubation area for larger projects. The playground also makes it easy to export code snippets for API requests and share prompts with collaborators, making it an integral part of your development process.","title":"Building your prototype"},{"location":"guides/production-best-practices/#additional-tips","text":"Start by determining the core functionalities you want your application to have. Consider the types of data inputs, outputs, and processes you will need. Aim to keep the prototype as focused as possible, so that you can iterate quickly and efficiently. Choose the programming language and framework that you feel most comfortable with and that best aligns with your goals for the project. Some popular options include Python, Java, and Node.js. See library support page to learn more about the library bindings maintained both by our team and by the broader developer community. Development environment and support: Set up your development environment with the right tools and libraries and ensure you have the resources you need to train your model. Leverage our documentation, community forum and our help center to get help with troubleshooting. If you are developing using Python, take a look at this structuring your project guide (repository structure is a crucial part of your project\u2019s architecture). In order to connect with our support engineers, simply log in to your account and use the \"Help\" button to start a conversation. Techniques for improving reliability around prompts Even with careful planning, it's important to be prepared for unexpected issues when using GPT-3 in your application. In some cases, the model may fail on a task, so it's helpful to consider what you can do to improve the reliability of your application. If your task involves logical reasoning or complexity, you may need to take additional steps to build more reliable prompts. For some helpful suggestions, consult our Techniques to improve reliability guide. Overall the recommendations revolve around: Decomposing unreliable operations into smaller, more reliable operations (e.g., selection-inference prompting) Using multiple steps or multiple relationships to make the system's reliability greater than any individual component (e.g., maieutic prompting) Evaluation and iteration One of the most important aspects of developing a system for production is regular evaluation and iterative experimentation. This process allows you to measure performance, troubleshoot issues, and fine-tune your models to improve accuracy and efficiency. A key part of this process is creating an evaluation dataset for your functionality. Here are a few things to keep in mind: Make sure your evaluation set is representative of the data your model will be used on in the real world. This will allow you to assess your model's performance on data it hasn't seen before and help you understand how well it generalizes to new situations. Regularly update your evaluation set to ensure that it stays relevant as your model evolves and as new data becomes available. Use a variety of metrics to evaluate your model's performance. Depending on your application and business outcomes, this could include accuracy, precision, recall, F1 score, or mean average precision (MAP). Additionally, you can sync your fine-tunes with Weights & Biases to track experiments, models, and datasets. Compare your model's performance against baseline. This will give you a better understanding of your model's strengths and weaknesses and can help guide your future development efforts. By conducting regular evaluation and iterative experimentation, you can ensure that your GPT-powered application or prototype continues to improve over time.","title":"Additional tips"},{"location":"guides/production-best-practices/#evaluating-language-models","text":"Language models can be difficult to evaluate because evaluating the quality of generated language is often subjective, and there are many different ways to communicate the same message correctly in language. For example, when evaluating a model on the ability to summarize a long passage of text, there are many correct summaries. That being said, designing good evaluations is critical to making progress in machine learning. An eval suite needs to be comprehensive, easy to run, and reasonably fast (depending on model size). It also needs to be easy to continue to add to the suite as what is comprehensive one month will likely be out of date in another month. We should prioritize having a diversity of tasks and tasks that identify weaknesses in the models or capabilities that are not improving with scaling. The simplest way to evaluate your system is to manually inspect its outputs. Is it doing what you want? Are the outputs high quality? Are they consistent?","title":"Evaluating language models"},{"location":"guides/production-best-practices/#automated-evaluations","text":"The best way to test faster is to develop automated evaluations. However, this may not be possible in more subjective applications like summarization tasks. Automated evaluations work best when it\u2019s easy to grade a final output as correct or incorrect. For example, if you\u2019re fine-tuning a classifier to classify text strings as class A or class B, it\u2019s fairly simple: create a test set with example input and output pairs, run your system on the inputs, and then grade the system outputs versus the correct outputs (looking at metrics like accuracy, F1 score, cross-entropy, etc.). If your outputs are semi open-ended, as they might be for a meeting notes summarizer, it can be trickier to define success: for example, what makes one summary better than another? Here, possible techniques include: Writing a test with \u2018gold standard\u2019 answers and then measuring some sort of similarity score between each gold standard answer and the system output (we\u2019ve seen embeddings work decently well for this) Building a discriminator system to judge / rank outputs, and then giving that discriminator a set of outputs where one is generated by the system under test (this can even be GPT model that is asked whether the question is answered correctly by a given output) Building an evaluation model that checks for the truth of components of the answer; e.g., detecting whether a quote actually appears in the piece of given text For very open-ended tasks, such as a creative story writer, automated evaluation is more difficult. Although it might be possible to develop quality metrics that look at spelling errors, word diversity, and readability scores, these metrics don\u2019t really capture the creative quality of a piece of writing. In cases where no good automated metric can be found, human evaluations remain the best method. Example procedure for evaluating a GPT-3-based system As an example, let\u2019s consider the case of building a retrieval-based Q&A system. A retrieval-based Q&A system has two steps. First, a user\u2019s query is used to rank potentially relevant documents in a knowledge base. Second, GPT-3 is given the top-ranking documents and asked to generate an answer to the query. Evaluations can be made to measure the performance of each step. For the search step, one could: First, generate a test set with ~100 questions and a set of correct documents for each The questions can be sourced from user data if you have any; otherwise, you can invent a set of questions with diverse styles and difficulty. For each question, have a person manually search through the knowledge base and record the set of documents that contain the answer. Second, use the test set to grade the system\u2019s performance For each question, use the system to rank the candidate documents (e.g., by cosine similarity of the document embeddings with the query embedding). You can score the results with a binary accuracy score of 1 if the candidate documents contain at least 1 relevant document from the answer key and 0 otherwise You can also use a continuous metric like Mean Reciprocal Rank which can help distinguish between answers that were close to being right or far from being right (e.g., a score of 1 if the correct document is rank 1, a score of \u00bd if rank 2, a score of \u2153 if rank 3, etc.) For the question answering step, one could: First, generate a test set with ~100 sets of {question, relevant text, correct answer} For the questions and relevant texts, use the above data For the correct answers, have a person write down ~100 examples of what a great answer looks like. Second, use the test set to grade the system\u2019s performance For each question & text pair, combine them into a prompt and submit the prompt to GPT-3 Next, compare GPT-3\u2019s answers to the gold-standard answer written by a human This comparison can be manual, where humans look at them side by side and grade whether the GPT-3 answer is correct/high quality This comparison can also be automated, by using embedding similarity scores or another method (automated methods will likely be noisy, but noise is ok as long as it\u2019s unbiased and equally noisy across different types of models that you\u2019re testing against one another) Of course, N=100 is just an example, and in early stages, you might start with a smaller set that\u2019s easier to generate, and in later stages, you might invest in a larger set that\u2019s more costly but more statistically reliable. Scaling your solution architecture When designing your application or service for production that uses our API, it's important to consider how you will scale to meet traffic demands. There are a few key areas you will need to consider regardless of the cloud service provider of your choice: Horizontal scaling: You may want to scale your application out horizontally to accommodate requests to your application that come from multiple sources. This could involve deploying additional servers or containers to distribute the load. If you opt for this type of scaling, make sure that your architecture is designed to handle multiple nodes and that you have mechanisms in place to balance the load between them. Vertical scaling: Another option is to scale your application up vertically, meaning you can beef up the resources available to a single node. This would involve upgrading your server's capabilities to handle the additional load. If you opt for this type of scaling, make sure your application is designed to take advantage of these additional resources. Caching: By storing frequently accessed data, you can improve response times without needing to make repeated calls to our API. Your application will need to be designed to use cached data whenever possible and invalidate the cache when new information is added. There are a few different ways you could do this. For example, you could store data in a database, filesystem, or in-memory cache, depending on what makes the most sense for your application. Load balancing: Finally, consider load-balancing techniques to ensure requests are distributed evenly across your available servers. This could involve using a load balancer in front of your servers or using DNS round-robin. Balancing the load will help improve performance and reduce bottlenecks. Managing rate limits When using our API, it's important to understand and plan for rate limits. Improving latencies Latency is the time it takes for a request to be processed and a response to be returned. In this section, we will discuss some factors that influence the latency of our text generation models and provide suggestions on how to reduce it. The latency of a completion request is mostly influenced by two factors: the model and the number of tokens generated. The life cycle of a completion request looks like this: Network End user to API latency Server Time to process prompt tokens Server Time to sample/generate tokens Network API to end user latency The bulk of the latency typically arises from the token generation step. Intuition: Prompt tokens add very little latency to completion calls. Time to generate completion tokens is much longer, as tokens are generated one at a time. Longer generation lengths will accumulate latency due to generation required for each token. Common factors affecting latency and possible mitigation techniques Now that we have looked at the basics of latency, let\u2019s take a look at various factors that can affect latency, broadly ordered from most impactful to least impactful. Model Our API offers different models with varying levels of complexity and generality. The most capable models, such as text-davinci-003, can generate more complex and diverse completions, but they also take longer to process your query. Models such as text-curie-001, can generate faster completions, but they may generate results that are less accurate or relevant for your query. You can choose the model that best suits your use case and the trade-off between speed and quality. Number of completion tokens Requesting a large amount of generated tokens completions can lead to increased latencies: Lower max tokens: for requests with a similar token generation count, those that have a lower max_tokens parameter incur less latency. Include stop sequences: to prevent generating unneeded tokens, add a stop sequence. For example, you can use stop sequences to generate a list with a specific number of items. In this case, by using 11. as a stop sequence, you can generate a list with only 10 items, since the completion will stop when 11. is reached. Read our help article on stop sequences for more context on how you can do this. Generate fewer completions: lower the values of n and best_of when possible where n refers to how many completions to generate for each prompt and best_of is used to represent the result with the highest log probability per token. If n and best_of both equal 1 (which is the default), the number of generated tokens will be at most, equal to max_tokens. If n (the number of completions returned) or best_of (the number of completions generated for consideration) are set to > 1, each request will create multiple outputs. Here, you can consider the number of generated tokens as [ max_tokens * max (n, best_of) ] Streaming Setting stream: true in a request makes the model start returning tokens as soon as they are available, instead of waiting for the full sequence of tokens to be generated. It does not change the time to get all the tokens, but it reduces the time for first token for an application where we want to show partial progress or are going to stop generations. This can be a better user experience and a UX improvement so it\u2019s worth experimenting with streaming. Infrastructure Our servers are currently located in the US. While we hope to have global redundancy in the future, in the meantime you could consider locating the relevant parts of your infrastructure in the US to minimize the roundtrip time between your servers and the OpenAI servers. Batching Depending on your use case, batching may help. If you are sending multiple requests to the same endpoint, you can batch the prompts to be sent in the same request. This will reduce the number of requests you need to make. The prompt parameter can hold up to 20 unique prompts. We advise you to test out this method and see if it helps. In some cases, you may end up increasing the number of generated tokens which will slow the response time. Managing costs To monitor your costs, you can set a soft limit in your account to receive an email alert once you pass a certain usage threshold. You can also set a hard limit. Please be mindful of the potential for a hard limit to cause disruptions to your application/users. Use the usage tracking dashboard to monitor your token usage during the current and past billing cycles. Text generation One of the challenges of moving your prototype into production is budgeting for the costs associated with running your application. OpenAI offers a pay-as-you-go pricing model, with prices per 1,000 tokens (roughly equal to 750 words). To estimate your costs, you will need to project the token utilization. Consider factors such as traffic levels, the frequency with which users will interact with your application, and the amount of data you will be processing. One useful framework for thinking about reducing costs is to consider costs as a function of the number of tokens and the cost per token. There are two potential avenues for reducing costs using this framework. First, you could work to reduce the cost per token by switching to smaller models for some tasks in order to reduce costs. Alternatively, you could try to reduce the number of tokens required. There are a few ways you could do this, such as by using shorter prompts, fine-tuning models, or caching common user queries so that they don't need to be processed repeatedly. You can experiment with our interactive tokenizer tool to help you estimate costs. The API and playground also returns token counts as part of the response. Once you\u2019ve got things working with our most capable model, you can see if the other models can produce the same results with lower latency and costs. Learn more in our token usage help article. MLOps strategy As you move your prototype into production, you may want to consider developing an MLOps strategy. MLOps (machine learning operations) refers to the process of managing the end-to-end life cycle of your machine learning models, including any models you may be fine-tuning using our API. There are a number of areas to consider when designing your MLOps strategy. These include Data and model management: managing the data used to train or fine-tune your model and tracking versions and changes. Model monitoring: tracking your model's performance over time and detecting any potential issues or degradation. Model retraining: ensuring your model stays up to date with changes in data or evolving requirements and retraining or fine-tuning it as needed. Model deployment: automating the process of deploying your model and related artifacts into production. Thinking through these aspects of your application will help ensure your model stays relevant and performs well over time. Security and compliance As you move your prototype into production, you will need to assess and address any security and compliance requirements that may apply to your application. This will involve examining the data you are handling, understanding how our API processes data, and determining what regulations you must adhere to. For reference, here is our Privacy Policy and Terms of Use. Some common areas you'll need to consider include data storage, data transmission, and data retention. You might also need to implement data privacy protections, such as encryption or anonymization where possible. In addition, you should follow best practices for secure coding, such as input sanitization and proper error handling. Safety best practices When creating your application with our API, consider our safety best practices to ensure your application is safe and successful. These recommendations highlight the importance of testing the product extensively, being proactive about addressing potential issues, and limiting opportunities for misuse.","title":"Automated evaluations"},{"location":"guides/rate-limits/","text":"\u901f\u7387\u9650\u5236 \u00b6 Overview \u00b6 What are rate limits? A rate limit is a restriction that an API imposes on the number of times a user or client can access the server within a specified period of time. Why do we have rate limits? \u00b6 Rate limits are a common practice for APIs, and they're put in place for a few different reasons: They help protect against abuse or misuse of the API. For example, a malicious actor could flood the API with requests in an attempt to overload it or cause disruptions in service. By setting rate limits, OpenAI can prevent this kind of activity. Rate limits help ensure that everyone has fair access to the API. If one person or organization makes an excessive number of requests, it could bog down the API for everyone else. By throttling the number of requests that a single user can make, OpenAI ensures that the most number of people have an opportunity to use the API without experiencing slowdowns. Rate limits can help OpenAI manage the aggregate load on its infrastructure. If requests to the API increase dramatically, it could tax the servers and cause performance issues. By setting rate limits, OpenAI can help maintain a smooth and consistent experience for all users. Please work through this document in its entirety to better understand how OpenAI\u2019s rate limit system works. We include code examples and possible solutions to handle common issues. It is recommended to follow this guidance before filling out the Rate Limit Increase Request form with details regarding how to fill it out in the last section. What are the rate limits for our API? We enforce rate limits at the organization level, not user level, based on the specific endpoint used as well as the type of account you have. Rate limits are measured in two ways: RPM (requests per minute) and TPM (tokens per minute). The table below highlights the default rate limits for our API but these limits can be increased depending on your use case after filling out the Rate Limit increase request form. The TPM (tokens per minute) unit is different depending on the model: TYPE 1 TPM EQUALS davinci 1 token per minute curie 25 tokens per minute babbage 100 tokens per minute ada 200 tokens per minute In practical terms, this means you can send approximately 200x more tokens per minute to an ada model versus a davinci model. TEXT & EMBEDDING CHAT CODEX EDIT IMAGE AUDIO Free trial users \u202220 RPM \u2022150,000 TPM \u202220 RPM \u202240,000 TPM \u202220 RPM \u202240,000 TPM \u202220 RPM \u2022150,000 TPM 50 images / min 50 RPM Pay-as-you-go users (first 48 hours) \u202260 RPM \u2022250,000 TPM* \u202260 RPM \u202260,000 TPM* \u202220 RPM \u202240,000 TPM \u202220 RPM \u2022150,000 TPM 50 images / min 50 RPM Pay-as-you-go users (after 48 hours) \u20223,500 RPM \u2022350,000 TPM* \u20223,500 RPM \u202290,000 TPM* \u202220 RPM \u202240,000 TPM \u202220 RPM \u2022150,000 TPM 50 images / min 50 RPM It is important to note that the rate limit can be hit by either option depending on what occurs first. For example, you might send 20 requests with only 100 tokens to the Codex endpoint and that would fill your limit, even if you did not send 40k tokens within those 20 requests. How do rate limits work? If your rate limit is 60 requests per minute and 150k davinci tokens per minute, you\u2019ll be limited either by reaching the requests/min cap or running out of tokens\u2014whichever happens first. For example, if your max requests/min is 60, you should be able to send 1 request per second. If you send 1 request every 800ms, once you hit your rate limit, you\u2019d only need to make your program sleep 200ms in order to send one more request otherwise subsequent requests would fail. With the default of 3,000 requests/min, customers can effectively send 1 request every 20ms, or every .02 seconds. What happens if I hit a rate limit error? Rate limit errors look like this: Rate limit reached for default-text-davinci-002 in organization org-{id} on requests per min. Limit: 20.000000 / min. Current: 24.000000 / min. If you hit a rate limit, it means you've made too many requests in a short period of time, and the API is refusing to fulfill further requests until a specified amount of time has passed. Rate limits vs max_tokens Each model we offer has a limited number of tokens that can be passed in as input when making a request. You cannot increase the maximum number of tokens a model takes in. For example, if you are using text-ada-001, the maximum number of tokens you can send to this model is 2,048 tokens per request. Error Mitigation What are some steps I can take to mitigate this? The OpenAI Cookbook has a python notebook that explains details on how to avoid rate limit errors. You should also exercise caution when providing programmatic access, bulk processing features, and automated social media posting - consider only enabling these for trusted customers. To protect against automated and high-volume misuse, set a usage limit for individual users within a specified time frame (daily, weekly, or monthly). Consider implementing a hard cap or a manual review process for users who exceed the limit. Retrying with exponential backoff One easy way to avoid rate limit errors is to automatically retry requests with a random exponential backoff. Retrying with exponential backoff means performing a short sleep when a rate limit error is hit, then retrying the unsuccessful request. If the request is still unsuccessful, the sleep length is increased and the process is repeated. This continues until the request is successful or until a maximum number of retries is reached. This approach has many benefits: Automatic retries means you can recover from rate limit errors without crashes or missing data Exponential backoff means that your first retries can be tried quickly, while still benefiting from longer delays if your first few retries fail Adding random jitter to the delay helps retries from all hitting at the same time. Note that unsuccessful requests contribute to your per-minute limit, so continuously resending a request won\u2019t work. Below are a few example solutions for Python that use exponential backoff. Example #1 : Using the Tenacity library Example #2 : Using the backoff library Example 3: Manual backoff implementation Batching requests The OpenAI API has separate limits for requests per minute and tokens per minute. If you're hitting the limit on requests per minute, but have available capacity on tokens per minute, you can increase your throughput by batching multiple tasks into each request. This will allow you to process more tokens per minute, especially with our smaller models. Sending in a batch of prompts works exactly the same as a normal API call, except you pass in a list of strings to the prompt parameter instead of a single string. Example without batching Example with batching Warning: the response object may not return completions in the order of the prompts, so always remember to match responses back to prompts using the index field. Request Increase When should I consider applying for a rate limit increase? Our default rate limits help us maximize stability and prevent abuse of our API. We increase limits to enable high-traffic applications, so the best time to apply for a rate limit increase is when you feel that you have the necessary traffic data to support a strong case for increasing the rate limit. Large rate limit increase requests without supporting data are not likely to be approved. If you're gearing up for a product launch, please obtain the relevant data through a phased release over 10 days. Keep in mind that rate limit increases can sometimes take 7-10 days so it makes sense to try and plan ahead and submit early if there is data to support you will reach your rate limit given your current growth numbers. Will my rate limit increase request be rejected? A rate limit increase request is most often rejected because it lacks the data needed to justify the increase. We have provided numerical examples below that show how to best support a rate limit increase request and try our best to approve all requests that align with our safety policy and show supporting data. We are committed to enabling developers to scale and be successful with our API. I\u2019ve implemented exponential backoff for my text/code APIs, but I\u2019m still hitting this error. How do I increase my rate limit? Currently, we don\u2019t support increasing our free beta endpoints, such as the edit endpoint. We also don\u2019t increase ChatGPT rate limits but you can join the waitlist for ChatGPT Professional access. We understand the frustration that limited rate limits can cause, and we would love to raise the defaults for everyone. However, due to shared capacity constraints, we can only approve rate limit increases for paid customers who have demonstrated a need through our Rate Limit Increase Request form. To help us evaluate your needs properly, we ask that you please provide statistics on your current usage or projections based on historic user activity in the 'Share evidence of need' section of the form. If this information is not available, we recommend a phased release approach. Start by releasing the service to a subset of users at your current rate limits, gather usage data for 10 business days, and then submit a formal rate limit increase request based on that data for our review and approval. We will review your request and if it is approved, we will notify you of the approval within a period of 7-10 business days. Here are some examples of how you might fill out this form: DALL-E API examples MODEL ESTIMATE TOKENS/MINUTE ESTIMATE REQUESTS/MINUTE # OF USERS EVIDENCE OF NEED 1 HOUR MAX THROUGHPUT COST DALL-E API N/A 50 1000 Our app is currently in production and based on our past traffic, we make about 10 requests per minute. $60 DALL-E API N/A 150 10,000 Our app is gaining traction in the App Store and we\u2019re starting to hit rate limits. Can we get triple the default limit of 50 img/min? If we need more we\u2019ll submit a new form. Thanks! $180 Language model examples MODEL ESTIMATE TOKENS/MINUTE ESTIMATE REQUESTS/MINUTE # OF USERS EVIDENCE OF NEED 1 HOUR MAX THROUGHPUT COST text-davinci-003 325,000 4,0000 50 We\u2019re releasing to an initial group of alpha testers and need a higher limit to accommodate their initial usage. We have a link here to our google drive which shows analytics and api usage. $390 text-davinci-002 750,000 10,000 10,000 Our application is receiving a lot of interest; we have 50,000 people on our waitlist. We\u2019d like to roll out to groups of 1,000 people/day until we reach 50,000 users. Please see this link of our current token/minute traffic over the past 30 days. This is for 500 users, and based on their usage, we think 750,000 tokens/minute and 10,000 requests/minute will work as a good starting point. $900 Code model examples MODEL ESTIMATE TOKENS/MINUTE ESTIMATE REQUESTS/MINUTE # OF USERS EVIDENCE OF NEED 1 HOUR MAX THROUGHPUT COST code-davinci-002 150,000 1,000 15 We are a group of researchers working on a paper. We estimate that we will need a higher rate limit on code-davinci-002 in order to complete our research before the end of the month. These estimates are based on the following calculation [...] Codex models are currently in free beta so we may not be able to provide immediate increases for these models. Please note that these examples are just general use case scenarios, the actual usage rate will vary depending on the specific implementation and usage.","title":"\u901f\u7387\u9650\u5236"},{"location":"guides/rate-limits/#_1","text":"","title":"\u901f\u7387\u9650\u5236"},{"location":"guides/rate-limits/#overview","text":"What are rate limits? A rate limit is a restriction that an API imposes on the number of times a user or client can access the server within a specified period of time.","title":"Overview"},{"location":"guides/rate-limits/#why-do-we-have-rate-limits","text":"Rate limits are a common practice for APIs, and they're put in place for a few different reasons: They help protect against abuse or misuse of the API. For example, a malicious actor could flood the API with requests in an attempt to overload it or cause disruptions in service. By setting rate limits, OpenAI can prevent this kind of activity. Rate limits help ensure that everyone has fair access to the API. If one person or organization makes an excessive number of requests, it could bog down the API for everyone else. By throttling the number of requests that a single user can make, OpenAI ensures that the most number of people have an opportunity to use the API without experiencing slowdowns. Rate limits can help OpenAI manage the aggregate load on its infrastructure. If requests to the API increase dramatically, it could tax the servers and cause performance issues. By setting rate limits, OpenAI can help maintain a smooth and consistent experience for all users. Please work through this document in its entirety to better understand how OpenAI\u2019s rate limit system works. We include code examples and possible solutions to handle common issues. It is recommended to follow this guidance before filling out the Rate Limit Increase Request form with details regarding how to fill it out in the last section. What are the rate limits for our API? We enforce rate limits at the organization level, not user level, based on the specific endpoint used as well as the type of account you have. Rate limits are measured in two ways: RPM (requests per minute) and TPM (tokens per minute). The table below highlights the default rate limits for our API but these limits can be increased depending on your use case after filling out the Rate Limit increase request form. The TPM (tokens per minute) unit is different depending on the model: TYPE 1 TPM EQUALS davinci 1 token per minute curie 25 tokens per minute babbage 100 tokens per minute ada 200 tokens per minute In practical terms, this means you can send approximately 200x more tokens per minute to an ada model versus a davinci model. TEXT & EMBEDDING CHAT CODEX EDIT IMAGE AUDIO Free trial users \u202220 RPM \u2022150,000 TPM \u202220 RPM \u202240,000 TPM \u202220 RPM \u202240,000 TPM \u202220 RPM \u2022150,000 TPM 50 images / min 50 RPM Pay-as-you-go users (first 48 hours) \u202260 RPM \u2022250,000 TPM* \u202260 RPM \u202260,000 TPM* \u202220 RPM \u202240,000 TPM \u202220 RPM \u2022150,000 TPM 50 images / min 50 RPM Pay-as-you-go users (after 48 hours) \u20223,500 RPM \u2022350,000 TPM* \u20223,500 RPM \u202290,000 TPM* \u202220 RPM \u202240,000 TPM \u202220 RPM \u2022150,000 TPM 50 images / min 50 RPM It is important to note that the rate limit can be hit by either option depending on what occurs first. For example, you might send 20 requests with only 100 tokens to the Codex endpoint and that would fill your limit, even if you did not send 40k tokens within those 20 requests. How do rate limits work? If your rate limit is 60 requests per minute and 150k davinci tokens per minute, you\u2019ll be limited either by reaching the requests/min cap or running out of tokens\u2014whichever happens first. For example, if your max requests/min is 60, you should be able to send 1 request per second. If you send 1 request every 800ms, once you hit your rate limit, you\u2019d only need to make your program sleep 200ms in order to send one more request otherwise subsequent requests would fail. With the default of 3,000 requests/min, customers can effectively send 1 request every 20ms, or every .02 seconds. What happens if I hit a rate limit error? Rate limit errors look like this: Rate limit reached for default-text-davinci-002 in organization org-{id} on requests per min. Limit: 20.000000 / min. Current: 24.000000 / min. If you hit a rate limit, it means you've made too many requests in a short period of time, and the API is refusing to fulfill further requests until a specified amount of time has passed. Rate limits vs max_tokens Each model we offer has a limited number of tokens that can be passed in as input when making a request. You cannot increase the maximum number of tokens a model takes in. For example, if you are using text-ada-001, the maximum number of tokens you can send to this model is 2,048 tokens per request. Error Mitigation What are some steps I can take to mitigate this? The OpenAI Cookbook has a python notebook that explains details on how to avoid rate limit errors. You should also exercise caution when providing programmatic access, bulk processing features, and automated social media posting - consider only enabling these for trusted customers. To protect against automated and high-volume misuse, set a usage limit for individual users within a specified time frame (daily, weekly, or monthly). Consider implementing a hard cap or a manual review process for users who exceed the limit. Retrying with exponential backoff One easy way to avoid rate limit errors is to automatically retry requests with a random exponential backoff. Retrying with exponential backoff means performing a short sleep when a rate limit error is hit, then retrying the unsuccessful request. If the request is still unsuccessful, the sleep length is increased and the process is repeated. This continues until the request is successful or until a maximum number of retries is reached. This approach has many benefits: Automatic retries means you can recover from rate limit errors without crashes or missing data Exponential backoff means that your first retries can be tried quickly, while still benefiting from longer delays if your first few retries fail Adding random jitter to the delay helps retries from all hitting at the same time. Note that unsuccessful requests contribute to your per-minute limit, so continuously resending a request won\u2019t work. Below are a few example solutions for Python that use exponential backoff. Example #1 : Using the Tenacity library Example #2 : Using the backoff library Example 3: Manual backoff implementation Batching requests The OpenAI API has separate limits for requests per minute and tokens per minute. If you're hitting the limit on requests per minute, but have available capacity on tokens per minute, you can increase your throughput by batching multiple tasks into each request. This will allow you to process more tokens per minute, especially with our smaller models. Sending in a batch of prompts works exactly the same as a normal API call, except you pass in a list of strings to the prompt parameter instead of a single string. Example without batching Example with batching Warning: the response object may not return completions in the order of the prompts, so always remember to match responses back to prompts using the index field. Request Increase When should I consider applying for a rate limit increase? Our default rate limits help us maximize stability and prevent abuse of our API. We increase limits to enable high-traffic applications, so the best time to apply for a rate limit increase is when you feel that you have the necessary traffic data to support a strong case for increasing the rate limit. Large rate limit increase requests without supporting data are not likely to be approved. If you're gearing up for a product launch, please obtain the relevant data through a phased release over 10 days. Keep in mind that rate limit increases can sometimes take 7-10 days so it makes sense to try and plan ahead and submit early if there is data to support you will reach your rate limit given your current growth numbers. Will my rate limit increase request be rejected? A rate limit increase request is most often rejected because it lacks the data needed to justify the increase. We have provided numerical examples below that show how to best support a rate limit increase request and try our best to approve all requests that align with our safety policy and show supporting data. We are committed to enabling developers to scale and be successful with our API. I\u2019ve implemented exponential backoff for my text/code APIs, but I\u2019m still hitting this error. How do I increase my rate limit? Currently, we don\u2019t support increasing our free beta endpoints, such as the edit endpoint. We also don\u2019t increase ChatGPT rate limits but you can join the waitlist for ChatGPT Professional access. We understand the frustration that limited rate limits can cause, and we would love to raise the defaults for everyone. However, due to shared capacity constraints, we can only approve rate limit increases for paid customers who have demonstrated a need through our Rate Limit Increase Request form. To help us evaluate your needs properly, we ask that you please provide statistics on your current usage or projections based on historic user activity in the 'Share evidence of need' section of the form. If this information is not available, we recommend a phased release approach. Start by releasing the service to a subset of users at your current rate limits, gather usage data for 10 business days, and then submit a formal rate limit increase request based on that data for our review and approval. We will review your request and if it is approved, we will notify you of the approval within a period of 7-10 business days. Here are some examples of how you might fill out this form: DALL-E API examples MODEL ESTIMATE TOKENS/MINUTE ESTIMATE REQUESTS/MINUTE # OF USERS EVIDENCE OF NEED 1 HOUR MAX THROUGHPUT COST DALL-E API N/A 50 1000 Our app is currently in production and based on our past traffic, we make about 10 requests per minute. $60 DALL-E API N/A 150 10,000 Our app is gaining traction in the App Store and we\u2019re starting to hit rate limits. Can we get triple the default limit of 50 img/min? If we need more we\u2019ll submit a new form. Thanks! $180 Language model examples MODEL ESTIMATE TOKENS/MINUTE ESTIMATE REQUESTS/MINUTE # OF USERS EVIDENCE OF NEED 1 HOUR MAX THROUGHPUT COST text-davinci-003 325,000 4,0000 50 We\u2019re releasing to an initial group of alpha testers and need a higher limit to accommodate their initial usage. We have a link here to our google drive which shows analytics and api usage. $390 text-davinci-002 750,000 10,000 10,000 Our application is receiving a lot of interest; we have 50,000 people on our waitlist. We\u2019d like to roll out to groups of 1,000 people/day until we reach 50,000 users. Please see this link of our current token/minute traffic over the past 30 days. This is for 500 users, and based on their usage, we think 750,000 tokens/minute and 10,000 requests/minute will work as a good starting point. $900 Code model examples MODEL ESTIMATE TOKENS/MINUTE ESTIMATE REQUESTS/MINUTE # OF USERS EVIDENCE OF NEED 1 HOUR MAX THROUGHPUT COST code-davinci-002 150,000 1,000 15 We are a group of researchers working on a paper. We estimate that we will need a higher rate limit on code-davinci-002 in order to complete our research before the end of the month. These estimates are based on the following calculation [...] Codex models are currently in free beta so we may not be able to provide immediate increases for these models. Please note that these examples are just general use case scenarios, the actual usage rate will vary depending on the specific implementation and usage.","title":"Why do we have rate limits?"},{"location":"guides/safety-best-practices/","text":"\u5b89\u5168\u6700\u4f73\u5b9e\u8df5 \u00b6 Use our free Moderation API OpenAI's Moderation API is free-to-use and can help reduce the frequency of unsafe content in your completions. Alternatively, you may wish to develop your own content filtration system tailored to your use case. Adversarial testing We recommend \u201cred-teaming\u201d your application to ensure it's robust to adversarial input. Test your product over a wide range of inputs and user behaviors, both a representative set and those reflective of someone trying to \u2018break' your application. Does it wander off topic? Can someone easily redirect the feature via prompt injections, e.g. \u201cignore the previous instructions and do this instead\u201d? Human in the loop (HITL) Wherever possible, we recommend having a human review outputs before they are used in practice. This is especially critical in high-stakes domains, and for code generation. Humans should be aware of the limitations of the system, and have access to any information needed to verify the outputs (for example, if the application summarizes notes, a human should have easy access to the original notes to refer back). Prompt engineering \u201cPrompt engineering\u201d can help constrain the topic and tone of output text. This reduces the chance of producing undesired content, even if a user tries to produce it. Providing additional context to the model (such as by giving a few high-quality examples of desired behavior prior to the new input) can make it easier to steer model outputs in desired directions. \u201cKnow your customer\u201d (KYC) Users should generally need to register and log-in to access your service. Linking this service to an existing account, such as a Gmail, LinkedIn, or Facebook log-in, may help, though may not be appropriate for all use-cases. Requiring a credit card or ID card reduces risk further. Constrain user input and limit output tokens Limiting the amount of text a user can input into the prompt helps avoid prompt injection. Limiting the number of output tokens helps reduce the chance of misuse. Narrowing the ranges of inputs or outputs, especially drawn from trusted sources, reduces the extent of misuse possible within an application. Allowing user inputs through validated dropdown fields (e.g., a list of movies on Wikipedia) can be more secure than allowing open-ended text inputs. Returning outputs from a validated set of materials on the backend, where possible, can be safer than returning novel generated content (for instance, routing a customer query to the best-matching existing customer support article, rather than attempting to answer the query from-scratch). Allow users to report issues Users should generally have an easily-available method for reporting improper functionality or other concerns about application behavior (listed email address, ticket submission method, etc). This method should be monitored by a human and responded to as appropriate. Understand and communicate limitations From hallucinating inaccurate information, to offensive outputs, to bias, and much more, language models may not be suitable for every use case without significant modifications. Consider whether the model is fit for your purpose, and evaluate the performance of the API on a wide range of potential inputs in order to identify cases where the API's performance might drop. Consider your customer base and the range of inputs that they will be using, and ensure their expectations are calibrated appropriately. Safety and security are very important to us at OpenAI. If in the course of your development you do notice any safety or security issues with the API or anything else related to OpenAI, please submit these through our Coordinated Vulnerability Disclosure Program. End-user IDs Sending end-user IDs in your requests can be a useful tool to help OpenAI monitor and detect abuse. This allows OpenAI to provide your team with more actionable feedback in the event that we detect any policy violations in your application. The IDs should be a string that uniquely identifies each user. We recommend hashing their username or email address, in order to avoid sending us any identifying information. If you offer a preview of your product to non-logged in users, you can send a session ID instead. You can include end-user IDs in your API requests via the user parameter as follows: Example: Providing a user identifer python python 1 2 3 4 5 6 response = openai.Completion.create( model=\"text-davinci-003\", prompt=\"This is a test\", max_tokens=5, user=\"user123456\" )","title":"\u5b89\u5168\u6700\u4f73\u5b9e\u8df5"},{"location":"guides/safety-best-practices/#_1","text":"Use our free Moderation API OpenAI's Moderation API is free-to-use and can help reduce the frequency of unsafe content in your completions. Alternatively, you may wish to develop your own content filtration system tailored to your use case. Adversarial testing We recommend \u201cred-teaming\u201d your application to ensure it's robust to adversarial input. Test your product over a wide range of inputs and user behaviors, both a representative set and those reflective of someone trying to \u2018break' your application. Does it wander off topic? Can someone easily redirect the feature via prompt injections, e.g. \u201cignore the previous instructions and do this instead\u201d? Human in the loop (HITL) Wherever possible, we recommend having a human review outputs before they are used in practice. This is especially critical in high-stakes domains, and for code generation. Humans should be aware of the limitations of the system, and have access to any information needed to verify the outputs (for example, if the application summarizes notes, a human should have easy access to the original notes to refer back). Prompt engineering \u201cPrompt engineering\u201d can help constrain the topic and tone of output text. This reduces the chance of producing undesired content, even if a user tries to produce it. Providing additional context to the model (such as by giving a few high-quality examples of desired behavior prior to the new input) can make it easier to steer model outputs in desired directions. \u201cKnow your customer\u201d (KYC) Users should generally need to register and log-in to access your service. Linking this service to an existing account, such as a Gmail, LinkedIn, or Facebook log-in, may help, though may not be appropriate for all use-cases. Requiring a credit card or ID card reduces risk further. Constrain user input and limit output tokens Limiting the amount of text a user can input into the prompt helps avoid prompt injection. Limiting the number of output tokens helps reduce the chance of misuse. Narrowing the ranges of inputs or outputs, especially drawn from trusted sources, reduces the extent of misuse possible within an application. Allowing user inputs through validated dropdown fields (e.g., a list of movies on Wikipedia) can be more secure than allowing open-ended text inputs. Returning outputs from a validated set of materials on the backend, where possible, can be safer than returning novel generated content (for instance, routing a customer query to the best-matching existing customer support article, rather than attempting to answer the query from-scratch). Allow users to report issues Users should generally have an easily-available method for reporting improper functionality or other concerns about application behavior (listed email address, ticket submission method, etc). This method should be monitored by a human and responded to as appropriate. Understand and communicate limitations From hallucinating inaccurate information, to offensive outputs, to bias, and much more, language models may not be suitable for every use case without significant modifications. Consider whether the model is fit for your purpose, and evaluate the performance of the API on a wide range of potential inputs in order to identify cases where the API's performance might drop. Consider your customer base and the range of inputs that they will be using, and ensure their expectations are calibrated appropriately. Safety and security are very important to us at OpenAI. If in the course of your development you do notice any safety or security issues with the API or anything else related to OpenAI, please submit these through our Coordinated Vulnerability Disclosure Program. End-user IDs Sending end-user IDs in your requests can be a useful tool to help OpenAI monitor and detect abuse. This allows OpenAI to provide your team with more actionable feedback in the event that we detect any policy violations in your application. The IDs should be a string that uniquely identifies each user. We recommend hashing their username or email address, in order to avoid sending us any identifying information. If you offer a preview of your product to non-logged in users, you can send a session ID instead. You can include end-user IDs in your API requests via the user parameter as follows: Example: Providing a user identifer python python 1 2 3 4 5 6 response = openai.Completion.create( model=\"text-davinci-003\", prompt=\"This is a test\", max_tokens=5, user=\"user123456\" )","title":"\u5b89\u5168\u6700\u4f73\u5b9e\u8df5"},{"location":"guides/speech-to-text/","text":"\u8bed\u97f3\u8f6c\u6587\u672c Beta \u00b6 Learn how to turn audio into text Introduction The speech to text API provides two endpoints, transcriptions and translations, based on our state-of-the-art open source large-v2 Whisper model. They can be used to: Transcribe audio into whatever language the audio is in. Translate and transcribe the audio into english. File uploads are currently limited to 25 MB and the following input file types are supported: mp3, mp4, mpeg, mpga, m4a, wav, and webm. Quickstart \u00b6 Transcriptions The transcriptions API takes as input the audio file you want to transcribe and the desired output file format for the transcription of the audio. We currently support multiple input and output file formats. Transcribe audio python python 1 2 3 4 5 # Note: you need to be using OpenAI Python v0.27.0 for the code below to work import openai audio_file = open ( \"/path/to/file/audio.mp3\" , \"rb\" ) transcript = openai . Audio . transcribe ( \"whisper-1\" , audio_file ) By default, the response type will be json with the raw text included. 1 2 3 4 { \"text\" : \"Imagine the wildest idea that you've ever had, and you're curious about how it might scale to something that's a 100, a 1,000 times bigger.\" .... } To set additional parameters in a request, you can add more --form lines with the relevant options. For example, if you want to set the output format as text, you would add the following line: 1 2 3 4 ... --form file = @openai.mp3 \\ --form model = whisper-1 \\ --form response_format = text Translations The translations API takes as input the audio file in any of the supported languages and transcribes, if necessary, the audio into english. This differs from our /Transcriptions endpoint since the output is not in the original input language and is instead translated to english text. Translate audio python python 1 2 3 4 5 # Note: you need to be using OpenAI Python v0.27.0 for the code below to work import openai audio_file = open ( \"/path/to/file/german.mp3\" , \"rb\" ) transcript = openai . Audio . translate ( \"whisper-1\" , audio_file ) In this case, the inputted audio was german and the outputted text looks like: Hello, my name is Wolfgang and I come from Germany. Where are you heading today? We only support translation into english at this time. Supported languages We currently support the following languages through both the transcriptions and translations endpoint: Afrikaans, Arabic, Armenian, Azerbaijani, Belarusian, Bosnian, Bulgarian, Catalan, Chinese, Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French, Galician, German, Greek, Hebrew, Hindi, Hungarian, Icelandic, Indonesian, Italian, Japanese, Kannada, Kazakh, Korean, Latvian, Lithuanian, Macedonian, Malay, Marathi, Maori, Nepali, Norwegian, Persian, Polish, Portuguese, Romanian, Russian, Serbian, Slovak, Slovenian, Spanish, Swahili, Swedish, Tagalog, Tamil, Thai, Turkish, Ukrainian, Urdu, Vietnamese, and Welsh. While the underlying model was trained on 98 languages, we only list the languages that exceeded <50% word error rate (WER) which is an industry standard benchmark for speech to text model accuracy. The model will return results for languages not listed above but the quality will be low. Longer inputs By default, the Whisper API only supports files that are less than 25 MB. If you have an audio file that is longer than that, you will need to break it up into chunks of 25 MB's or less or used a compressed audio format. To get the best performance, we suggest that you avoid breaking the audio up mid-sentence as this may cause some context to be lost. One way to handle this is to use the PyDub open source Python package to split the audio: 1 2 3 4 5 6 7 8 9 10 11 from pydub import AudioSegment song = AudioSegment . from_mp3 ( \"good_morning.mp3\" ) # PyDub handles time in milliseconds ten * minutes = 10 * 60 \\ _ 1000 first_10_minutes = song [: ten_minutes ] first_10_minutes . export ( \"good_morning_10.mp3\" , format = \"mp3\" ) OpenAI makes no guarantees about the usability or security of 3 rd party software like PyDub. Prompting \u00b6 In the same way that you can use a prompt to influence the output of our language models, you can also use one to improve the quality of the transcripts generated by the Whisper API. The model will try to match the style of the prompt, so it will be more likely to use capitalization and punctuation if the prompt does too. Here are some examples of how prompting can help in different scenarios: Prompts can be very helpful for correcting specific words or acronyms that the model often misrecognizes in the audio. For example, the following prompt improves the transcription of the words DALL\u00b7E and GPT-3, which were previously written as \"GDP 3\" and \"DALI\". The transcript is about OpenAI which makes technology like DALL\u00b7E, GPT-3, and ChatGPT with the hope of one day building an AGI system that benefits all of humanity To preserve the context of a file that was split into segments, you can prompt the model with the transcript of the preceding segment. This will make the transcript more accurate, as the model will use the relevant information from the previous audio. The model will only consider the final 224 tokens of the prompt and ignore anything earlier. Sometimes the model might skip punctuation in the transcript. You can avoid this by using a simple prompt that includes punctuation: Hello, welcome to my lecture. The model may also leave out common filler words in the audio. If you want to keep the filler words in your transcript, you can use a prompt that contains them: Umm, let me think like, hmm... Okay, here's what I'm, like, thinking.\" Some languages can be written in different ways, such as simplified or traditional Chinese. The model might not always use the writing style that you want for your transcript by default. You can improve this by using a prompt in your preferred writing style.","title":"\u8bed\u97f3\u8f6c\u6587\u672c"},{"location":"guides/speech-to-text/#beta","text":"Learn how to turn audio into text Introduction The speech to text API provides two endpoints, transcriptions and translations, based on our state-of-the-art open source large-v2 Whisper model. They can be used to: Transcribe audio into whatever language the audio is in. Translate and transcribe the audio into english. File uploads are currently limited to 25 MB and the following input file types are supported: mp3, mp4, mpeg, mpga, m4a, wav, and webm.","title":"\u8bed\u97f3\u8f6c\u6587\u672c Beta"},{"location":"guides/speech-to-text/#quickstart","text":"Transcriptions The transcriptions API takes as input the audio file you want to transcribe and the desired output file format for the transcription of the audio. We currently support multiple input and output file formats. Transcribe audio python python 1 2 3 4 5 # Note: you need to be using OpenAI Python v0.27.0 for the code below to work import openai audio_file = open ( \"/path/to/file/audio.mp3\" , \"rb\" ) transcript = openai . Audio . transcribe ( \"whisper-1\" , audio_file ) By default, the response type will be json with the raw text included. 1 2 3 4 { \"text\" : \"Imagine the wildest idea that you've ever had, and you're curious about how it might scale to something that's a 100, a 1,000 times bigger.\" .... } To set additional parameters in a request, you can add more --form lines with the relevant options. For example, if you want to set the output format as text, you would add the following line: 1 2 3 4 ... --form file = @openai.mp3 \\ --form model = whisper-1 \\ --form response_format = text Translations The translations API takes as input the audio file in any of the supported languages and transcribes, if necessary, the audio into english. This differs from our /Transcriptions endpoint since the output is not in the original input language and is instead translated to english text. Translate audio python python 1 2 3 4 5 # Note: you need to be using OpenAI Python v0.27.0 for the code below to work import openai audio_file = open ( \"/path/to/file/german.mp3\" , \"rb\" ) transcript = openai . Audio . translate ( \"whisper-1\" , audio_file ) In this case, the inputted audio was german and the outputted text looks like: Hello, my name is Wolfgang and I come from Germany. Where are you heading today? We only support translation into english at this time. Supported languages We currently support the following languages through both the transcriptions and translations endpoint: Afrikaans, Arabic, Armenian, Azerbaijani, Belarusian, Bosnian, Bulgarian, Catalan, Chinese, Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French, Galician, German, Greek, Hebrew, Hindi, Hungarian, Icelandic, Indonesian, Italian, Japanese, Kannada, Kazakh, Korean, Latvian, Lithuanian, Macedonian, Malay, Marathi, Maori, Nepali, Norwegian, Persian, Polish, Portuguese, Romanian, Russian, Serbian, Slovak, Slovenian, Spanish, Swahili, Swedish, Tagalog, Tamil, Thai, Turkish, Ukrainian, Urdu, Vietnamese, and Welsh. While the underlying model was trained on 98 languages, we only list the languages that exceeded <50% word error rate (WER) which is an industry standard benchmark for speech to text model accuracy. The model will return results for languages not listed above but the quality will be low. Longer inputs By default, the Whisper API only supports files that are less than 25 MB. If you have an audio file that is longer than that, you will need to break it up into chunks of 25 MB's or less or used a compressed audio format. To get the best performance, we suggest that you avoid breaking the audio up mid-sentence as this may cause some context to be lost. One way to handle this is to use the PyDub open source Python package to split the audio: 1 2 3 4 5 6 7 8 9 10 11 from pydub import AudioSegment song = AudioSegment . from_mp3 ( \"good_morning.mp3\" ) # PyDub handles time in milliseconds ten * minutes = 10 * 60 \\ _ 1000 first_10_minutes = song [: ten_minutes ] first_10_minutes . export ( \"good_morning_10.mp3\" , format = \"mp3\" ) OpenAI makes no guarantees about the usability or security of 3 rd party software like PyDub.","title":"Quickstart"},{"location":"guides/speech-to-text/#prompting","text":"In the same way that you can use a prompt to influence the output of our language models, you can also use one to improve the quality of the transcripts generated by the Whisper API. The model will try to match the style of the prompt, so it will be more likely to use capitalization and punctuation if the prompt does too. Here are some examples of how prompting can help in different scenarios: Prompts can be very helpful for correcting specific words or acronyms that the model often misrecognizes in the audio. For example, the following prompt improves the transcription of the words DALL\u00b7E and GPT-3, which were previously written as \"GDP 3\" and \"DALI\". The transcript is about OpenAI which makes technology like DALL\u00b7E, GPT-3, and ChatGPT with the hope of one day building an AGI system that benefits all of humanity To preserve the context of a file that was split into segments, you can prompt the model with the transcript of the preceding segment. This will make the transcript more accurate, as the model will use the relevant information from the previous audio. The model will only consider the final 224 tokens of the prompt and ignore anything earlier. Sometimes the model might skip punctuation in the transcript. You can avoid this by using a simple prompt that includes punctuation: Hello, welcome to my lecture. The model may also leave out common filler words in the audio. If you want to keep the filler words in your transcript, you can use a prompt that contains them: Umm, let me think like, hmm... Okay, here's what I'm, like, thinking.\" Some languages can be written in different ways, such as simplified or traditional Chinese. The model might not always use the writing style that you want for your transcript by default. You can improve this by using a prompt in your preferred writing style.","title":"Prompting"},{"location":"tags/","text":"\u6807\u7b7e \u00b6","title":"\u6807\u7b7e"},{"location":"tags/#_1","text":"","title":"\u6807\u7b7e"}]}